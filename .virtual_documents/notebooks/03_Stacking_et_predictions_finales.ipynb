








import sys, os, logging
from pathlib import Path

# ── 0. Logger clair (avec Rich si dispo)
try:
    from rich.logging import RichHandler
    logging.basicConfig(level="INFO",
                        format="%(message)s",
                        handlers=[RichHandler(rich_tracebacks=True, markup=True)],
                        force=True)
except ModuleNotFoundError:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s",
                        stream=sys.stdout,
                        force=True)
logger = logging.getLogger(__name__)

# ── 1. Détection environnement Colab
def _in_colab() -> bool:
    try: import google.colab
    except ImportError: return False
    else: return True

# ── 2. Montage Drive manuel rapide
if _in_colab():
    from google.colab import drive
    if not Path("/content/drive/MyDrive/Colab Notebooks").exists():
        logger.info("🔗 Montage de Google Drive en cours…")
        drive.mount("/content/drive", force_remount=False)

# ── 3. Localisation racine projet STA211
def find_project_root() -> Path:
    env_path = os.getenv("STA211_PROJECT_PATH")
    if env_path and (Path(env_path) / "modules").exists():
        return Path(env_path).expanduser().resolve()

    # Chemin Colab correct
    default_colab = Path("/content/drive/MyDrive/Colab Notebooks/projet_sta211_2025")
    if _in_colab() and (default_colab / "modules").exists():
        return default_colab.resolve()

    cwd = Path.cwd()
    for p in [cwd, *cwd.parents]:
        if (p / "modules").exists():
            return p.resolve()

    raise FileNotFoundError("❌ Impossible de localiser un dossier contenant 'modules/'.")

# ── 4. Définition racine + PYTHONPATH
ROOT_DIR = find_project_root()
os.environ["STA211_PROJECT_PATH"] = str(ROOT_DIR)
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))
logger.info(f"📂 Racine projet détectée : {ROOT_DIR}")
logger.info(f"PYTHONPATH ← {ROOT_DIR}")

# ── 5. Initialisation de la configuration projet
from modules.config import cfg
cats = ['noad.', 'ad.']
LABEL_MAP = {0: "noad.", 1: "ad."} 


# ── 6. Affichage des chemins configurés automatiquement
def display_paths(style: bool = True):
    import pandas as pd
    paths_dict = {
        "root": cfg.paths.root,
        "raw": cfg.paths.raw,
        "processed": cfg.paths.processed,
        "models": cfg.paths.models,
        "outputs": cfg.paths.outputs,
        "artifacts": cfg.paths.artifacts
    }
    rows = [{"Clé": k, "Chemin": str(v)} for k, v in paths_dict.items()]
    df = pd.DataFrame(rows).set_index("Clé")

    # Vérification existence
    df["Existe"] = [
        "✅" if Path(v).exists() else "❌"
        for v in paths_dict.values()
    ]

    from IPython.display import display
    display(df.style.set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]},
    ]) if style else df)

display_paths()
logger.info("✅ Initialisation complète réussie - Notebook 03 prêt !")





# %pip install imbalanced-learn --quiet
# %pip install xgboost --quiet
# %pip install shap --quiet


## 0.2 · Chargement des bibliothèques ──────────────────────────────────────────

from IPython.display import Markdown, display

# ⬇️ Imports directs des bibliothèques nécessaires
try:
  # Bibliothèques de base
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import matplotlib
  import seaborn as sns

  # Scikit-learn et extensions
  import sklearn
  from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
  from sklearn.linear_model import LogisticRegression
  from sklearn.svm import SVC
  from sklearn.neural_network import MLPClassifier
  from sklearn.preprocessing import StandardScaler
  from sklearn.metrics import (
      classification_report, confusion_matrix, roc_auc_score,
      precision_recall_curve, f1_score, precision_score, recall_score
  )

  # Imbalanced-learn pour le traitement du déséquilibre
  import imblearn
  from imblearn.over_sampling import BorderlineSMOTE
  from imblearn.pipeline import Pipeline as ImbPipeline

  # XGBoost
  import xgboost as xgb
  from xgboost import XGBClassifier

  # ✅ SYSTÈME DE STOCKAGE UNIFIÉ - Plus besoin de joblib !
  from modules.utils import load_artifact, save_artifact

  # Utilitaires
  import json
  import warnings
  from tqdm import tqdm
  import scipy

  # Configuration des warnings
  warnings.filterwarnings('ignore', category=UserWarning)
  warnings.filterwarnings('ignore', category=FutureWarning)

  logger.info("📚 Bibliothèques importées avec succès")

except ImportError as e:
  logger.error(f"❌ Erreur d'importation : {e}")
  raise

# ───────────────────────────────────────────────────────────────────────────
# ✅ Affichage des versions principales
# ───────────────────────────────────────────────────────────────────────────

def _safe_version(mod, fallback="—"):
    """Retourne mod.__version__ ou un fallback si le module est absent."""
    try:
        return mod.__version__
    except Exception:
        return fallback

def display_modeling_library_versions():
    mods = {
        "pandas"           : pd,
        "numpy"            : np,
        "scikit-learn"     : sklearn,
        "imbalanced-learn" : imblearn,
        "xgboost"          : xgb,
        "matplotlib"       : matplotlib,
        "seaborn"          : sns,
        "scipy"            : scipy,
        "tqdm"             : __import__("tqdm"),
        "ipython"          : __import__("IPython")
    }
    versions_md = "\n".join(f"- `{k}` : {_safe_version(v)}" for k, v in mods.items())
    display(Markdown(f"✅ Versions des bibliothèques de modélisation\n{versions_md}"))

display_modeling_library_versions()
logger.info("✅ Chargement des bibliothèques terminé")


#!pip install scikit-optimize --quiet


# Imports pour la section etudes des variables importantes
try:
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFECV, SelectKBest, f_classif
    from sklearn.metrics import roc_auc_score
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer
    print("✅ Imports complémentaires chargés avec succès")
except ImportError as e:
    print(f"⚠️ Erreur d'import : {e}")
    print("Installez les dépendances manquantes avec:")
    print("pip install scikit-optimize")








# === CHARGEMENT MANUEL DES ARTEFACTS DU NOTEBOOK 02 ===
from modules.utils import load_artifact
import pandas as pd

print("📦 Chargement des artefacts du Notebook 02...")

# ✅ 1. Chargement du tableau des seuils (bon répertoire)
df_all_thr = pd.read_csv(cfg.paths.artifacts / "models" / "df_all_thresholds.csv")
print(f"✅ Métriques chargées : {len(df_all_thr)} modèles")

# ✅ 2. Récupération des chemins des pipelines depuis les fichiers JSON
models_dir = cfg.paths.models / "notebook2"
pipeline_paths = {}

for method in ["knn", "mice"]:
  for version in ["full", "reduced"]:
      key = f"{method}_{version}"
      json_file = f"best_{key}_pipelines.json"
      try:
          paths_dict = load_artifact(json_file, models_dir)
          pipeline_paths[key] = paths_dict
          print(f"✅ Chemins {key} : {len(paths_dict)} modèles")
      except Exception as e:
          print(f"❌ Erreur {key} : {e}")

# ✅ 3. Tentative de chargement des pipelines
all_optimized_pipelines = {}
total_loaded = 0

for key, paths_dict in pipeline_paths.items():
  all_optimized_pipelines[key] = {}
  for model_name, path_str in paths_dict.items():
      try:
          # Extraire le nom de fichier du chemin
          filename = Path(path_str).name
          pipeline = load_artifact(filename, models_dir)
          all_optimized_pipelines[key][model_name] = pipeline
          total_loaded += 1
      except Exception as e:
          print(f"⚠️ Pipeline {model_name} ({key}) non trouvé : {e}")

print(f"\n📊 RÉSUMÉ DU CHARGEMENT :")
print(f"  • Seuils : {len(df_all_thr)} ✅")
print(f"  • Pipelines : {total_loaded} ✅")
print(f"  • Configurations : {len(pipeline_paths)} ✅")

if total_loaded > 0:
  print("🚀 Prêt pour le stacking !")
else:
  print("⚠️ Aucun pipeline chargé - vérification nécessaire")


# Test de la nouvelle fonction load_stacking_data()
from modules.ensembles import load_stacking_data, select_champion_models

print("=" * 60)

try:
  # Chargement avec la nouvelle fonction
  splits_new, pipelines_new, metrics_df_new, features_new = load_stacking_data()

  print("\n✅ CHARGEMENT RÉUSSI !")
  print(f"📊 Splits : {len(splits_new)} méthodes")
  print(f"🔧 Pipelines : {sum(len(v) for v in pipelines_new.values())} modèles")
  print(f"📈 Métriques : {len(metrics_df_new)} entrées")
  print(f"🔍 Features : KNN({len(features_new.get('knn', []))}), MICE({len(features_new.get('mice', []))})")

  # Sélection des 4 champions FULL
  print("\n🏆 SÉLECTION DES 4 CHAMPIONS FULL :")
  print("=" * 50)

  champion_pipelines, champions_df = select_champion_models(
      metrics_df_new, pipelines_new, version="FULL", top_n=4
  )

  print(f"\n📊 DÉTAILS DES CHAMPIONS :")
  print(champions_df[['model', 'Imputation', 'Version', 'f1', 'precision', 'recall']].to_string(index=False))

  # Vérification de la compatibilité des features
  knn_features = len(features_new['knn'])
  mice_features = len(features_new['mice'])

  print(f"\n🔍 VÉRIFICATION COMPATIBILITÉ :")
  print(f"   • Features KNN : {knn_features}")
  print(f"   • Features MICE : {mice_features}")
  print(f"   • Compatible : {'✅ OUI' if knn_features == mice_features == 660 else '❌ NON'}")

  print(f"\n🎯 CHAMPIONS PRÊTS POUR STACKING :")
  print(f"   • Nombre de modèles : {len(champion_pipelines)}")
  print(f"   • Modèles : {list(champion_pipelines.keys())}")

  # Variables pour la suite
  globals()['splits_clean'] = splits_new
  globals()['pipelines_clean'] = pipelines_new
  globals()['metrics_clean'] = metrics_df_new
  globals()['features_clean'] = features_new
  globals()['champions_full'] = champion_pipelines
  globals()['champions_df'] = champions_df

  print(f"\n✅ Variables créées pour la suite du notebook")

except Exception as e:
  print(f"❌ ERREUR : {e}")
  import traceback
  traceback.print_exc()


# Implémentation du StackingClassifier avec les 4 champions FULL
from modules.ensembles import create_stacking_ensemble
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score, classification_report

print("🥞 IMPLÉMENTATION DU STACKING AVEC LES 4 CHAMPIONS")
print("=" * 60)

# Préparation des données pour le stacking

X_train_knn = splits_clean['knn']['X_train']
y_train_knn = splits_clean['knn']['y_train']
X_val_knn = splits_clean['knn']['X_val']
y_val_knn = splits_clean['knn']['y_val']
X_test_knn = splits_clean['knn']['X_test']
y_test_knn = splits_clean['knn']['y_test']

X_train_mice = splits_clean['mice']['X_train']
y_train_mice = splits_clean['mice']['y_train']
X_val_mice = splits_clean['mice']['X_val']
y_val_mice = splits_clean['mice']['y_val']
X_test_mice = splits_clean['mice']['X_test']
y_test_mice = splits_clean['mice']['y_test']

print(" Données de stacking :")
print(f"   • Train KNN : {X_train_knn.shape}")
print(f"   • Validation KNN : {X_val_knn.shape}")
print(f"   • Test KNN: {X_test_knn.shape}")

print(f"   • Train MICE : {X_train_mice.shape}")
print(f"   • Validation MICE : {X_val_mice.shape}")
print(f"   • Test MICE: {X_test_mice.shape}")






from modules.modeling import run_stacking_with_refit


# Pour le modèle KNN
results_knn_dict = run_stacking_with_refit(
    X_train_knn, y_train_knn, X_val_knn, y_val_knn, X_test_knn, y_test_knn,
    imputation_method='knn',
    models_dir=cfg.paths.models,
    output_dir=cfg.paths.models / "notebook3" / "stacking",
    # La fonction déduira automatiquement la clé 'stacking_classifier_knn'
    # mais on peut la spécifier si on prefere:
    stacking_model_key='stacking_classifier_knn'
)

# Pour le modèle MICE
results_mice_dict = run_stacking_with_refit(
    X_train_mice, y_train_mice, X_val_mice, y_val_mice, X_test_mice, y_test_mice,
    imputation_method='mice',
    models_dir=cfg.paths.models,
    output_dir=cfg.paths.models / "notebook3" / "stacking",
    # La fonction déduira automatiquement la clé 'stacking_classifier_mice'
    stacking_model_key='stacking_classifier_mice'
)


# 2. Accéder aux modèles entraînés à partir des résultats
trained_stacking_knn = results_knn_dict['model']
trained_stacking_mice = results_mice_dict['model']

# 3. Ré-optimiser les seuils en utilisant les modèles déjà entraînés
# et obtenir le dictionnaire 'results' au format requis
from modules.modeling import optimize_stacking_thresholds_with_trained_models

print("\n Ré-optimisation des seuils pour la visualisation...")
optimization_results = optimize_stacking_thresholds_with_trained_models(
    stacking_knn=trained_stacking_knn, # Modèle KNN entraîné
    stacking_mice=trained_stacking_mice, # Modèle MICE entraîné
    X_val_knn=X_val_knn,
    y_val_knn=y_val_knn,
    X_val_mice=X_val_mice,
    y_val_mice=y_val_mice,
    verbose=True
)
print("Ré-optimisation des seuils pour la visualisation...")
optimization_results = optimize_stacking_thresholds_with_trained_models(
    stacking_knn=trained_stacking_knn, # Modèle KNN entraîné
    stacking_mice=trained_stacking_mice, # Modèle MICE entraîné
    X_val_knn=X_val_knn,
    y_val_knn=y_val_knn,
    X_val_mice=X_val_mice,
    y_val_mice=y_val_mice,
    verbose=True
)
print("Ré-optimisation terminée.")


from modules.modeling import analyze_model_performance

# Pour KNN
analyze_model_performance(results_knn_dict, X_test_knn, y_test_knn, "Stacking avec Refit KNN")

# Pour MICE
analyze_model_performance(results_mice_dict, X_test_mice, y_test_mice, "Stacking avec Refit MICE")


# VALIDATION FINALE DES PERFORMANCES SUR LE TEST
print("🏆 VALIDATION FINALE - STACKING vs CHAMPION INDIVIDUEL")
print("=" * 60)

# Test des modèles de stacking sur le jeu de test
print("📊 PERFORMANCES SUR LE JEU DE TEST :")
print("-" * 40)

# Stacking KNN sur test
y_test_pred_stacking_knn = trained_stacking_knn.predict(X_test_knn)
y_test_proba_stacking_knn = trained_stacking_knn.predict_proba(X_test_knn)[:, 1]

# Application du seuil optimal (0.20)
y_test_pred_knn_optimized = (y_test_proba_stacking_knn >= 0.20).astype(int)
knn_f1_test = f1_score(y_test_knn, y_test_pred_knn_optimized)
knn_precision_test = precision_score(y_test_knn, y_test_pred_knn_optimized)
knn_recall_test = recall_score(y_test_knn, y_test_pred_knn_optimized)

print(f"✅ STACKING KNN (seuil=0.20) :")
print(f"   F1-Score  : {knn_f1_test:.4f}")
print(f"   Précision : {knn_precision_test:.4f}")
print(f"   Rappel    : {knn_recall_test:.4f}")

# Stacking MICE sur test
y_test_pred_stacking_mice = trained_stacking_mice.predict(X_test_mice)
y_test_proba_stacking_mice = trained_stacking_mice.predict_proba(X_test_mice)[:, 1]

# Application du seuil optimal (0.26)
y_test_pred_mice_optimized = (y_test_proba_stacking_mice >= 0.26).astype(int)
mice_f1_test = f1_score(y_test_mice, y_test_pred_mice_optimized)
mice_precision_test = precision_score(y_test_mice, y_test_pred_mice_optimized)
mice_recall_test = recall_score(y_test_mice, y_test_pred_mice_optimized)

print(f"\n✅ STACKING MICE (seuil=0.26) :")
print(f"   F1-Score  : {mice_f1_test:.4f}")
print(f"   Précision : {mice_precision_test:.4f}")
print(f"   Rappel    : {mice_recall_test:.4f}")

# Comparaison avec champion individuel
print(f"\n🏆 COMPARAISON FINALE :")
print("=" * 30)
print(f"🥇 XGBoost KNN individuel : F1 = 0.9385")
print(f"🥈 Stacking MICE optimisé : F1 = {mice_f1_test:.4f}")
print(f"🥉 Stacking KNN optimisé  : F1 = {knn_f1_test:.4f}")

# Déterminer le nouveau champion
best_f1 = max(0.9385, mice_f1_test, knn_f1_test)
if best_f1 == mice_f1_test and mice_f1_test > 0.9385:
  champion = "Stacking MICE"
  print(f"\n🎉 NOUVEAU CHAMPION : Stacking MICE (F1 = {mice_f1_test:.4f})")
elif best_f1 == knn_f1_test and knn_f1_test > 0.9385:
  champion = "Stacking KNN"
  print(f"\n🎉 NOUVEAU CHAMPION : Stacking KNN (F1 = {knn_f1_test:.4f})")
else:
  champion = "XGBoost KNN"
  print(f"\n🏆 CHAMPION MAINTENU : XGBoost KNN (F1 = 0.9385)")

  # Analyser la proximité
  mice_diff = 0.9385 - mice_f1_test
  knn_diff = 0.9385 - knn_f1_test

  if mice_diff < 0.01:
      print(f"   Stacking MICE très proche (-{mice_diff:.4f})")
  if knn_diff < 0.01:
      print(f"   Stacking KNN très proche (-{knn_diff:.4f})")

print(f"\n📈 ANALYSE DES PERFORMANCES :")
print("-" * 35)
print(f"• Objectif initial F1 > 0.92 : ✅ LARGEMENT DÉPASSÉ")
print(f"• Stacking MICE atteint : {mice_f1_test:.4f} ({(mice_f1_test/0.92-1)*100:+.1f}%)")
print(f"• Stacking KNN atteint : {knn_f1_test:.4f} ({(knn_f1_test/0.92-1)*100:+.1f}%)")
print(f"• Champion XGBoost : 0.9385 (+{(0.9385/0.92-1)*100:.1f}%)")

# Recommandation finale mise à jour
print(f"\n💡 RECOMMANDATION FINALE MISE À JOUR :")
print("=" * 40)

if mice_f1_test > 0.93:
  print(f"🏆 RECOMMANDATION : Stacking MICE")
  print(f"   • Performance exceptionnelle : F1 = {mice_f1_test:.4f}")
  print(f"   • Seuil optimal : 0.26")
  print(f"   • Combine la force de 5 modèles MICE")
elif knn_f1_test > 0.93:
  print(f"🏆 RECOMMANDATION : Stacking KNN")
  print(f"   • Performance exceptionnelle : F1 = {knn_f1_test:.4f}")
  print(f"   • Seuil optimal : 0.20")
  print(f"   • Combine la force de 5 modèles KNN")
else:
  print(f"🏆 RECOMMANDATION : XGBoost KNN FULL")
  print(f"   • Performance de référence : F1 = 0.9385")
  print(f"   • Plus simple et stable")
  print(f"   • Alternative : Stacking MICE (F1 = {mice_f1_test:.4f})")

# Sauvegarde des résultats finaux
final_stacking_results = {
  'stacking_knn': {
      'f1_test': knn_f1_test,
      'precision_test': knn_precision_test,
      'recall_test': knn_recall_test,
      'optimal_threshold': 0.20,
      'model': trained_stacking_knn
  },
  'stacking_mice': {
      'f1_test': mice_f1_test,
      'precision_test': mice_precision_test,
      'recall_test': mice_recall_test,
      'optimal_threshold': 0.26,
      'model': trained_stacking_mice
  },
  'champion': champion,
  'best_f1': best_f1
}

globals()['final_stacking_results'] = final_stacking_results
print(f"\n💾 Résultats finaux sauvegardés dans 'final_stacking_results'")
print(f"🚀 Analyse complète terminée !")





# --- 1. STACKING SANS REFIT (Utilisation de la fonction refactorisée) ---
print("\n" + "="*80)
print("DÉMARRAGE DU STACKING SANS REFIT")
print("="*80)

from modules.modeling import run_stacking_no_refit
# Garde le chemin Windows
MODELS_DIR_NB2 = r"C:\sta211-project\models\notebook2"

print(f"📁 Répertoire : {MODELS_DIR_NB2}")

# Stacking sans refit KNN
print("STACKING SANS REFIT - KNN")
print("=" * 80)
results_knn_no_refit = run_stacking_no_refit(
  X_val_knn_clean, y_val_knn_clean, X_test_knn_clean, y_test_knn_clean,
  imputation_method="knn",
  models_dir=MODELS_DIR_NB2
)

print("\nSTACKING SANS REFIT - MICE")
print("=" * 80)
results_mice_no_refit = run_stacking_no_refit(
  X_val_mice_clean, y_val_mice_clean, X_test_mice_clean, y_test_mice_clean,
  imputation_method="mice",
  models_dir=MODELS_DIR_NB2
)

# Résultats
print(f"\n🏆 RÉSULTATS")
print("=" * 20)
if results_knn_no_refit:
  print(f"✅ KNN  : F1 = {results_knn_no_refit.get('test_f1', 'N/A')}")
if results_mice_no_refit:
  print(f"✅ MICE : F1 = {results_mice_no_refit.get('test_f1', 'N/A')}")





# --- 5. COMPARAISON DES MODÈLES ---

from modules.modeling import build_comparison_table 
from pathlib import Path


# --- Définir les chemins des fichiers JSON de résultats ---
# Utilisez les chemins réels où vos fichiers sont sauvegardés
json_paths = [
    cfg.paths.models / "notebook3" / "stacking" / "stacking_no_refit_knn_full.json",
    cfg.paths.models /"notebook3"  / "stacking" / "stacking_no_refit_mice_full.json",
    cfg.paths.models /"notebook3"  / "stacking"/ "stacking_with_refit_knn.json",
    cfg.paths.models /"notebook3" / "stacking"/ "stacking_with_refit_mice.json"
]

# --- Définir les détails pour l'affichage dans le tableau ---
# Les clés DOIVENT correspondre exactement aux noms des FICHIERS (ce qui suit le dernier '/')
details = {
    "stacking_no_refit_knn_full.json": {"Nom Affiché": "Stacking sans refit KNN", "Type": "Complet", "Imputation": "KNN"},
    "stacking_no_refit_mice_full.json": {"Nom Affiché": "Stacking sans refit MICE", "Type": "Complet", "Imputation": "MICE"},
    "stacking_with_refit_knn.json": {"Nom Affiché": "Stacking avec refit KNN", "Type": "Complet", "Imputation": "KNN"}, # Nom corrigé (enlever _full)
    "stacking_with_refit_mice.json": {"Nom Affiché": "Stacking avec refit MICE", "Type": "Complet", "Imputation": "MICE"} # Nom corrigé (enlever _full)
}



# --- Généreration et affichage du tableau de comparaison ---
try:
    df_comparison = build_comparison_table(json_paths, details)
    if not df_comparison.empty:
        print("\n TABLEAU DE COMPARAISON FINAL:")
        print("=" * 100) # Ajusté pour plus de colonnes

        # --- Version avec style coloré ---
        try:
            # Vérifier si on est dans un environnement qui supporte le HTML (comme Jupyter/Colab)
            from IPython.display import display, HTML
            import pandas as pd

            # Appliquer le style : dégradé sur la colonne F1-score (test)
            # 'background_gradient' colore les cellules. cmap='Blues'/'Greens'/'viridis' sont des options.
            # subset permet de spécifier les colonnes concernées.
            # axis=0 pour normaliser sur toute la colonne, axis=None pour normaliser sur tout le tableau.
            styled_df = df_comparison.style.background_gradient(
                cmap='Greens', # Choix du dégradé de couleur (GnBu, Blues, Greens, viridis, etc.)
                subset=['F1-score (test)'], # Colonnes sur lesquelles appliquer le style
                axis=0 # Normalisation par colonne
            ).format({ # Formater les colonnes numériques
                'F1-score (test)': "{:.4f}",
                'Précision (test)': "{:.4f}",
                'Rappel (test)': "{:.4f}",
                'Seuil utilisé': "{:.3f}"
            }) # On peut ajouter .set_properties(**{'text-align': 'center'}) pour centrer le texte

            display(styled_df) # Affichage plus joli et coloré dans Colab/Jupyter
            print("(💡 Le meilleur F1-score est mis en évidence par une couleur plus foncée)")

        except ImportError:
            # Fallback si IPython.display n'est pas disponible ou échoue
            print(df_comparison.to_string(index=False, float_format="%.4f")) # Affichage standard
            print("\n( Pour un affichage coloré, exécutez ce code dans Jupyter/Colab)")

    else:
        print("⚠️ Le tableau de comparaison est vide.")
        # Afficher les chemins tentés pour aider au debug
        print("Chemins tentés :")
        for p in json_paths:
            print(f" - {p}")
except Exception as e:
    print(f"❌ Erreur lors de la génération du tableau de comparaison: {e}")
    import traceback
    traceback.print_exc() # Affiche la pile d'appels pour aider au debug

print("\n✅ TOUTES LES ÉTAPES DE STACKING SONT TERMINÉES !")






# Imports et Chargement Initial
from sklearn.pipeline import Pipeline as SklearnPipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import GradientBoostingClassifier

# Chargement du pipeline GradientBoosting optimisé MICE
pipeline_gradboost_mice = all_optimized_pipelines["gradboost_mice_full"]

# Suppression de l'étape SMOTE et reconstruction du préprocesseur
steps = [s for s in pipeline_gradboost_mice.steps[:-1] if "smote" not in s[0].lower()]
preprocessor_mice = SklearnPipeline(steps=steps)

# Prétraitement des données
X_train_mice_processed = preprocessor_mice.fit_transform(X_train_mice)
X_val_mice_processed = preprocessor_mice.transform(X_val_mice)
X_test_mice_processed = preprocessor_mice.transform(X_test_mice)

# Extraction du GradientBoosting optimisé
gradboost_estimator = pipeline_gradboost_mice.named_steps["clf"]

# Import et analyse des features
from modules.modeling import analyze_feature_importance

print("ANALYSE DE L'IMPORTANCE DES FEATURES")
print("="*50)
print("Modèle : GradientBoosting MICE FULL (F1=0.9313)")
print("Version : FULL, Seuil optimal : 0.491")

analyze_feature_importance(
    model=gradboost_estimator,
    X_train=X_train_mice_processed,
    y_train=y_train_mice,
    X_eval=X_val_mice_processed,
    y_eval=y_val_mice,
    feature_names=feature_cols,
    method='all',
    cv_folds=5,
    model_name="GradientBoosting_MICE_FULL"
)


def get_feature_names_after_preprocessing(preprocessor, X_original):
    """
    Tente d'obtenir les noms de features après transformation par un pipeline.
    """
    try:
        # Si le preprocessor et tous ses steps supportent get_feature_names_out
        return preprocessor.get_feature_names_out(X_original.columns).tolist()
    except Exception as e:
        # Si échec, créer des noms génériques
        # Vérifier le nombre de features après transformation
        X_transformed_sample = preprocessor.transform(X_original.head(1)) # Un échantillon
        n_features_out = X_transformed_sample.shape[1]
        print(" Impossible d'obtenir les noms des features via get_feature_names_out. "
              f"Génération de noms génériques pour {n_features_out} features.")
        return [f"feature_processed_{i}" for i in range(n_features_out)]



print("\n" + "="*80)
print(" CHARGEMENT DU MODÈLE GRADIENT BOOSTING COMPLET")
print("="*80)

import joblib
from pathlib import Path

# 1. Charger le pipeline Gradient Boosting complet
pipeline_gradboost_mice_full_path = cfg.paths.models / "notebook2" / "mice" / "pipeline_gradboost_mice_full.joblib"

print(f"📁 Chargement depuis : {pipeline_gradboost_mice_full_path}")

if pipeline_gradboost_mice_full_path.exists():
    pipeline_gradboost_mice_full = joblib.load(pipeline_gradboost_mice_full_path)
    print("✅ Pipeline Gradient Boosting MICE complet chargé avec succès")
else:
    print(f"❌ Fichier non trouvé : {pipeline_gradboost_mice_full_path}")
    raise FileNotFoundError(f"Le fichier {pipeline_gradboost_mice_full_path} n'existe pas")

# 2. Extraire le modèle et le préprocesseur (correction des noms d'étapes)
gradboost_estimator_full = pipeline_gradboost_mice_full.named_steps["clf"]
scaler_full = pipeline_gradboost_mice_full.named_steps["scale"]

print(f"📊 Informations du modèle :")
print(f"   • Type de modèle : {type(gradboost_estimator_full).__name__}")
print(f"   • Nombre de features attendues : {gradboost_estimator_full.n_features_in_}")
print(f"   • Nombre d'estimateurs : {getattr(gradboost_estimator_full, 'n_estimators', 'N/A')}")

# 3. Utiliser les données MICE originales
X_train_mice_original = splits["mice"]["X_train"][features_mice]
X_test_mice_original = splits["mice"]["X_test"][features_mice]
y_train_mice_original = splits["mice"]["y_train"]
y_test_mice_original = splits["mice"]["y_test"]

print(f"📊 Dimensions des données originales :")
print(f"   • X_train_original : {X_train_mice_original.shape}")
print(f"   • X_test_original : {X_test_mice_original.shape}")

# 4. Créer un préprocesseur sans SMOTE (pour l'analyse)
from sklearn.pipeline import Pipeline as SklearnPipeline
preprocessor_without_smote = SklearnPipeline([
    ('scale', scaler_full)
])

# 5. Appliquer le préprocesseur (sans SMOTE pour l'analyse)
print(f"\n Application du préprocesseur (sans SMOTE)...")
X_train_mice_transformed = preprocessor_without_smote.fit_transform(X_train_mice_original)
X_test_mice_transformed = preprocessor_without_smote.transform(X_test_mice_original)

print(f"📊 Dimensions après transformation :")
print(f"   • X_train_transformed : {X_train_mice_transformed.shape}")
print(f"   • X_test_transformed : {X_test_mice_transformed.shape}")

# 6. Obtenir les noms de features après transformation
try:
    feature_names_transformed = preprocessor_without_smote.get_feature_names_out()
    print(f"✅ Noms de features obtenus : {len(feature_names_transformed)} features")
except Exception as e:
    print(f"⚠️ Impossible d'obtenir les noms de features : {e}")
    feature_names_transformed = [f"feature_{i}" for i in range(X_train_mice_transformed.shape[1])]
    print(f" Noms génériques créés : {len(feature_names_transformed)} features")

# 7. Vérifier la cohérence des dimensions
if X_train_mice_transformed.shape[1] == gradboost_estimator_full.n_features_in_:
    print("✅ Dimensions cohérentes entre le modèle et les données transformées")
else:
    print(f"❌ Incohérence de dimensions :")
    print(f"   • Modèle attend : {gradboost_estimator_full.n_features_in_} features")
    print(f"   • Données transformées : {X_train_mice_transformed.shape[1]} features")
    raise ValueError("Dimensions incompatibles entre le modèle et les données")

# 8. Utiliser la version debug pour l'analyse
print(f"\n🔍 DÉMARRAGE DE L'ANALYSE AVEC LE MODÈLE COMPLET")
print("="*60)

from modules.modeling import analyze_feature_importance

results_gradboost_analysis_full = analyze_feature_importance(
    model=gradboost_estimator_full,
    X_train=X_train_mice_transformed,
    y_train=y_train_mice_original,
    X_eval=X_test_mice_transformed,
    y_eval=y_test_mice_original,
    feature_names=feature_names_transformed,
    method='all',
    cv_folds=10,
    n_repeats_perm=20,
    output_dir=OUTPUTS_DIR / "features_selection" / "gradboost_mice",
    model_name="GradBoost_MICE_Full_Complete",
    save_results=True
)

# 9. Créer les graphiques avec le modèle complet
if results_gradboost_analysis_full is not None:
    print("\n" + "="*80)
    print("📊 CRÉATION DES GRAPHIQUES AVEC LE MODÈLE COMPLET")
    print("="*80)
    
    from modules.modeling import (
        plot_rfecv_evolution, 
        plot_simple_rfecv_evolution,
        plot_feature_importance_comparison, 
        create_feature_selection_summary
    )
    
    # Graphique d'évolution du score en fonction du nombre de variables
    print("\n🎯 Graphique d'évolution RFECV (version simple) :")
    fig_simple = plot_simple_rfecv_evolution(
        results_dict=results_gradboost_analysis_full,
        model_name="GradBoost MICE Complet",
        save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "rfecv_evolution_simple_full.png",
        figsize=(12, 8)
    )
    
    # Graphique détaillé avec écart-type
    print("\n📊 Graphique d'évolution RFECV (version détaillée) :")
    fig_detailed = plot_rfecv_evolution(
        results_dict=results_gradboost_analysis_full,
        model_name="GradBoost MICE Complet",
        save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "rfecv_evolution_detailed_full.png",
        figsize=(14, 10)
    )
    
    # Graphique comparatif complet
    print("\n📈 Graphique comparatif complet :")
    fig_comparison = plot_feature_importance_comparison(
        results_dict=results_gradboost_analysis_full,
        model_name="GradBoost MICE Complet",
        save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "feature_importance_comparison_full.png",
        figsize=(14, 10)
    )
    
    # Résumé textuel détaillé
    print("\n Résumé détaillé de l'analyse :")
    create_feature_selection_summary(
        results_dict=results_gradboost_analysis_full,
        model_name="GradBoost MICE Complet"
    )
    
    print("\n✅ Graphiques créés et sauvegardés avec succès !")
    print(" Fichiers sauvegardés dans : outputs/features_selection/gradboost_mice/")
    
else:
    print("❌ Impossible de créer les graphiques : résultats d'analyse non disponibles")

print("\n" + "="*80)
print("🎉 ANALYSE TERMINÉE AVEC LE MODÈLE COMPLET")
print("="*80)


# =============================================================================
# GRAPHIQUE : ÉVOLUTION DU SCORE EN FONCTION DU NOMBRE DE VARIABLES
# =============================================================================

if results_gradboost_analysis_full is not None:
    print("\n" + "="*80)
    print("📊 CRÉATION DES GRAPHIQUES D'ANALYSE DES FEATURES")
    print("="*80)
    
    # Import du module de visualisation
    from modules.visualization import (
        plot_rfecv_evolution, 
        plot_simple_rfecv_evolution,
        plot_feature_importance_comparison, 
        create_feature_selection_summary
    )
    
    try:
        # 1. Graphique simple et clair : Évolution du score vs nombre de features
        print("\n🎯 Graphique d'évolution RFECV (version simple) :")
        fig_simple = plot_simple_rfecv_evolution(
            results_dict=results_gradboost_analysis_full,
            model_name="GradBoost MICE Complet",
            save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "rfecv_evolution_simple.png",
            figsize=(14, 8)
        )
        print("   ✅ Graphique simple créé avec succès")
        
        # 2. Graphique détaillé avec écart-type
        print("\n📊 Graphique d'évolution RFECV (version détaillée) :")
        fig_detailed = plot_rfecv_evolution(
            results_dict=results_gradboost_analysis_full,
            model_name="GradBoost MICE Complet",
            save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "rfecv_evolution_detailed.png",
            figsize=(14, 10)
        )
        print("   ✅ Graphique détaillé créé avec succès")
        
        # 3. Graphique comparatif complet (avec gestion des erreurs)
        print("\n📈 Graphique comparatif complet :")
        fig_comparison = plot_feature_importance_comparison(
            results_dict=results_gradboost_analysis_full,
            model_name="GradBoost MICE Complet",
            save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "feature_importance_comparison.png",
            figsize=(16, 12)
        )
        print("   ✅ Graphique comparatif créé avec succès")
        
        # 4. Résumé textuel détaillé
        print("\n Résumé détaillé de l'analyse :")
        create_feature_selection_summary(
            results_dict=results_gradboost_analysis_full,
            model_name="GradBoost MICE Complet"
        )
        
        print("\n" + "="*80)
        print("✅ TOUS LES GRAPHIQUES ONT ÉTÉ CRÉÉS AVEC SUCCÈS !")
        print("="*80)
        print(" Fichiers sauvegardés dans : outputs/features_selection/gradboost_mice/")
        print("   • rfecv_evolution_simple.png")
        print("   • rfecv_evolution_detailed.png") 
        print("   • feature_importance_comparison.png")
        print("="*80)
        
    except Exception as e:
        print(f"\n❌ Erreur lors de la création des graphiques : {e}")
        print("🔍 Tentative de création avec des paramètres simplifiés...")
        
        try:
            # Version simplifiée en cas d'erreur
            print("\n🎯 Création d'un graphique simple de secours :")
            fig_simple = plot_simple_rfecv_evolution(
                results_dict=results_gradboost_analysis_full,
                model_name="GradBoost MICE Complet",
                save_path=OUTPUTS_DIR / "features_selection" / "gradboost_mice" / "rfecv_evolution_simple_backup.png",
                figsize=(10, 6)
            )
            print("   ✅ Graphique de secours créé avec succès")
            
        except Exception as e2:
            print(f"❌ Impossible de créer même le graphique de secours : {e2}")
            print("📊 Affichage des données brutes disponibles :")
            print(f"   • Méthodes appliquées : {results_gradboost_analysis_full.get('methods_applied', [])}")
            if 'rfecv' in results_gradboost_analysis_full:
                rfecv_data = results_gradboost_analysis_full['rfecv']
                print(f"   • Features optimales RFECV : {rfecv_data.get('n_features_optimal', 'N/A')}")
                print(f"   • Meilleur score RFECV : {rfecv_data.get('best_score', 'N/A')}")
    
else:
    print("\n" + "="*80)
    print("❌ IMPOSSIBLE DE CRÉER LES GRAPHIQUES")
    print("="*80)
    print("🔍 Raisons possibles :")
    print("   • L'analyse n'a pas été exécutée correctement")
    print("   • La variable 'results_gradboost_analysis_full' est None")
    print("   • Erreur lors de l'analyse des features")
    print("="*80)
    print("💡 Vérifiez que la cellule d'analyse précédente s'est bien terminée")
    print("   et que la variable 'results_gradboost_analysis_full' existe.")
    print("="*80)


# Extraction des résultats pour la personnalisation du markdown
if results_gradboost_analysis_full and 'rfecv' in results_gradboost_analysis_full:
    rfecv_data = results_gradboost_analysis_full['rfecv']
    n_optimal = rfecv_data.get('n_features_optimal', 'N/A')
    best_score = rfecv_data.get('best_score', 'N/A')
    print(f"Nombre optimal de variables : {n_optimal}")
    print(f"Meilleur score RFECV : {best_score}")








from utils import generate_final_predictions

# Avec le modèle champion (GradBoost KNN Reduced)
#submission = generate_final_predictions(use_stacking=False)

# Ou avec le stacking
submission = generate_final_predictions(use_stacking=True)






