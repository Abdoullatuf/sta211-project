








import sys, os, logging
from pathlib import Path

# ── 0. Logger clair (avec Rich si dispo)
try:
    from rich.logging import RichHandler
    logging.basicConfig(level="INFO",
                        format="%(message)s",
                        handlers=[RichHandler(rich_tracebacks=True, markup=True)],
                        force=True)
except ModuleNotFoundError:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s",
                        stream=sys.stdout,
                        force=True)
logger = logging.getLogger(__name__)

# ── 1. Détection environnement Colab
def _in_colab() -> bool:
    try: import google.colab
    except ImportError: return False
    else: return True

# ── 2. Montage Drive manuel rapide
if _in_colab():
    from google.colab import drive
    if not Path("/content/drive/MyDrive/Colab Notebooks").exists():
        logger.info("🔗 Montage de Google Drive en cours…")
        drive.mount("/content/drive", force_remount=False)

# ── 3. Localisation racine projet STA211
def find_project_root() -> Path:
    env_path = os.getenv("STA211_PROJECT_PATH")
    if env_path and (Path(env_path) / "modules").exists():
        return Path(env_path).expanduser().resolve()

    # Chemin Colab correct
    default_colab = Path("/content/drive/MyDrive/Colab Notebooks/projet_sta211_2025")
    if _in_colab() and (default_colab / "modules").exists():
        return default_colab.resolve()

    cwd = Path.cwd()
    for p in [cwd, *cwd.parents]:
        if (p / "modules").exists():
            return p.resolve()

    raise FileNotFoundError("❌ Impossible de localiser un dossier contenant 'modules/'.")

# ── 4. Définition racine + PYTHONPATH
ROOT_DIR = find_project_root()
os.environ["STA211_PROJECT_PATH"] = str(ROOT_DIR)
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))
logger.info(f"📂 Racine projet détectée : {ROOT_DIR}")
logger.info(f"PYTHONPATH ← {ROOT_DIR}")

# ── 5. Initialisation de la configuration projet
from modules.config import cfg
cats = ['noad.', 'ad.']
LABEL_MAP = {0: "noad.", 1: "ad."} 


# ── 6. Affichage des chemins configurés automatiquement
def display_paths(style: bool = True):
    import pandas as pd
    paths_dict = {
        "root": cfg.paths.root,
        "raw": cfg.paths.raw,
        "processed": cfg.paths.processed,
        "models": cfg.paths.models,
        "outputs": cfg.paths.outputs,
        "artifacts": cfg.paths.artifacts
    }
    rows = [{"Clé": k, "Chemin": str(v)} for k, v in paths_dict.items()]
    df = pd.DataFrame(rows).set_index("Clé")

    # Vérification existence
    df["Existe"] = [
        "✅" if Path(v).exists() else "❌"
        for v in paths_dict.values()
    ]

    from IPython.display import display
    display(df.style.set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]},
    ]) if style else df)

display_paths()
logger.info("✅ Initialisation complète réussie - Notebook 03 prêt !")





# %pip install imbalanced-learn --quiet
# %pip install xgboost --quiet
# %pip install shap --quiet


## 0.2 · Chargement des bibliothèques ──────────────────────────────────────────

from IPython.display import Markdown, display

# ⬇️ Imports directs des bibliothèques nécessaires
try:
  # Bibliothèques de base
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import matplotlib
  import seaborn as sns

  # Scikit-learn et extensions
  import sklearn
  from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
  from sklearn.linear_model import LogisticRegression
  from sklearn.svm import SVC
  from sklearn.neural_network import MLPClassifier
  from sklearn.preprocessing import StandardScaler
  from sklearn.metrics import (
      classification_report, confusion_matrix, roc_auc_score,
      precision_recall_curve, f1_score, precision_score, recall_score
  )

  # Imbalanced-learn pour le traitement du déséquilibre
  import imblearn
  from imblearn.over_sampling import BorderlineSMOTE
  from imblearn.pipeline import Pipeline as ImbPipeline

  # XGBoost
  import xgboost as xgb
  from xgboost import XGBClassifier

  # ✅ SYSTÈME DE STOCKAGE UNIFIÉ - Plus besoin de joblib !
  from modules.utils import load_artifact, save_artifact

  # Utilitaires
  import json
  import warnings
  from tqdm import tqdm
  import scipy

  # Configuration des warnings
  warnings.filterwarnings('ignore', category=UserWarning)
  warnings.filterwarnings('ignore', category=FutureWarning)

  logger.info("📚 Bibliothèques importées avec succès")

except ImportError as e:
  logger.error(f"❌ Erreur d'importation : {e}")
  raise

# ───────────────────────────────────────────────────────────────────────────
# ✅ Affichage des versions principales
# ───────────────────────────────────────────────────────────────────────────

def _safe_version(mod, fallback="—"):
    """Retourne mod.__version__ ou un fallback si le module est absent."""
    try:
        return mod.__version__
    except Exception:
        return fallback

def display_modeling_library_versions():
    mods = {
        "pandas"           : pd,
        "numpy"            : np,
        "scikit-learn"     : sklearn,
        "imbalanced-learn" : imblearn,
        "xgboost"          : xgb,
        "matplotlib"       : matplotlib,
        "seaborn"          : sns,
        "scipy"            : scipy,
        "tqdm"             : __import__("tqdm"),
        "ipython"          : __import__("IPython")
    }
    versions_md = "\n".join(f"- `{k}` : {_safe_version(v)}" for k, v in mods.items())
    display(Markdown(f"✅ Versions des bibliothèques de modélisation\n{versions_md}"))

display_modeling_library_versions()
logger.info("✅ Chargement des bibliothèques terminé")


#!pip install scikit-optimize --quiet


# Imports pour la section etudes des variables importantes
try:
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFECV, SelectKBest, f_classif
    from sklearn.metrics import roc_auc_score
    from skopt import BayesSearchCV
    from skopt.space import Real, Integer
    print("✅ Imports complémentaires chargés avec succès")
except ImportError as e:
    print(f"⚠️ Erreur d'import : {e}")
    print("Installez les dépendances manquantes avec:")
    print("pip install scikit-optimize")








# === CHARGEMENT MANUEL DES ARTEFACTS DU NOTEBOOK 02 ===
from modules.utils import load_artifact
import pandas as pd

print("📦 Chargement des artefacts du Notebook 02...")

# ✅ 1. Chargement du tableau des seuils (bon répertoire)
df_all_thr = pd.read_csv(cfg.paths.artifacts / "models" / "df_all_thresholds.csv")
print(f"✅ Métriques chargées : {len(df_all_thr)} modèles")

# ✅ 2. Récupération des chemins des pipelines depuis les fichiers JSON
models_dir = cfg.paths.models / "notebook2"
pipeline_paths = {}

for method in ["knn", "mice"]:
  for version in ["full", "reduced"]:
      key = f"{method}_{version}"
      json_file = f"best_{key}_pipelines.json"
      try:
          paths_dict = load_artifact(json_file, models_dir)
          pipeline_paths[key] = paths_dict
          print(f"✅ Chemins {key} : {len(paths_dict)} modèles")
      except Exception as e:
          print(f"❌ Erreur {key} : {e}")

# ✅ 3. Tentative de chargement des pipelines
all_optimized_pipelines = {}
total_loaded = 0

for key, paths_dict in pipeline_paths.items():
  all_optimized_pipelines[key] = {}
  for model_name, path_str in paths_dict.items():
      try:
          # Extraire le nom de fichier du chemin
          filename = Path(path_str).name
          pipeline = load_artifact(filename, models_dir)
          all_optimized_pipelines[key][model_name] = pipeline
          total_loaded += 1
      except Exception as e:
          print(f"⚠️ Pipeline {model_name} ({key}) non trouvé : {e}")

print(f"\n📊 RÉSUMÉ DU CHARGEMENT :")
print(f"  • Seuils : {len(df_all_thr)} ✅")
print(f"  • Pipelines : {total_loaded} ✅")
print(f"  • Configurations : {len(pipeline_paths)} ✅")

if total_loaded > 0:
  print("🚀 Prêt pour le stacking !")
else:
  print("⚠️ Aucun pipeline chargé - vérification nécessaire")


# Test de la nouvelle fonction load_stacking_data()
from modules.ensembles import load_stacking_data, select_champion_models

print("=" * 60)

try:
  # Chargement avec la nouvelle fonction
  splits_new, pipelines_new, metrics_df_new, features_new = load_stacking_data()

  print("\n✅ CHARGEMENT RÉUSSI !")
  print(f"📊 Splits : {len(splits_new)} méthodes")
  print(f"🔧 Pipelines : {sum(len(v) for v in pipelines_new.values())} modèles")
  print(f"📈 Métriques : {len(metrics_df_new)} entrées")
  print(f"🔍 Features : KNN({len(features_new.get('knn', []))}), MICE({len(features_new.get('mice', []))})")

  # Sélection des 4 champions FULL
  print("\n🏆 SÉLECTION DES 4 CHAMPIONS FULL :")
  print("=" * 50)

  champion_pipelines, champions_df = select_champion_models(
      metrics_df_new, pipelines_new, version="FULL", top_n=4
  )

  print(f"\n📊 DÉTAILS DES CHAMPIONS :")
  print(champions_df[['model', 'Imputation', 'Version', 'f1', 'precision', 'recall']].to_string(index=False))

  # Vérification de la compatibilité des features
  knn_features = len(features_new['knn'])
  mice_features = len(features_new['mice'])

  print(f"\n🔍 VÉRIFICATION COMPATIBILITÉ :")
  print(f"   • Features KNN : {knn_features}")
  print(f"   • Features MICE : {mice_features}")
  print(f"   • Compatible : {'✅ OUI' if knn_features == mice_features == 660 else '❌ NON'}")

  print(f"\n🎯 CHAMPIONS PRÊTS POUR STACKING :")
  print(f"   • Nombre de modèles : {len(champion_pipelines)}")
  print(f"   • Modèles : {list(champion_pipelines.keys())}")

  # Variables pour la suite
  globals()['splits_clean'] = splits_new
  globals()['pipelines_clean'] = pipelines_new
  globals()['metrics_clean'] = metrics_df_new
  globals()['features_clean'] = features_new
  globals()['champions_full'] = champion_pipelines
  globals()['champions_df'] = champions_df

  print(f"\n✅ Variables créées pour la suite du notebook")

except Exception as e:
  print(f"❌ ERREUR : {e}")
  import traceback
  traceback.print_exc()


# Implémentation du StackingClassifier avec les 4 champions FULL
from modules.ensembles import create_stacking_ensemble
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score, classification_report

print("🥞 IMPLÉMENTATION DU STACKING AVEC LES 4 CHAMPIONS")
print("=" * 60)

# Préparation des données pour le stacking

X_train_knn = splits_clean['knn']['X_train']
y_train_knn = splits_clean['knn']['y_train']
X_val_knn = splits_clean['knn']['X_val']
y_val_knn = splits_clean['knn']['y_val']
X_test_knn = splits_clean['knn']['X_test']
y_test_knn = splits_clean['knn']['y_test']

X_train_mice = splits_clean['mice']['X_train']
y_train_mice = splits_clean['mice']['y_train']
X_val_mice = splits_clean['mice']['X_val']
y_val_mice = splits_clean['mice']['y_val']
X_test_mice = splits_clean['mice']['X_test']
y_test_mice = splits_clean['mice']['y_test']

print(" Données de stacking :")
print(f"   • Train KNN : {X_train_knn.shape}")
print(f"   • Validation KNN : {X_val_knn.shape}")
print(f"   • Test KNN: {X_test_knn.shape}")

print(f"   • Train MICE : {X_train_mice.shape}")
print(f"   • Validation MICE : {X_val_mice.shape}")
print(f"   • Test MICE: {X_test_mice.shape}")






from modules.modeling import run_stacking_with_refit


# Pour le modèle KNN
results_knn_dict = run_stacking_with_refit(
    X_train_knn, y_train_knn, X_val_knn, y_val_knn, X_test_knn, y_test_knn,
    imputation_method='knn',
    models_dir=cfg.paths.models,
    output_dir=cfg.paths.models / "notebook3" / "stacking",
    # La fonction déduira automatiquement la clé 'stacking_classifier_knn'
    # mais on peut la spécifier si on prefere:
    stacking_model_key='stacking_classifier_knn'
)

# Pour le modèle MICE
results_mice_dict = run_stacking_with_refit(
    X_train_mice, y_train_mice, X_val_mice, y_val_mice, X_test_mice, y_test_mice,
    imputation_method='mice',
    models_dir=cfg.paths.models,
    output_dir=cfg.paths.models / "notebook3" / "stacking",
    # La fonction déduira automatiquement la clé 'stacking_classifier_mice'
    stacking_model_key='stacking_classifier_mice'
)


# 2. Accéder aux modèles entraînés à partir des résultats
trained_stacking_knn = results_knn_dict['model']
trained_stacking_mice = results_mice_dict['model']

# 3. Ré-optimiser les seuils en utilisant les modèles déjà entraînés
# et obtenir le dictionnaire 'results' au format requis
from modules.modeling import optimize_stacking_thresholds_with_trained_models

print("\n Ré-optimisation des seuils pour la visualisation...")
optimization_results = optimize_stacking_thresholds_with_trained_models(
    stacking_knn=trained_stacking_knn, # Modèle KNN entraîné
    stacking_mice=trained_stacking_mice, # Modèle MICE entraîné
    X_val_knn=X_val_knn,
    y_val_knn=y_val_knn,
    X_val_mice=X_val_mice,
    y_val_mice=y_val_mice,
    verbose=True
)
print("Ré-optimisation des seuils pour la visualisation...")
optimization_results = optimize_stacking_thresholds_with_trained_models(
    stacking_knn=trained_stacking_knn, # Modèle KNN entraîné
    stacking_mice=trained_stacking_mice, # Modèle MICE entraîné
    X_val_knn=X_val_knn,
    y_val_knn=y_val_knn,
    X_val_mice=X_val_mice,
    y_val_mice=y_val_mice,
    verbose=True
)
print("Ré-optimisation terminée.")


from modules.modeling import analyze_model_performance

# Pour KNN
analyze_model_performance(results_knn_dict, X_test_knn, y_test_knn, "Stacking avec Refit KNN")

# Pour MICE
analyze_model_performance(results_mice_dict, X_test_mice, y_test_mice, "Stacking avec Refit MICE")


# VALIDATION FINALE DES PERFORMANCES SUR LE TEST
print("🏆 VALIDATION FINALE - STACKING vs CHAMPION INDIVIDUEL")
print("=" * 60)

# Test des modèles de stacking sur le jeu de test
print("📊 PERFORMANCES SUR LE JEU DE TEST :")
print("-" * 40)

# Stacking KNN sur test
y_test_pred_stacking_knn = trained_stacking_knn.predict(X_test_knn)
y_test_proba_stacking_knn = trained_stacking_knn.predict_proba(X_test_knn)[:, 1]

# Application du seuil optimal (0.20)
y_test_pred_knn_optimized = (y_test_proba_stacking_knn >= 0.20).astype(int)
knn_f1_test = f1_score(y_test_knn, y_test_pred_knn_optimized)
knn_precision_test = precision_score(y_test_knn, y_test_pred_knn_optimized)
knn_recall_test = recall_score(y_test_knn, y_test_pred_knn_optimized)

print(f"✅ STACKING KNN (seuil=0.20) :")
print(f"   F1-Score  : {knn_f1_test:.4f}")
print(f"   Précision : {knn_precision_test:.4f}")
print(f"   Rappel    : {knn_recall_test:.4f}")

# Stacking MICE sur test
y_test_pred_stacking_mice = trained_stacking_mice.predict(X_test_mice)
y_test_proba_stacking_mice = trained_stacking_mice.predict_proba(X_test_mice)[:, 1]

# Application du seuil optimal (0.26)
y_test_pred_mice_optimized = (y_test_proba_stacking_mice >= 0.26).astype(int)
mice_f1_test = f1_score(y_test_mice, y_test_pred_mice_optimized)
mice_precision_test = precision_score(y_test_mice, y_test_pred_mice_optimized)
mice_recall_test = recall_score(y_test_mice, y_test_pred_mice_optimized)

print(f"\n✅ STACKING MICE (seuil=0.26) :")
print(f"   F1-Score  : {mice_f1_test:.4f}")
print(f"   Précision : {mice_precision_test:.4f}")
print(f"   Rappel    : {mice_recall_test:.4f}")

# Comparaison avec champion individuel
print(f"\n🏆 COMPARAISON FINALE :")
print("=" * 30)
print(f"🥇 XGBoost KNN individuel : F1 = 0.9385")
print(f"🥈 Stacking MICE optimisé : F1 = {mice_f1_test:.4f}")
print(f"🥉 Stacking KNN optimisé  : F1 = {knn_f1_test:.4f}")

# Déterminer le nouveau champion
best_f1 = max(0.9385, mice_f1_test, knn_f1_test)
if best_f1 == mice_f1_test and mice_f1_test > 0.9385:
  champion = "Stacking MICE"
  print(f"\n🎉 NOUVEAU CHAMPION : Stacking MICE (F1 = {mice_f1_test:.4f})")
elif best_f1 == knn_f1_test and knn_f1_test > 0.9385:
  champion = "Stacking KNN"
  print(f"\n🎉 NOUVEAU CHAMPION : Stacking KNN (F1 = {knn_f1_test:.4f})")
else:
  champion = "XGBoost KNN"
  print(f"\n🏆 CHAMPION MAINTENU : XGBoost KNN (F1 = 0.9385)")

  # Analyser la proximité
  mice_diff = 0.9385 - mice_f1_test
  knn_diff = 0.9385 - knn_f1_test

  if mice_diff < 0.01:
      print(f"   Stacking MICE très proche (-{mice_diff:.4f})")
  if knn_diff < 0.01:
      print(f"   Stacking KNN très proche (-{knn_diff:.4f})")

print(f"\n📈 ANALYSE DES PERFORMANCES :")
print("-" * 35)
print(f"• Objectif initial F1 > 0.92 : ✅ LARGEMENT DÉPASSÉ")
print(f"• Stacking MICE atteint : {mice_f1_test:.4f} ({(mice_f1_test/0.92-1)*100:+.1f}%)")
print(f"• Stacking KNN atteint : {knn_f1_test:.4f} ({(knn_f1_test/0.92-1)*100:+.1f}%)")
print(f"• Champion XGBoost : 0.9385 (+{(0.9385/0.92-1)*100:.1f}%)")

# Recommandation finale mise à jour
print(f"\n💡 RECOMMANDATION FINALE MISE À JOUR :")
print("=" * 40)

if mice_f1_test > 0.93:
  print(f"🏆 RECOMMANDATION : Stacking MICE")
  print(f"   • Performance exceptionnelle : F1 = {mice_f1_test:.4f}")
  print(f"   • Seuil optimal : 0.26")
  print(f"   • Combine la force de 5 modèles MICE")
elif knn_f1_test > 0.93:
  print(f"🏆 RECOMMANDATION : Stacking KNN")
  print(f"   • Performance exceptionnelle : F1 = {knn_f1_test:.4f}")
  print(f"   • Seuil optimal : 0.20")
  print(f"   • Combine la force de 5 modèles KNN")
else:
  print(f"🏆 RECOMMANDATION : XGBoost KNN FULL")
  print(f"   • Performance de référence : F1 = 0.9385")
  print(f"   • Plus simple et stable")
  print(f"   • Alternative : Stacking MICE (F1 = {mice_f1_test:.4f})")

# Sauvegarde des résultats finaux
final_stacking_results = {
  'stacking_knn': {
      'f1_test': knn_f1_test,
      'precision_test': knn_precision_test,
      'recall_test': knn_recall_test,
      'optimal_threshold': 0.20,
      'model': trained_stacking_knn
  },
  'stacking_mice': {
      'f1_test': mice_f1_test,
      'precision_test': mice_precision_test,
      'recall_test': mice_recall_test,
      'optimal_threshold': 0.26,
      'model': trained_stacking_mice
  },
  'champion': champion,
  'best_f1': best_f1
}

globals()['final_stacking_results'] = final_stacking_results
print(f"\n💾 Résultats finaux sauvegardés dans 'final_stacking_results'")
print(f"🚀 Analyse complète terminée !")





from modules.config import cfg
from modules.modeling import run_stacking_no_refit

MODELS_DIR_NB2  = cfg.paths.models / "notebook2"
OUTPUT_DIR      = cfg.paths.models / "notebook3" / "stacking"

results_knn_no_refit  = run_stacking_no_refit(
    X_val_knn, y_val_knn, X_test_knn, y_test_knn,
    imputation_method="knn",
    models_dir=MODELS_DIR_NB2,
    output_dir=OUTPUT_DIR
)

results_mice_no_refit = run_stacking_no_refit(
    X_val_mice, y_val_mice, X_test_mice, y_test_mice,
    imputation_method="mice",
    models_dir=MODELS_DIR_NB2,
    output_dir=OUTPUT_DIR
)

# Résultats de F1 sur le jeu test
if results_knn_no_refit:
    print(f"KNN : F1 = {results_knn_no_refit['metrics'].get('f1_score_test'):.4f}")
if results_mice_no_refit:
    print(f"MICE : F1 = {results_mice_no_refit['metrics'].get('f1_score_test'):.4f}")






# --- 5. COMPARAISON DES MODÈLES ---

from modules.modeling import build_comparison_table 
from pathlib import Path


# --- Définir les chemins des fichiers JSON de résultats ---

json_paths = [
    cfg.paths.models / "notebook3" / "stacking" / "stacking_no_refit_knn_full.json",
    cfg.paths.models /"notebook3"  / "stacking" / "stacking_no_refit_mice_full.json",
    cfg.paths.models /"notebook3"  / "stacking"/ "stacking_with_refit_knn.json",
    cfg.paths.models /"notebook3" / "stacking"/ "stacking_with_refit_mice.json"
]

# --- Définir les détails pour l'affichage dans le tableau ---
# Les clés DOIVENT correspondre exactement aux noms des FICHIERS (ce qui suit le dernier '/')
details = {
    "stacking_no_refit_knn_full.json": {"Nom Affiché": "Stacking sans refit KNN", "Type": "Complet", "Imputation": "KNN"},
    "stacking_no_refit_mice_full.json": {"Nom Affiché": "Stacking sans refit MICE", "Type": "Complet", "Imputation": "MICE"},
    "stacking_with_refit_knn.json": {"Nom Affiché": "Stacking avec refit KNN", "Type": "Complet", "Imputation": "KNN"}, # Nom corrigé (enlever _full)
    "stacking_with_refit_mice.json": {"Nom Affiché": "Stacking avec refit MICE", "Type": "Complet", "Imputation": "MICE"} # Nom corrigé (enlever _full)
}



# --- Généreration et affichage du tableau de comparaison ---
try:
    df_comparison = build_comparison_table(json_paths, details)
    if not df_comparison.empty:
        print("\n TABLEAU DE COMPARAISON FINAL:")
        print("=" * 100) # Ajusté pour plus de colonnes

        # --- Version avec style coloré ---
        try:
            # Vérifier si on est dans un environnement qui supporte le HTML (comme Jupyter/Colab)
            from IPython.display import display, HTML
            import pandas as pd

            # Appliquer le style : dégradé sur la colonne F1-score (test)
            # 'background_gradient' colore les cellules. cmap='Blues'/'Greens'/'viridis' sont des options.
            # subset permet de spécifier les colonnes concernées.
            # axis=0 pour normaliser sur toute la colonne, axis=None pour normaliser sur tout le tableau.
            styled_df = df_comparison.style.background_gradient(
                cmap='Greens', # Choix du dégradé de couleur (GnBu, Blues, Greens, viridis, etc.)
                subset=['F1-score (test)'], # Colonnes sur lesquelles appliquer le style
                axis=0 # Normalisation par colonne
            ).format({ # Formater les colonnes numériques
                'F1-score (test)': "{:.4f}",
                'Précision (test)': "{:.4f}",
                'Rappel (test)': "{:.4f}",
                'Seuil utilisé': "{:.3f}"
            }) # On peut ajouter .set_properties(**{'text-align': 'center'}) pour centrer le texte

            display(styled_df) # Affichage plus joli et coloré dans Colab/Jupyter
            print("(💡 Le meilleur F1-score est mis en évidence par une couleur plus foncée)")

        except ImportError:
            # Fallback si IPython.display n'est pas disponible ou échoue
            print(df_comparison.to_string(index=False, float_format="%.4f")) # Affichage standard
            print("\n( Pour un affichage coloré, exécutez ce code dans Jupyter/Colab)")

    else:
        print("⚠️ Le tableau de comparaison est vide.")
        # Afficher les chemins tentés pour aider au debug
        print("Chemins tentés :")
        for p in json_paths:
            print(f" - {p}")
except Exception as e:
    print(f"❌ Erreur lors de la génération du tableau de comparaison: {e}")
    import traceback
    traceback.print_exc() # Affiche la pile d'appels pour aider au debug

print("\n✅ TOUTES LES ÉTAPES DE STACKING SONT TERMINÉES !")






from utils import generate_final_predictions

# Avec le modèle champion (GradBoost KNN Reduced)
#submission = generate_final_predictions(use_stacking=False)

# Ou avec le stacking
submission = generate_final_predictions(use_stacking=True)






