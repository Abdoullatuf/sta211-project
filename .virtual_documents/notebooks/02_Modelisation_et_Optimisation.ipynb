














import sys, os, logging
from pathlib import Path

# ‚îÄ‚îÄ 0. Logger clair (avec Rich si dispo)
try:
    from rich.logging import RichHandler
    logging.basicConfig(level="INFO",
                        format="%(message)s",
                        handlers=[RichHandler(rich_tracebacks=True, markup=True)],
                        force=True)
except ModuleNotFoundError:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s",
                        stream=sys.stdout,
                        force=True)
logger = logging.getLogger(__name__)

# ‚îÄ‚îÄ 1. D√©tection environnement Colab
def _in_colab() -> bool:
    try: import google.colab
    except ImportError: return False
    else: return True

# ‚îÄ‚îÄ 2. Montage Drive manuel rapide
if _in_colab():
    from google.colab import drive
    if not Path("/content/drive/MyDrive/Colab Notebooks").exists():
        logger.info("üîó Montage de Google Drive en cours‚Ä¶")
        drive.mount("/content/drive", force_remount=False)

# ‚îÄ‚îÄ 3. Localisation racine projet STA211
def find_project_root() -> Path:
    env_path = os.getenv("STA211_PROJECT_PATH")
    if env_path and (Path(env_path) / "modules").exists():
        return Path(env_path).expanduser().resolve()

    # Chemin Colab correct
    default_colab = Path("/content/drive/MyDrive/Colab Notebooks/projet_sta211_2025")
    if _in_colab() and (default_colab / "modules").exists():
        return default_colab.resolve()

    cwd = Path.cwd()
    for p in [cwd, *cwd.parents]:
        if (p / "modules").exists():
            return p.resolve()

    raise FileNotFoundError("‚ùå Impossible de localiser un dossier contenant 'modules/'.")

# ‚îÄ‚îÄ 4. D√©finition racine + PYTHONPATH
ROOT_DIR = find_project_root()
os.environ["STA211_PROJECT_PATH"] = str(ROOT_DIR)
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))
logger.info(f"üìÇ Racine projet d√©tect√©e : {ROOT_DIR}")
logger.info(f"PYTHONPATH ‚Üê {ROOT_DIR}")

# ‚îÄ‚îÄ 5. Initialisation de la configuration projet
from modules.config import cfg
cats = ['noad.', 'ad.']
LABEL_MAP = {0: "noad.", 1: "ad."} 


# ‚îÄ‚îÄ 6. Affichage des chemins configur√©s automatiquement
def display_paths(style: bool = True):
    import pandas as pd
    paths_dict = {
        "root": cfg.paths.root,
        "raw": cfg.paths.raw,
        "processed": cfg.paths.processed,
        "models": cfg.paths.models,
        "outputs": cfg.paths.outputs,
        "artifacts": cfg.paths.artifacts
    }
    rows = [{"Cl√©": k, "Chemin": str(v)} for k, v in paths_dict.items()]
    df = pd.DataFrame(rows).set_index("Cl√©")

    # V√©rification existence
    df["Existe"] = [
        "‚úÖ" if Path(v).exists() else "‚ùå"
        for v in paths_dict.values()
    ]

    from IPython.display import display
    display(df.style.set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]},
    ]) if style else df)

display_paths()
logger.info("‚úÖ Initialisation compl√®te r√©ussie - Notebook 02 pr√™t !")





## 0.2 ¬∑ Chargement des biblioth√®ques ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

from IPython.display import Markdown, display

# ‚¨áÔ∏è Imports directs des biblioth√®ques n√©cessaires
try:
    # Biblioth√®ques de base
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import matplotlib
    import seaborn as sns

    # Scikit-learn et extensions
    import sklearn
    from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.neural_network import MLPClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import (
        classification_report, confusion_matrix, roc_auc_score,
        precision_recall_curve, f1_score, precision_score, recall_score
    )

    # Imbalanced-learn pour le traitement du d√©s√©quilibre
    import imblearn
    from imblearn.over_sampling import BorderlineSMOTE
    from imblearn.pipeline import Pipeline as ImbPipeline

    # XGBoost
    import xgboost as xgb
    from xgboost import XGBClassifier

    # Utilitaires
    import joblib
    import json
    import warnings
    from tqdm import tqdm
    import scipy

    # Configuration des warnings
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=FutureWarning)

    logger.info("üìö Biblioth√®ques import√©es avec succ√®s")

except ImportError as e:
    logger.error(f"‚ùå Erreur d'importation : {e}")
    raise

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚úÖ Affichage des versions principales
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _safe_version(mod, fallback="‚Äî"):
    """Retourne mod.__version__ ou un fallback si le module est absent."""
    try:
        return mod.__version__
    except Exception:
        return fallback

def display_modeling_library_versions():
    mods = {
        "pandas"           : pd,
        "numpy"            : np,
        "scikit-learn"     : sklearn,
        "imbalanced-learn" : imblearn,
        "xgboost"          : xgb,
        "matplotlib"       : matplotlib,
        "seaborn"          : sns,
        "scipy"            : scipy,
        "joblib"           : joblib,
        "tqdm"             : __import__("tqdm"),
        "ipython"          : __import__("IPython")
    }
    versions_md = "\n".join(f"- `{k}` : {_safe_version(v)}" for k, v in mods.items())
    display(Markdown(f"‚úÖ Versions des biblioth√®ques de mod√©lisation\n{versions_md}"))

display_modeling_library_versions()
logger.info("‚úÖ Chargement des biblioth√®ques termin√©")





import gc
from pathlib import Path
from IPython.display import display, Markdown

try:
    import ipywidgets as wd
    WIDGETS_AVAILABLE = True
    print("‚úÖ ipywidgets disponible - Interface interactive activ√©e")
except ImportError:
    WIDGETS_AVAILABLE = False
    print("‚ö†Ô∏è ipywidgets non disponible - Mode standard activ√©")


# ‚úÖ CORRECTION : Utiliser cfg.paths au lieu de paths
data_dir = cfg.paths.processed  / "final_data_for_modeling"

expected_names = {
    "mice_no_outliers",
    "knn_no_outliers",
    "mice_with_outliers",
    "knn_with_outliers"
}

available_files = [f for f in data_dir.glob("df_final_for_modeling_*.csv")
                   if f.stem.replace("df_final_for_modeling_", "") in expected_names]

NAMES = [f.stem.replace("df_final_for_modeling_", "") for f in available_files]

print(f"\nüìÇ {len(NAMES)} datasets d√©tect√©s : {NAMES}")

def _csv_path(name: str) -> Path:
    return data_dir / f"df_final_for_modeling_{name}.csv"

# Distribution cible renomm√©e outcome
if NAMES:
    print("\n Distribution cible (outcome) dans chaque fichier")
    for n in NAMES:
        fp = _csv_path(n)
        try:
            df_temp = pd.read_csv(fp)
            s = df_temp["outcome"] if "outcome" in df_temp.columns else df_temp["y"]

            pct = s.map(LABEL_MAP).value_counts(normalize=True).mul(100).round(2)
            print(f"{n:<20} | ad.: {pct.get('ad.', 0):>5.1f}% | noad.: {pct.get('noad.', 0):>5.1f}%")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lecture {fp.name} ‚Üí {e}")
else:
    print("‚ùå Aucun dataset CSV trouv√© dans :", data_dir)

def load_dataset(name: str) -> pd.DataFrame | None:
    fp = _csv_path(name)
    try:
        df = pd.read_csv(fp)
        if "y" in df.columns:
            df.rename(columns={"y": "outcome"}, inplace=True)
        print(f"‚úÖ {name} charg√© : {df.shape}")
        return df
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        return None

if NAMES:
    if WIDGETS_AVAILABLE:
        dropdown = wd.Dropdown(options=NAMES, value=NAMES[0], description="Dataset :")
        out = wd.Output()

        def _on_change(change):
            with out:
                out.clear_output()
                df = load_dataset(change["new"])
                if df is not None:
                    display(df.head())

        dropdown.observe(_on_change, names="value")
        display(Markdown(" S√©lectionnez un dataset :"))
        display(dropdown, out)
    else:
        print("\n Mode standard :")
        for i, name in enumerate(NAMES, 1):
            print(f"   {i}. {name}")
        print(" Utilisez load_dataset('nom_du_dataset') pour charger un dataset")
else:
    print(" Aucune donn√©e √† charger. V√©rifiez les fichiers dans :", data_dir)

print("\n Nettoyage m√©moire...")
gc.collect()
print("‚úÖ Initialisation des datasets termin√©e !")





print("\n Chargement des datasets principaux...")
print("‚îÄ" * 55)

df_knn  = load_dataset("knn_no_outliers")
df_mice = load_dataset("mice_no_outliers")





# ## R√©partition de la variable cible (ad. vs noad.) ‚Äî version courte

cats = ['noad.', 'ad.']

def prep(df):
    s = pd.Series(df['outcome']).map(LABEL_MAP)
    return df.assign(outcome_label=pd.Categorical(s, categories=cats, ordered=True))

def add_perc(ax, total):
    if not ax.patches: return
    ymax = max(p.get_height() for p in ax.patches)
    ax.set_ylim(0, ymax * 1.08)
    for p in ax.patches:
        ax.text(p.get_x()+p.get_width()/2, p.get_height()+ymax*0.02,
                f'{100*p.get_height()/total:.1f}%', ha='center', va='bottom', fontsize=10)

dfs = {'KNN': prep(df_knn), 'MICE': prep(df_mice)}

fig, axes = plt.subplots(1, 2, figsize=(8, 4))
for ax, (title, dfx) in zip(axes, dfs.items()):
    sns.countplot(data=dfx, x='outcome_label', order=cats, palette='Set2', ax=ax)
    ax.set_title(title); ax.set_xlabel(''); ax.set_ylabel('Nombre d\'observations')
    add_perc(ax, len(dfx))

fig.suptitle("R√©partition de la variable cible (KNN vs MICE)", fontsize=14, fontweight='bold')
plt.tight_layout(); plt.subplots_adjust(top=0.85)

fig_path = cfg.paths.outputs / "figures" / "notebook2" / "target_distribution_knn-mice_comparison.png"
fig_path.parent.mkdir(parents=True, exist_ok=True)

fig.savefig(fig_path, dpi=150, bbox_inches='tight'); plt.show()

print("üìä Statistiques d√©taill√©es :\n" + "-"*50)
for name, dfx in dfs.items():
    vc = dfx['outcome_label'].value_counts(dropna=False)
    total = vc.sum(); ads = vc.get('ad.', 0); noads = vc.get('noad.', 0)
    print(f"{name:<20} | Total: {total:>4} | Ads: {ads:>3} ({ads/total:>5.1%}) | No-ads: {noads:>4} ({noads/total:>5.1%})")

logger.info(f"üìä Graphe comparatif sauvegard√© ‚Üí {fig_path.name}")












# Split des donn√©es (train ‚Äî validation ‚Äî test) <a id="2-split-des-donn√©es-train--validation--test"></a>
from sklearn.model_selection import train_test_split

# ‚úÖ Import du nouveau syst√®me
from modules.utils import save_artifact

log = logging.getLogger(__name__)
RANDOM_STATE = 42

TARGET = "outcome"                

def tri_split_and_save(df: pd.DataFrame, name: str):
    """
    ‚Ä¢ Split stratifi√© 60 / 20 / 20  (train / val / test) 
    ‚Ä¢ Sauvegarde columns + trois pickles dans le dossier sp√©cifique (knn ou mice)
    ‚Ä¢ Retourne un dict des sous-ensembles
    """
    if TARGET not in df.columns:
        raise KeyError(f"La colonne cible ¬´ {TARGET} ¬ª est absente de {name}.")

    
    save_dir = cfg.paths.models / "notebook2" / name
    save_dir.mkdir(parents=True, exist_ok=True)

    X = df.drop(columns=[TARGET])
    y = df[TARGET].astype(int)

    # 80 / 20  ‚ûú  temp / test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE
    )
    # 75 / 25 du temp ‚ûú train / val  ‚áí 60 / 20 / 20 global
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE
    )

    cols = X_train.columns.tolist()

    # ‚úÖ On a Remplac√© joblib par save_artifact
    save_artifact(cols, f"columns_{name}.pkl", save_dir)
    save_artifact({"X": X_train, "y": y_train}, f"{name}_train.pkl", save_dir)
    save_artifact({"X": X_val,   "y": y_val},   f"{name}_val.pkl", save_dir)
    save_artifact({"X": X_test,  "y": y_test},  f"{name}_test.pkl", save_dir)

    log.info(f"{name.upper():<4} | split 60/20/20 sauvegard√© dans {save_dir} (cols = {len(cols)})")

    return dict(X_train=X_train, y_train=y_train,
                X_val=X_val,     y_val=y_val,
                X_test=X_test,   y_test=y_test)

# ‚îÄ‚îÄ Cr√©ation pour KNN & MICE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
splits = {
    "knn":  tri_split_and_save(df_knn,  "knn"),
    "mice": tri_split_and_save(df_mice, "mice"),
}

# ‚îÄ‚îÄ R√©sum√© visuel ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def split_summary(spl):
    lines = []
    for name, d in spl.items():
        lines.append(
            f"- **{name.upper()}** : "
            f"train {d['X_train'].shape} (ads {d['y_train'].mean():.2%}) ¬∑ "
            f"val {d['X_val'].shape} (ads {d['y_val'].mean():.2%}) ¬∑ "
            f"test {d['X_test'].shape} (ads {d['y_test'].mean():.2%})"
        )
    display(Markdown(" D√©coupage 60 / 20 / 20 (cible = outcome)\n" + "\n".join(lines)))

split_summary(splits)


# --- Sauvegarde de l'imputeur par la m√©diane pour la colonne X4 ---

print(" Sauvegarde de la m√©diane pour l'imputation de X4...")


models_dir = cfg.paths.models / "notebook2"
models_dir.mkdir(parents=True, exist_ok=True) # Cr√©e le dossier s'il n'existe pas

try:
    if "X4" not in splits['mice']['X_train']:
        raise KeyError("Colonne 'X4' absente de X_train.")


    median_x4_train = splits['mice']['X_train']['X4'].median()

    # 2. D√©finir le chemin de sauvegarde
    save_path = models_dir / "median_imputer_X4.pkl"

    # 3. Sauvegarder la valeur de la m√©diane avec joblib
    save_artifact(median_x4_train, "median_imputer_X4.pkl", models_dir)

    print(f" M√©diane de X4 ({median_x4_train:.4f}) sauvegard√©e dans : {save_path}")

except KeyError:
    print("‚ùå ERREUR : La colonne 'X4' n'a pas √©t√© trouv√©e dans le jeu d'entra√Ænement.")
except Exception as e:
    print(f"‚ùå Une erreur inattendue est survenue : {e}")





from imblearn.pipeline import Pipeline 

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import BorderlineSMOTE



def to_cat_y(y):
    s = pd.Series(y, name='y').map(LABEL_MAP)
    return pd.Categorical(s, categories=cats, ordered=True)

def add_perc(ax, total):
    if not ax.patches: return
    ymax = max(p.get_height() for p in ax.patches)
    ax.set_ylim(0, ymax * 1.08)
    for p in ax.patches:
        ax.text(p.get_x()+p.get_width()/2, p.get_height()+ymax*0.02,
                f'{100*p.get_height()/total:.1f}%', ha='center', va='bottom', fontsize=9)

# Donn√©es brutes
X_raw = splits["knn"]["X_train"]
y_raw = splits["knn"]["y_train"]

# Avant SMOTE
y_before = to_cat_y(y_raw)

# Pipeline + BorderlineSMOTE
pipe = Pipeline([
    ("imp",   SimpleImputer(strategy="median")),
    ("scale", StandardScaler(with_mean=False)),
    ("smote", BorderlineSMOTE(sampling_strategy=0.8, random_state=RANDOM_STATE))
])
X_res, y_res = pipe.fit_resample(X_raw, y_raw)

# Apr√®s SMOTE
y_after = to_cat_y(y_res)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(6, 3.5), sharey=True)
sns.countplot(x=y_before, order=cats, palette="Set2", ax=axes[0])
axes[0].set_title("Avant SMOTE"); axes[0].set_xlabel(""); axes[0].set_ylabel("Observations")
add_perc(axes[0], len(y_before))

sns.countplot(x=y_after, order=cats, palette="Set2", ax=axes[1])
axes[1].set_title("Apr√®s BorderlineSMOTE"); axes[1].set_xlabel(""); axes[1].set_ylabel("")
add_perc(axes[1], len(y_after))

fig.suptitle("Distribution avant / apr√®s SMOTE (KNN)", fontsize=12, fontweight="bold")
plt.tight_layout(); plt.subplots_adjust(top=0.88)

fig_path = cfg.paths.outputs / "figures" / "notebook2" / "smote_effect_knn.png"
fig.savefig(fig_path, dpi=150, bbox_inches="tight"); plt.show()

# Stats rapides
print(" Avant SMOTE :", dict(pd.Series(y_before).value_counts()))
print(" Apr√®s SMOTE :", dict(pd.Series(y_after).value_counts()))

logger.info(f" Graphe SMOTE sauvegard√© ‚Üí {fig_path.name}")






#from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import json, logging

log = logging.getLogger(__name__)
RANDOM_STATE = 42


# === GRILLES ANTI-OVERFITTING ===
# Objectif : R√©duire l'√©cart validation/test (0.9394 ‚Üí 0.88)

# 1. Gradient Boosting
gradboost_params = {
    "clf__n_estimators": [50, 100, 200],
    "clf__max_depth": [3, 4, 5],
    "clf__learning_rate": [0.05, 0.1],
    "clf__min_samples_split": [10, 20],
    "clf__min_samples_leaf": [5, 10],
    "clf__subsample": [0.7, 0.8]
}

# 2. Random Forest - BEAUCOUP PLUS CONSERVATEUR
rf_params = {
    "clf__n_estimators": [50, 100, 200],    # Moins d'arbres
    "clf__max_depth": [5, 10, 15],          # Profondeur limit√©e (vs None)
    "clf__min_samples_split": [10, 20],     # Plus strict (vs 2)
    "clf__min_samples_leaf": [5, 10],       # Feuilles plus grosses (vs 1)
    "clf__max_features": ["sqrt", "log2"],   # Retrait "None" qui overfitte
}

# 3. SVM - Plus de r√©gularisation
svc_params = {
    "clf__C": [0.1, 1, 10],                # Moins de valeurs extr√™mes
    "clf__kernel": ["rbf", "linear"],       # Retrait poly (complexe)
    "clf__gamma": ["scale", "auto"],        # Retrait valeurs num√©riques
}

# 4. MLP - Architectures plus simples
mlp_params = {
    "clf__hidden_layer_sizes": [(50,), (100,)],  # Plus simple
    "clf__activation": ["relu", "tanh"],          # Retrait logistic
    "clf__alpha": [1e-3, 1e-2, 1e-1],           # Plus de r√©gularisation
    "clf__learning_rate": ["constant", "adaptive"],
    "clf__solver": ["adam"],                      # Plus stable que sgd
}

# 5. XGBoost - R√©gularisation renforc√©e
xgb_params = {
    "clf__n_estimators": [50, 100, 200],        #  Moins d'arbres
    "clf__max_depth": [3, 4, 5],                # Moins profond (vs 8)
    "clf__learning_rate": [0.05, 0.1, 0.15],    # Plus conservateur
    "clf__subsample": [0.7, 0.8],               # Sous-√©chantillonnage
    "clf__colsample_bytree": [0.7, 0.8],        # Idem pour features
    "clf__reg_lambda": [1, 5, 10],              # üîß Plus de r√©gularisation L2
    "clf__reg_alpha": [0, 1, 5],                # Ajout r√©gularisation L1
    "clf__scale_pos_weight": [1, 5, 10],        # Pour d√©s√©quilibre
}

# Dictionnaire mis √† jour (sans Decision Tree et KNN)
param_grids = {
    "GradBoost": gradboost_params,
    "RandForest": rf_params,
    "SVM": svc_params,
    "MLP": mlp_params,
    "XGBoost": xgb_params,
}

# Sauvegarde pour r√©f√©rence
save_artifact(param_grids, "hyperparam_grids.json", cfg.paths.models)
log.info("‚úÖ Grilles d'hyper-param√®tres anti-overfitting sauvegard√©es")


# dictionnaire des mod√®les de base (sans class_weight : SMOTE s'en charge)
models = {
    "GradBoost": GradientBoostingClassifier(random_state=RANDOM_STATE),
    "RandForest": RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),
    "SVM": SVC(probability=True, random_state=RANDOM_STATE),
    "MLP": MLPClassifier(max_iter=500, random_state=RANDOM_STATE),
    "XGBoost": xgb.XGBClassifier(
        use_label_encoder=False, eval_metric="logloss",
        random_state=RANDOM_STATE, n_jobs=-1),
}





# === IMPORT DES FONCTIONS RFECV ===
from modules.notebook2_modeling import perform_rfecv_selection, plot_rfecv_results

# === EX√âCUTION POUR KNN ET MICE ===
rfecv_results = {}

for method in ["knn", "mice"]:
    rfecv_results[method] = perform_rfecv_selection(
        splits[method],
        method,
        cfg.paths.models
    )
    plot_rfecv_results(rfecv_results[method], method)
    print("-" * 50)


# === SAUVEGARDE DES DATASETS R√âDUITS ===
from modules.notebook2_modeling import save_reduced_datasets

# Sauvegarde pour KNN et MICE en une seule fois
save_reduced_datasets(splits, rfecv_results, cfg.paths.models)








# === VALIDATION CROIS√âE RENFORC√âE + FONCTIONS D'√âVALUATION ===
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
  f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix
)
# ‚úÖ IMPORT DU SYST√àME DE STOCKAGE UNIFI√â
from modules.utils import save_artifact

log = logging.getLogger(__name__)

# Validation crois√©e renforc√©e (10-fold au lieu de 5)
CV = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)

# üîß Suppression des warnings XGBoost
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')

# === OBJECTIF CIBLE ===
# R√©duire le gap validation/test de 0.06 √† < 0.02
# Score test cible : > 0.90 (au lieu de 0.88)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

## Pipeline unifi√© (imputation de s√©curit√©) + scale + BorderlineSMOTE + mod√®le)

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import BorderlineSMOTE
from sklearn.preprocessing import StandardScaler

def make_pipeline(model, sampling_ratio=0.8):
  """
  Pipeline complet :
    StandardScaler ‚ûú BorderlineSMOTE ‚ûú mod√®le
  """
  return Pipeline(steps=[
      ("scale",  StandardScaler(with_mean=False)),
      ("smote",  BorderlineSMOTE(
          sampling_strategy=sampling_ratio,
          random_state=RANDOM_STATE,
          kind="borderline-1")),
      ("clf",    model),
  ])


def _plot_cm_with_metrics(y_true, y_pred, y_proba,
                        title="Confusion Matrix", plot=True) -> dict:
  """Affiche (optionnel) une matrice compacte + m√©triques, retourne les scores."""
  f1_bin = f1_score(y_true, y_pred, zero_division=0)
  prec   = precision_score(y_true, y_pred, zero_division=0)
  rec    = recall_score(y_true, y_pred, zero_division=0)
  auc    = roc_auc_score(y_true, y_proba) if y_proba is not None else None

  if plot:
      cm = confusion_matrix(y_true, y_pred)
      plt.figure(figsize=(4, 3))
      sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
      subtitle = f"F1={f1_bin:.3f} ¬∑ P={prec:.3f} ¬∑ R={rec:.3f}"
      if auc is not None:
          subtitle += f" ¬∑ AUC={auc:.3f}"
      plt.title(f"{title}\n{subtitle}", pad=20)
      plt.xlabel("Pr√©dit"); plt.ylabel("R√©el")
      plt.tight_layout(); plt.show()

  return dict(f1_binary=f1_bin, precision=prec, recall=rec, auc=auc)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def evaluate_model(name: str, base_model, param_grid: dict,
                 X_train, y_train, X_val, y_val, dataset: str,
                 plot_cm: bool = True):
  """
  GridSearchCV ‚Üí meilleur pipeline.

  Param√®tres:
  -----------
  X_train, y_train : Donn√©es d'entra√Ænement (60%) pour GridSearchCV
  X_val, y_val     : Donn√©es de validation (20%) pour √©valuation finale

  Retourne:
  ---------
  (pipeline, m√©triques) avec surveillance de l'overfitting
  """
  pipe = make_pipeline(base_model)
  gs   = GridSearchCV(pipe, param_grid, scoring="f1",
                      cv=CV, n_jobs=-1, verbose=0).fit(X_train, y_train)

  best   = gs.best_estimator_
  y_pred = best.predict(X_val)
  y_prob = best.predict_proba(X_val)[:, 1] if hasattr(best, "predict_proba") else None

  metrics = _plot_cm_with_metrics(
      y_val, y_pred, y_prob,
      title=f"{name} ({dataset})", plot=plot_cm
  )

  # üîß Calcul de l'√©cart CV/validation pour surveiller l'overfitting
  f1_cv = gs.best_score_          # Score moyen de la validation crois√©e
  f1_val = metrics["f1_binary"]   # Score sur le jeu de validation
  gap = f1_cv - f1_val

  metrics.update({
      "model": name,
      "dataset": dataset,
      "f1_weighted": f1_score(y_val, y_pred, average="weighted"),
      "best_params": gs.best_params_,
      "f1_cv": round(f1_cv, 4),      # Score validation crois√©e
      "f1_val": round(f1_val, 4),    # Score validation finale
      "gap": round(gap, 4),          # √âcart CV/validation (indicateur overfitting)
  })

  # üîß Affichage enrichi avec surveillance de l'overfitting
  print(f" {name} ({dataset}): F1_CV={f1_cv:.3f}, F1_VAL={f1_val:.3f}, Gap={gap:.3f}")
  if gap > 0.05:
      print(f"‚ö†Ô∏è  Overfitting d√©tect√© (gap > 0.05)")
  elif gap > 0.03:
      print(f"üî∂ L√©ger overfitting (gap > 0.03)")
  else:
      print(f"‚úÖ Bonne g√©n√©ralisation (gap ‚â§ 0.03)")

  # ‚úÖ CORRECTION: Utiliser save_artifact au lieu de joblib + chemins corrig√©s
  models_dir = cfg.paths.models / "notebook2"
  save_artifact(best, f"best_{name.lower()}_{dataset}.pkl", models_dir)
  save_artifact(gs.best_params_, f"best_params_{name.lower()}_{dataset}.json", models_dir)

  return best, metrics

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def evaluate_all(models_dict, grids_dict,
               X_train, y_train, X_val, y_val, dataset: str,
               plot_cm: bool = True) -> tuple[pd.DataFrame, dict]:
  """
  Boucle d'√©valuation sur tous les mod√®les.

  Param√®tres:
  -----------
  X_train, y_train : Donn√©es d'entra√Ænement (60%)
  X_val, y_val     : Donn√©es de validation (20%)

  Retourne:
  ---------
  (DataFrame des r√©sultats, dict des meilleurs pipelines)
  """
  records, best_pipelines = [], {}

  print(f"üîÑ √âvaluation sur {dataset.upper()} avec CV={CV.n_splits}-fold")
  print("=" * 60)
  print("üìà TRAIN ‚Üí VAL (s√©lection des mod√®les)")
  print("-" * 60)

  for name, base_model in models_dict.items():
      print(f"üîç {name} ({dataset})")
      best, rec = evaluate_model(
          name, base_model, grids_dict[name],
          X_train, y_train, X_val, y_val, dataset,
          plot_cm=plot_cm
      )
      records.append(rec)
      best_pipelines[name] = best

  df = (pd.DataFrame(records)
          .sort_values("f1_val", ascending=False)  # Tri par F1 validation
          .reset_index(drop=True))

  # Affichage enrichi avec m√©triques d'overfitting
  fmt = {c: "{:.4f}" for c in ["f1_val", "precision", "recall", "auc", "f1_cv", "gap"] if c in df}
  display(df.style.format(fmt, na_rep="‚Äî")
                .background_gradient(subset=["f1_val"], cmap="Greens")
                .background_gradient(subset=["gap"], cmap="Reds_r"))  # Rouge pour gap √©lev√©

  # R√©sum√© de l'overfitting
  print(f"\nüìä R√âSUM√â DE L'OVERFITTING ({dataset.upper()}):")
  print("-" * 50)
  for _, row in df.iterrows():
      gap = row.get('gap', 0)
      if gap > 0.05:
          status = "‚ö†Ô∏è  OVERFITTING"
      elif gap > 0.03:
          status = "üî∂ L√âGER OVERFITTING"
      else:
          status = "‚úÖ BONNE G√âN√âRALISATION"
      print(f"  {row['model']:<12}: gap={gap:+.3f} ‚Üí {status}")

  print(f"\n Champion {dataset.upper()}: {df.iloc[0]['model']} (F1_VAL={df.iloc[0]['f1_val']:.4f})")

  return df, best_pipelines





# === CHARGEMENT DES DONN√âES COMPL√àTES ET R√âDUITES ===

# ‚úÖ Chemins corrig√©s
knn_dir = cfg.paths.models / "notebook2" / "knn"
mice_dir = cfg.paths.models / "notebook2" / "mice"

# Compl√®tes (d√©j√† charg√©es pr√©c√©demment)
splits_knn = splits["knn"]
splits_mice = splits["mice"]

# ‚úÖ 6 splits: (X_train, y_train, X_val, y_val, X_test, y_test)
from modules.utils import load_artifact

splits_knn_reduced = {
  f"{prefix}_{subset}": load_artifact(f"{prefix}_{subset}_reduced.pkl", knn_dir / "reduced")
  for subset in ["train", "val", "test"]
  for prefix in ["X", "y"]
}

splits_mice_reduced = {
  f"{prefix}_{subset}": load_artifact(f"{prefix}_{subset}_reduced.pkl", mice_dir / "reduced")
  for subset in ["train", "val", "test"]
  for prefix in ["X", "y"]
}

print(f"‚úÖ Donn√©es KNN compl√®tes : {len(splits_knn)} splits")
print(f"‚úÖ Donn√©es KNN r√©duites : {len(splits_knn_reduced)} splits")
print(f"‚úÖ Donn√©es MICE compl√®tes : {len(splits_mice)} splits")
print(f"‚úÖ Donn√©es MICE r√©duites : {len(splits_mice_reduced)} splits")

# ‚úÖ V√©rification de la structure coh√©rente
print(f"\n Cl√©s KNN r√©duites : {list(splits_knn_reduced.keys())}")
print(f" Cl√©s MICE r√©duites : {list(splits_mice_reduced.keys())}")


# KNN - R√âDUIT
results_knn_reduced, best_knn_reduced = evaluate_all(
  models, param_grids,
  splits_knn_reduced["X_train"], splits_knn_reduced["y_train"],
  splits_knn_reduced["X_val"], splits_knn_reduced["y_val"],
  dataset="knn_reduced"
)


# MICE - R√âDUIT
results_mice_reduced, best_mice_reduced = evaluate_all(
  models, param_grids,
  splits_mice_reduced["X_train"], splits_mice_reduced["y_train"],
  splits_mice_reduced["X_val"], splits_mice_reduced["y_val"],
  dataset="mice_reduced"
)


# KNN - COMPLET
results_knn_full, best_knn_full = evaluate_all(
    models, param_grids,
    splits_knn["X_train"], splits_knn["y_train"],
    splits_knn["X_val"], splits_knn["y_val"],
    dataset="knn_full"
)




# MICE - COMPLET
results_mice_full, best_mice_full = evaluate_all(
    models, param_grids,
    splits_mice["X_train"], splits_mice["y_train"],
    splits_mice["X_val"], splits_mice["y_val"],
    dataset="mice_full"
)



df_global_results = pd.concat([
    results_knn_full.assign(version="knn_full"),
    results_knn_reduced.assign(version="knn_reduced"),
    results_mice_full.assign(version="mice_full"),
    results_mice_reduced.assign(version="mice_reduced")
], ignore_index=True)

# Tri√© selon le score F1 validation
df_global_results.sort_values(by="f1_val", ascending=False).reset_index(drop=True).head()

df_global_results.to_csv(cfg.paths.models / "notebook2" / "eval_all_models_summary.csv", index=False)




print("\n Sauvegarde des r√©sultats globaux d'√©valuation")
print("=" * 60)

# Mapping des r√©sultats
results_dict = {
  "knn_full": results_knn_full,
  "knn_reduced": results_knn_reduced,
  "mice_full": results_mice_full,
  "mice_reduced": results_mice_reduced,
}

# Mapping des meilleurs pipelines
pipelines_dict = {
  "knn_full": best_knn_full,
  "knn_reduced": best_knn_reduced,
  "mice_full": best_mice_full,
  "mice_reduced": best_mice_reduced,
}

# R√©pertoire commun
eval_dir = cfg.paths.models / "notebook2"
eval_dir.mkdir(parents=True, exist_ok=True)

# ‚úÖ Utilisons save_artifact pour les sauvegardes
from modules.utils import save_artifact

# Sauvegardes CSV, JSON, pipelines
for name, df_result in results_dict.items():
  # R√©sultats validation (CSV et JSON)
  df_result.to_csv(eval_dir / f"eval_{name}_val_scores.csv", index=False)
  save_artifact(df_result.to_dict("records"), f"eval_{name}_val_scores.json", eval_dir)

  # ‚úÖ CORRECTION: Pipelines avec save_artifact au lieu de joblib
  best_pipes = pipelines_dict[name]
  for model_name, pipeline in best_pipes.items():
      pipeline_filename = f"pipeline_{model_name.lower()}_{name}.pkl"
      save_artifact(pipeline, pipeline_filename, eval_dir)
      print(f"‚úÖ Pipeline sauvegard√© : {pipeline_filename}")

  # ‚úÖ Dictionnaire des chemins avec save_artifact
  pipelines_paths = {
      k: str(eval_dir / f"pipeline_{k.lower()}_{name}.pkl")
      for k in best_pipes
  }
  save_artifact(pipelines_paths, f"best_{name}_pipelines.json", eval_dir)

print("\n‚úÖ Toutes les √©valuations et pipelines ont √©t√© sauvegard√©s avec le syst√®me unifi√©.")


  # === RECHARGEMENT DES R√âSULTATS SAUVEGARD√âS ===
  from modules.utils import load_artifact
  import pandas as pd

  eval_dir = cfg.paths.models / "notebook2"

  # Rechargement des DataFrames de r√©sultats
  print("üìÇ Rechargement des r√©sultats d'√©valuation...")

  results_knn_full = pd.DataFrame(load_artifact("eval_knn_full_val_scores.json", eval_dir))
  results_knn_reduced = pd.DataFrame(load_artifact("eval_knn_reduced_val_scores.json", eval_dir))
  results_mice_full = pd.DataFrame(load_artifact("eval_mice_full_val_scores.json", eval_dir))
  results_mice_reduced = pd.DataFrame(load_artifact("eval_mice_reduced_val_scores.json", eval_dir))

  print("‚úÖ Tous les r√©sultats recharg√©s avec succ√®s")
  print(f"  ‚Ä¢ KNN Full: {len(results_knn_full)} mod√®les")
  print(f"  ‚Ä¢ KNN Reduced: {len(results_knn_reduced)} mod√®les")
  print(f"  ‚Ä¢ MICE Full: {len(results_mice_full)} mod√®les")
  print(f"  ‚Ä¢ MICE Reduced: {len(results_mice_reduced)} mod√®les")


# Synth√®se des performances KNN vs MICE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚úÖ IMPORT DES FONCTIONS DU MODULE
from modules.notebook2_modeling import create_comparison_table

# ‚îÄ‚îÄ 1. Cr√©ation du tableau de comparaison avec les fonctions du module ‚îÄ‚îÄ
results_dict = {
  "knn_full": results_knn_full,
  "knn_reduced": results_knn_reduced,
  "mice_full": results_mice_full,
  "mice_reduced": results_mice_reduced,
}

# ‚úÖ Utilisation de la fonction du module + fixation du chemin
eval_dir = cfg.paths.models / "notebook2"
results_comparison = create_comparison_table(results_dict, save_dir=eval_dir)

# ‚îÄ‚îÄ 2. Affichage stylis√© ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
display(
  results_comparison
    .style
    .background_gradient(subset=["F1-score"], cmap="Greens", vmin=0.80, vmax=0.94)
    .background_gradient(subset=["Gap CV/Val"], cmap="Reds_r", vmin=-0.05, vmax=0.05)
    .background_gradient(subset=["AUC"], cmap="Blues", vmin=0.95, vmax=0.98)
    .format({
        "F1-score": "{:.4f}", "Pr√©cision": "{:.4f}",
        "Rappel":   "{:.4f}", "AUC": "{:.4f}",
        "Gap CV/Val": "{:+.3f}"
    }, na_rep="‚Äî")
    .highlight_max(subset=["F1-score"], color="#006400",
                   props="color: white; font-weight: bold; border: 2px solid #004d00")
    .highlight_min(subset=["Gap CV/Val"], color="#000080",
                   props="color: white; font-weight: bold; border: 2px solid #000060")
)

# ‚îÄ‚îÄ 3. L√©gende et statistiques (reste identique) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("\n L√âGENDE DES COULEURS :")
print("F1-score : Vert (plus fonc√© = meilleur)")
print("Gap CV/Val : Rouge (plus fonc√© = overfitting)")
print("AUC : Bleu (plus fonc√© = meilleur)")
print("Meilleur F1-score : Surlign√© en vert fonc√©")
print("Meilleur gap : Surlign√© en bleu fonc√©")

print(f"\nüìä STATISTIQUES RAPIDES :")
print(f"Meilleur F1-score : {results_comparison['F1-score'].max():.4f}")
print(f"F1-score moyen : {results_comparison['F1-score'].mean():.4f}")
print(f"Gap moyen : {results_comparison['Gap CV/Val'].mean():.3f}")
print(f"‚úÖ Mod√®les avec gap ‚â§ 0.03 : {(results_comparison['Gap CV/Val'].abs() <=0.03).sum()}/{len(results_comparison)}")

# ‚îÄ‚îÄ 4. R√©sum√© final ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
champion_model = results_comparison.iloc[0]['Mod√®le']
champion_imputation = results_comparison.iloc[0]["M√©thode d'imputation"]
champion_f1 = results_comparison.iloc[0]['F1-score']
champion_gap = results_comparison.iloc[0]['Gap CV/Val']

print("\n CONTEXTE : Performances sur jeu de VALIDATION (20%)")
print(" Phase suivante : √âvaluation finale sur jeu de TEST (20%)")
print(f" Champion provisoire : {champion_model} + {champion_imputation}")
print(f"    F1-score = {champion_f1:.4f}, Gap = {champion_gap:+.3f}")


# !pip install scikit-optimize --quiet









from modules.modeling import (
  optimize_threshold,
  optimize_multiple,
  save_optimized_thresholds,
  load_best_pipelines
)






# === CHARGEMENT DES PIPELINES AVEC LOAD_ARTIFACT ===
from modules.utils import load_artifact

eval_dir = cfg.paths.models / "notebook2"


print("üìÇ Chargement des pipelines sauvegard√©s...")

# Chargement via les fichiers JSON des chemins
knn_full_paths = load_artifact("best_knn_full_pipelines.json", eval_dir)
knn_reduced_paths = load_artifact("best_knn_reduced_pipelines.json", eval_dir)
mice_full_paths = load_artifact("best_mice_full_pipelines.json", eval_dir)
mice_reduced_paths = load_artifact("best_mice_reduced_pipelines.json", eval_dir)

# Chargement des pipelines r√©els
best_knn_full_pipes = {
  model: load_artifact(f"pipeline_{model.lower()}_knn_full.pkl", eval_dir)
  for model in knn_full_paths.keys()
}

best_knn_reduced_pipes = {
  model: load_artifact(f"pipeline_{model.lower()}_knn_reduced.pkl", eval_dir)
  for model in knn_reduced_paths.keys()
}

best_mice_full_pipes = {
  model: load_artifact(f"pipeline_{model.lower()}_mice_full.pkl", eval_dir)
  for model in mice_full_paths.keys()
}

best_mice_reduced_pipes = {
  model: load_artifact(f"pipeline_{model.lower()}_mice_reduced.pkl", eval_dir)
  for model in mice_reduced_paths.keys()
}

print("‚úÖ Tous les pipelines recharg√©s avec succ√®s")
print(f"  ‚Ä¢ KNN Full: {list(best_knn_full_pipes.keys())}")
print(f"  ‚Ä¢ KNN Reduced: {list(best_knn_reduced_pipes.keys())}")
print(f"  ‚Ä¢ MICE Full: {list(best_mice_full_pipes.keys())}")
print(f"  ‚Ä¢ MICE Reduced: {list(best_mice_reduced_pipes.keys())}")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Optimisation des seuils pour toutes les combinaisons : KNN/MICE √ó Full/Reduced
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

from modules.modeling import optimize_all_thresholds


THRESHOLDS_DIR = cfg.paths.outputs / "modeling" / "thresholds"

if not THRESHOLDS_DIR.exists():
  THRESHOLDS_DIR.mkdir(parents=True)

# Pipelines (recharg√©s pr√©c√©demment)
all_pipelines = {
  "knn_full":    best_knn_full_pipes,
  "knn_reduced": best_knn_reduced_pipes,
  "mice_full":   best_mice_full_pipes,
  "mice_reduced": best_mice_reduced_pipes,
}

# ‚úÖ CORRECTION 2: V√©rifier la coh√©rence des noms de variables
# Si vous avez utilis√© la nouvelle structure de chargement
all_splits = {
  "knn_full":    splits_knn,           # splits["knn"]
  "knn_reduced": splits_knn_reduced,   # Structure 6 splits
  "mice_full":   splits_mice,          # splits["mice"]
  "mice_reduced": splits_mice_reduced, # Structure 6 splits
}

# ‚úÖ CORRECTION 3: Validation avant ex√©cution
print("üîÑ Validation des donn√©es avant optimisation des seuils...")
for name, pipelines in all_pipelines.items():
  splits = all_splits[name]
  print(f"  ‚Ä¢ {name}: {len(pipelines)} pipelines, {len(splits)} splits")

print(f"üìÅ Sauvegarde dans: {THRESHOLDS_DIR}")

# Lancer l'optimisation
optimize_all_thresholds(all_pipelines, all_splits, THRESHOLDS_DIR)





# ‚îÄ‚îÄ 7.4 Synth√®se visuelle (KNN + MICE) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# üîÅ Fusion des 4 jeux : KNN / MICE √ó Full / Reduced
from modules.modeling import load_optimized_thresholds


log = logging.getLogger(__name__) # Get a logger instance

thresholds_dir = OUTPUTS_DIR / "modeling" / "thresholds"

# Load data and add 'Imputation' and 'Version' columns
df_knn_full_thr = load_optimized_thresholds("knn", "full", thresholds_dir).assign(Imputation="KNN", Version="FULL")
df_knn_reduced_thr = load_optimized_thresholds("knn", "reduced", thresholds_dir).assign(Imputation="KNN", Version="REDUCED")
df_mice_full_thr = load_optimized_thresholds("mice", "full", thresholds_dir).assign(Imputation="MICE", Version="FULL")
df_mice_reduced_thr = load_optimized_thresholds("mice", "reduced", thresholds_dir).assign(Imputation="MICE", Version="REDUCED")


df_thr_all = pd.concat([
    df_knn_full_thr,
    df_knn_reduced_thr,
    df_mice_full_thr,
    df_mice_reduced_thr,
], ignore_index=True)

# Mise en forme
df_thr_all = df_thr_all[["model", "threshold", "f1", "precision", "recall", "Imputation", "Version"]]
df_thr_all = df_thr_all.sort_values("f1", ascending=False).reset_index(drop=True)

# üé® Affichage stylis√©
display(
    df_thr_all
        .style
        .background_gradient(subset=["f1"], cmap="Greens", vmin=0.85, vmax=0.95)
        .background_gradient(subset=["threshold"], cmap="Blues", vmin=0.1, vmax=0.9)
        .format({"f1": "{:.4f}", "precision": "{:.4f}", "recall": "{:.4f}", "threshold": "{:.3f}"})
        .highlight_max(subset=["f1"], color="#006400", props="color: white; font-weight: bold")
        .set_caption("Seuils optimaux par mod√®le et m√©thode d'imputation (jeu de validation)")
)

# üìä Insights
print(f"\n INSIGHTS RAPIDES :")
print(f" Meilleur F1 global : {df_thr_all['f1'].max():.4f} ({df_thr_all.iloc[0]['model']} + {df_thr_all.iloc[0]['Imputation']} {df_thr_all.iloc[0]['Version']})")
print(f" F1 moyen : {df_thr_all['f1'].mean():.4f}")
print(f" Seuil moyen : {df_thr_all['threshold'].mean():.3f}")
print(f" √âcart F1 (max-min) : {(df_thr_all['f1'].max() - df_thr_all['f1'].min()):.4f}")

# üîç Comparaison KNN vs MICE (m√™me mod√®le, m√™me version)
print("\n COMPARAISON KNN vs MICE (√† version √©gale) :")
print("-" * 45)
for version in ["FULL", "REDUCED"]:
    models = df_thr_all["model"].unique()
    for model in models:
        try:
            knn_f1 = df_thr_all.query("model == @model and Imputation == 'KNN' and Version == @version")["f1"].values[0]
            mice_f1 = df_thr_all.query("model == @model and Imputation == 'MICE' and Version == @version")["f1"].values[0]
            diff = knn_f1 - mice_f1
            winner = "KNN" if diff > 0 else "MICE" if diff < 0 else "√âgalit√©"
            print(f"{model:<12} ({version}): KNN={knn_f1:.4f}, MICE={mice_f1:.4f} ‚Üí {winner} ({diff:+.4f})")
        except IndexError:
            continue

#  Sauvegarde du tableau consolid√©
df_thr_all.to_csv(cfg.paths.models / "df_all_thresholds.csv", index=False)
log.info(f" Tableau des seuils optimis√©s consolid√© sauvegard√© ‚Üí df_all_thresholds.csv")


# F1 optimal + seuil (bar-plot lisible avec donn√©es FULL/REDUCED) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import matplotlib.ticker as mtick

# V√©rification des colonnes requises
required_cols = {"model", "f1", "threshold", "Imputation", "Version"}
assert required_cols.issubset(df_thr_all.columns), f"Colonnes manquantes : {required_cols - set(df_thr_all.columns)}"

print(" G√©n√©ration du graphique F1 + seuils (FULL vs REDUCED)...")

fig, axes = plt.subplots(1, 2, figsize=(13, 6), sharex=True)

# Boucle sur les deux m√©thodes d'imputation
for ax, (imp, sub) in zip(axes, df_thr_all.groupby("Imputation")):
    sub = sub.sort_values("f1")

    # Ajout de version dans les labels pour diff√©rencier
    y_labels = sub.apply(lambda row: f"{row['model']} ({row['Version']})", axis=1)

    # Palette personnalis√©e
    colors = ["#ff7f7f" if f1 < 0.85 else "#ffcc99" if f1 < 0.90 else "#99ff99"
              for f1 in sub["f1"]]

    bars = sns.barplot(x="f1", y=y_labels, data=sub, palette=colors, ax=ax)

    # Annotations des seuils
    for i, row in enumerate(sub.itertuples()):
        x_pos = max(row.f1 + 0.005, 0.82)
        ax.text(x_pos, i, f"{row.threshold:.2f}",
                va="center", fontsize=9, color="black",
                bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=0.8))

    ax.set_title(f"Imputation : {imp}", fontsize=12, weight="bold")
    ax.set_xlabel("F1-score optimis√©", fontsize=10)
    ax.set_ylabel("Mod√®le (Version)" if imp == "KNN" else "", fontsize=10)
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    ax.set_axisbelow(True)

# Limites X dynamiques
f1_min, f1_max = df_thr_all["f1"].min(), df_thr_all["f1"].max()
x_min = max(0.80, f1_min - 0.02)
x_max = min(0.98, f1_max + 0.02)

for ax in axes:
    ax.set_xlim(x_min, x_max)
    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter("%.3f"))

# Titre principal
fig.suptitle("F1-score optimis√© + seuils de d√©cision\n(FULL vs REDUCED ‚Äî Validation)", fontsize=14, weight="bold")

# L√©gende
legend_elements = [
    plt.Rectangle((0,0),1,1, facecolor="#ff7f7f", label="F1 < 0.85"),
    plt.Rectangle((0,0),1,1, facecolor="#ffcc99", label="0.85 ‚â§ F1 < 0.90"),
    plt.Rectangle((0,0),1,1, facecolor="#99ff99", label="F1 ‚â• 0.90")
]
fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.02),
           ncol=3, fontsize=9, title="Performance")

plt.tight_layout(rect=[0, 0.08, 1, 0.92])

# Sauvegarde
fig_path = FIGURES_NB2_DIR / "f1_opt_with_thresholds_readable_full_vs_reduced.png"
os.makedirs(fig_path.parent, exist_ok=True)

try:
    fig.savefig(fig_path, dpi=150, bbox_inches="tight", facecolor='white')
    log.info(f"üìä Graphique sauvegard√© ‚Üí {fig_path.name}")
    print(f"‚úÖ Graphique sauvegard√© : {fig_path}")
except Exception as e:
    log.error(f"‚ùå Erreur sauvegarde : {e}")
    print(f"‚ùå Erreur lors de la sauvegarde : {e}")

plt.show()

# ‚úÖ R√©sum√© du champion
best_overall = df_thr_all.loc[df_thr_all["f1"].idxmax()]
print(f"\n CHAMPION GLOBAL:")
print(f"   Mod√®le          : {best_overall['model']} + {best_overall['Imputation']} ({best_overall['Version']})")
print(f"   F1-score        : {best_overall['f1']:.4f}")
print(f"   Seuil optimal   : {best_overall['threshold']:.3f}")
print(f"   Pr√©cision       : {best_overall['precision']:.4f}")
print(f"   Rappel          : {best_overall['recall']:.4f}")






# 7.5 Sauvegarde organis√©e de tous les mod√®les optimis√©s ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from datetime import datetime

print("üíæ Sauvegarde de tous les mod√®les optimis√©s...")
print("=" * 65)

# ‚úÖ Statistiques pour le r√©sum√© final
total_models = len(df_thr_all)
saved_models = 0
errors = []

# Dictionnaires des pipelines complets
pipe_dict = {
    ("KNN", "FULL"): best_knn_full_pipes,
    ("KNN", "REDUCED"): best_knn_reduced_pipes,
    ("MICE", "FULL"): best_mice_full_pipes,
    ("MICE", "REDUCED"): best_mice_reduced_pipes,
}

for idx, row in df_thr_all.iterrows():
    model_name = row["model"]
    imp_method = row["Imputation"]
    version    = row["Version"]
    threshold  = float(row["threshold"])
    f1_score   = float(row["f1"])
    precision  = float(row["precision"])
    recall     = float(row["recall"])

    try:
        # Acc√®s au bon pipeline
        pipes = pipe_dict.get((imp_method, version))
        if pipes is None:
            raise ValueError(f"Dictionnaire de pipelines manquant pour {imp_method}-{version}")
        if model_name not in pipes:
            raise KeyError(f"Pipeline {model_name} non trouv√© dans {imp_method}-{version}")

        pipe = pipes[model_name]

        # Dossier par mod√®le + version + imputation
        model_dir = cfg.paths.models / model_name.lower() / version.lower()
        os.makedirs(model_dir, exist_ok=True)

        # Noms des fichiers
        suffix = f"{model_name.lower()}_{imp_method.lower()}_{version.lower()}"
        pipe_path = model_dir / f"pipeline_{suffix}.joblib"
        thr_path  = model_dir / f"threshold_{suffix}.json"
        info_path = model_dir / f"model_info_{suffix}.json"

        # Sauvegarde du pipeline
        joblib.dump(pipe, pipe_path)

        # Sauvegarde enrichie du seuil
        threshold_data = {
            "threshold": threshold,
            "model": model_name,
            "imputation": imp_method,
            "version": version,
            "performance": {
                "f1_score": f1_score,
                "precision": precision,
                "recall": recall
            },
            "ranking": idx + 1,
            "total_models": total_models,
            "save_date": datetime.now().isoformat(),
            "validation_dataset": f"splits['{imp_method.lower()}_{version.lower()}']['X_val']"
        }
        with open(thr_path, "w") as f:
            json.dump(threshold_data, f, indent=2)

        # Fichier info rapide
        model_info = {
            "model_name": model_name,
            "imputation": imp_method,
            "version": version,
            "files": {
                "pipeline": pipe_path.name,
                "threshold": thr_path.name
            },
            "performance": {
                "f1": f1_score,
                "precision": precision,
                "recall": recall,
                "threshold": threshold
            }
        }
        with open(info_path, "w") as f:
            json.dump(model_info, f, indent=2)

        # V√©rification
        if pipe_path.exists() and thr_path.exists():
            saved_models += 1
            size_kb = pipe_path.stat().st_size / 1024
            print(f"‚úÖ {model_name:<12} ({imp_method:<4}-{version:<7}) ‚Üí {model_dir.name} "
                  f"[F1={f1_score:.3f}, {size_kb:.1f}KB]")
        else:
            raise FileNotFoundError("Fichiers non cr√©√©s correctement")

    except Exception as e:
        error_msg = f"‚ùå {model_name} ({imp_method}-{version}): {e}"
        errors.append(error_msg)
        print(error_msg)

# R√©sum√© final
print("\n" + "=" * 65)
print(f"üìä R√âSUM√â DE LA SAUVEGARDE :")
print(f"   ‚úÖ Mod√®les sauvegard√©s : {saved_models}/{total_models}")
print(f"   ‚ùå Erreurs             : {len(errors)}")

if errors:
    print(f"\n‚ö†Ô∏è  ERREURS D√âTECT√âES :")
    for error in errors:
        print(f"   {error}")

# Structure des dossiers cr√©√©s
print(f"\nüìÅ STRUCTURE CR√â√âE :")
print(f"   cfg.paths.models/notebook2/")
for model in df_thr_all["model"].unique():
    for version in ["full", "reduced"]:
        model_dir = cfg.paths.models / model.lower() / version
        if model_dir.exists():
            files = list(model_dir.glob("*"))
            print(f"   ‚îú‚îÄ‚îÄ {model.lower()}/{version}/")
            for file in sorted(files):
                print(f"   ‚îÇ   ‚îî‚îÄ‚îÄ {file.name}")

# Instructions pour chargement futur
print(f"\n CHARGEMENT FUTUR :")
print(f"   model_dir = cfg.paths.models / 'xgboost' / 'reduced'")
print(f"   pipeline = joblib.load(model_dir / 'pipeline_xgboost_knn_reduced.joblib')")
print(f"   with open(model_dir / 'threshold_xgboost_knn_reduced.json') as f:")
print(f"       threshold_data = json.load(f)")

print("\n Tous les mod√®les sont pr√™ts pour le stacking et l'√©valuation finale !")






# 7.6  S√©lection & sauvegarde du meilleur mod√®le (validation F1 optimis√©) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import joblib, json, logging, os
from datetime import datetime
from pathlib import Path

log = logging.getLogger(__name__)

print(" S√©lection du mod√®le champion...")
print("=" * 60)

# V√©rification que df_thr_all contient des donn√©es
if df_thr_all.empty:
    raise ValueError("‚ùå df_thr_all est vide ! V√©rifiez l'optimisation des seuils.")

# 1) Localisation du meilleur mod√®le (champion)
champ_row = df_thr_all.loc[df_thr_all["f1"].idxmax()]
champ_name = champ_row["model"]
champ_imp  = champ_row["Imputation"]
champ_ver  = champ_row["Version"]
champ_thr  = float(champ_row["threshold"])

# Affichage d√©taill√© du champion
print(" CHAMPION S√âLECTIONN√â :")
print(f"   Mod√®le      : {champ_name}")
print(f"   Imputation  : {champ_imp}")
print(f"   Version     : {champ_ver}")
print(f"   F1-score    : {champ_row['f1']:.4f}")
print(f"   Pr√©cision   : {champ_row['precision']:.4f}")
print(f"   Rappel      : {champ_row['recall']:.4f}")
print(f"   Seuil       : {champ_thr:.3f}")

# 2) R√©cup√©ration du pipeline correspondant
key = (champ_imp, champ_ver)
pipe_dict = {
    ("KNN", "FULL"): best_knn_full_pipes,
    ("KNN", "REDUCED"): best_knn_reduced_pipes,
    ("MICE", "FULL"): best_mice_full_pipes,
    ("MICE", "REDUCED"): best_mice_reduced_pipes,
}
champ_pipes = pipe_dict.get(key)

# V√©rification que le pipeline existe
if champ_pipes is None or champ_name not in champ_pipes:
    raise KeyError(f"‚ùå Pipeline {champ_name} non trouv√© dans {champ_imp}-{champ_ver}")

champ_pipe = champ_pipes[champ_name]

# 3) Pr√©paration des chemins de sauvegarde
models_dir = cfg.paths.models / "meilleur_modele"
os.makedirs(models_dir, exist_ok=True)

suffix = f"{champ_name.lower()}_{champ_imp.lower()}_{champ_ver.lower()}"
champ_path = models_dir / f"pipeline_{suffix}_champion.joblib"
thr_path   = models_dir / f"threshold_{suffix}.json"

# 4) Sauvegarde du pipeline
joblib.dump(champ_pipe, champ_path)
print(f"‚úÖ Pipeline sauvegard√© : {champ_path.name}")

# 5) Sauvegarde du seuil avec m√©tadonn√©es
seuil_data = {
    "threshold": champ_thr,
    "model": champ_name,
    "imputation": champ_imp,
    "version": champ_ver,
    "f1_score": float(champ_row["f1"]),
    "precision": float(champ_row["precision"]),
    "recall": float(champ_row["recall"]),
    "selection_date": datetime.now().isoformat(),
    "selection_criteria": "best_f1_on_validation"
}

with open(thr_path, "w") as f:
    json.dump(seuil_data, f, indent=2)
print(f"‚úÖ Seuil + m√©tadonn√©es sauvegard√©s : {thr_path.name}")

# 6) V√©rification des fichiers cr√©√©s
if champ_path.exists() and thr_path.exists():
    print(f"\n‚úÖ SAUVEGARDE R√âUSSIE :")
    print(f"   üì¶ Pipeline : {champ_path}")
    print(f"   Seuil   : {thr_path}")
    print(f"   Taille  : {champ_path.stat().st_size / 1024:.1f} KB")
else:
    raise FileNotFoundError("‚ùå Erreur : Fichiers non cr√©√©s correctement")

# 7) Info pour la suite
print("\n PR√äT POUR LA SUITE :")
print("  √âvaluation finale sur TEST")
print(" üîó Stacking (Notebook 03)")
print("  Pr√©dictions finales")

# 8) Sauvegarde des informations du champion
champion_info = {
    "model_name": champ_name,
    "imputation": champ_imp,
    "version": champ_ver,
    "pipeline_path": str(champ_path),
    "threshold_path": str(thr_path),
    "performance": {
        "f1": float(champ_row["f1"]),
        "precision": float(champ_row["precision"]),
        "recall": float(champ_row["recall"]),
        "threshold": champ_thr
    }
}

info_path = models_dir / "champion_info.json"
with open(info_path, "w") as f:
    json.dump(champion_info, f, indent=2)
print(f"‚úÖ Info champion sauvegard√©es : {info_path.name}")












from modules.modeling import (
    evaluate_all_models_on_test,
    load_optimized_thresholds,
    plot_test_performance
)

# Ajout des imports manquants
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Chemins
knn_dir = cfg.paths.models / "notebook2" / "knn"
mice_dir = cfg.paths.models / "notebook2" / "mice"

# donn√©es Compl√®tes
splits_knn = splits["knn"]
splits_mice = splits["mice"]

# Donn√©es R√©duites
splits_knn_reduced = {
    k: joblib.load(knn_dir / "reduced" / f"knn_{k}_reduced.pkl")
    for k in ["train", "val", "test"]
}
splits_mice_reduced = {
    k: joblib.load(mice_dir / "reduced" / f"mice_{k}_reduced.pkl")
    for k in ["train", "val", "test"]
}

# G√©n√©ration des dictionnaires
pipelines_dict = {
    "knn_full": best_knn_full_pipes,
    "knn_reduced": best_knn_reduced_pipes,
    "mice_full": best_mice_full_pipes,
    "mice_reduced": best_mice_reduced_pipes,
}

# Correction pour utiliser load_optimized_thresholds
thresholds_dict = {}
for key in ["knn_full", "knn_reduced", "mice_full", "mice_reduced"]:
    imputation, version = key.split("_")
    try:
        df_thresh = load_optimized_thresholds(imputation, version, THRESHOLDS_DIR)
        # Convertir le DataFrame en dictionnaire
        thresholds_dict[key] = {}
        for _, row in df_thresh.iterrows():
            thresholds_dict[key][row["model"]] = {
                "threshold": row["threshold"],
                "f1": row["f1"],
                "precision": row["precision"],
                "recall": row["recall"]
            }
    except Exception as e:
        print(f"Erreur lors du chargement des seuils pour {key}: {e}")

splits_dict = {
    "knn_full": splits["knn"],
    "knn_reduced": splits_knn_reduced,
    "mice_full": splits["mice"],
    "mice_reduced": splits_mice_reduced,
}

# Evaluation globale sur TEST
df_test_results = evaluate_all_models_on_test(
    pipelines_dict, thresholds_dict, splits_dict,
    output_dir=OUTPUTS_DIR / "notebook2" / "evaluation_test"
)

# Visualisation manuelle (au lieu d'utiliser plot_test_performance)
if not df_test_results.empty:
    df_sorted = df_test_results.sort_values("f1", ascending=True)
    plt.figure(figsize=(8, 4))
    ax = sns.barplot(data=df_sorted, x="f1", y="model", hue="imputation", dodge=True)
    plt.title("F1-score sur TEST (toutes configurations)", fontsize=14)
    plt.xlabel("F1-score")
    plt.ylabel("Mod√®le")
    plt.grid(axis="x", alpha=0.3, linestyle="--")
    plt.legend(title="Imputation")
    plt.tight_layout()
    plt.show()
else:
    print("Donn√©es vides. Rien √† afficher.")


# Comparaison des Courbes ROC (via module)
from modules.modeling import plot_best_roc_curves_comparison
from sklearn.metrics import roc_curve, auc

# Patcher le module avec les imports manquants
import modules.modeling as modeling_module
modeling_module.roc_curve = roc_curve
modeling_module.auc = auc

# D√©finir les chemins principaux
MODELS_DIR_ROC = cfg.paths.models
FIGURES_NB2_DIR_ROC = FIGURES_NB2_DIR / "final_comparison"

# Construction du dictionnaire des splits avec validation uniquement
all_splits = {
    "knn_full": {"val": {"X": splits["knn"]["X_val"], "y": splits["knn"]["y_val"]}},
    "knn_reduced": {"val": {"X": splits_knn_reduced["val"]["X"], "y": splits_knn_reduced["val"]["y"]}},
    "mice_full": {"val": {"X": splits["mice"]["X_val"], "y": splits["mice"]["y_val"]}},
    "mice_reduced": {"val": {"X": splits_mice_reduced["val"]["X"], "y": splits_mice_reduced["val"]["y"]}},
}

plot_best_roc_curves_comparison(
    models_dir=MODELS_DIR_ROC,
    FIGURES_NB2_DIR=FIGURES_NB2_DIR_ROC,
    splits=all_splits
)



