














import sys, os, logging
from pathlib import Path

# â”€â”€ 0. Logger clair (avec Rich si dispo)
try:
    from rich.logging import RichHandler
    logging.basicConfig(level="INFO",
                        format="%(message)s",
                        handlers=[RichHandler(rich_tracebacks=True, markup=True)],
                        force=True)
except ModuleNotFoundError:
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s - %(levelname)s - %(message)s",
                        stream=sys.stdout,
                        force=True)
logger = logging.getLogger(__name__)

# â”€â”€ 1. DÃ©tection environnement Colab
def _in_colab() -> bool:
    try: import google.colab
    except ImportError: return False
    else: return True

# â”€â”€ 2. Montage Drive manuel rapide
if _in_colab():
    from google.colab import drive
    if not Path("/content/drive/MyDrive/Colab Notebooks").exists():
        logger.info("ğŸ”— Montage de Google Drive en coursâ€¦")
        drive.mount("/content/drive", force_remount=False)

# â”€â”€ 3. Localisation racine projet STA211
def find_project_root() -> Path:
    env_path = os.getenv("STA211_PROJECT_PATH")
    if env_path and (Path(env_path) / "modules").exists():
        return Path(env_path).expanduser().resolve()

    # Chemin Colab correct
    default_colab = Path("/content/drive/MyDrive/Colab Notebooks/projet_sta211_2025")
    if _in_colab() and (default_colab / "modules").exists():
        return default_colab.resolve()

    cwd = Path.cwd()
    for p in [cwd, *cwd.parents]:
        if (p / "modules").exists():
            return p.resolve()

    raise FileNotFoundError("âŒ Impossible de localiser un dossier contenant 'modules/'.")

# â”€â”€ 4. DÃ©finition racine + PYTHONPATH
ROOT_DIR = find_project_root()
os.environ["STA211_PROJECT_PATH"] = str(ROOT_DIR)
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))
logger.info(f"ğŸ“‚ Racine projet dÃ©tectÃ©e : {ROOT_DIR}")
logger.info(f"PYTHONPATH â† {ROOT_DIR}")

# â”€â”€ 5. Initialisation de la configuration projet
from modules.config import cfg
cats = ['noad.', 'ad.']
LABEL_MAP = {0: "noad.", 1: "ad."} 


# â”€â”€ 6. Affichage des chemins configurÃ©s automatiquement
def display_paths(style: bool = True):
    import pandas as pd
    paths_dict = {
        "root": cfg.paths.root,
        "raw": cfg.paths.raw,
        "processed": cfg.paths.processed,
        "models": cfg.paths.models,
        "outputs": cfg.paths.outputs,
        "artifacts": cfg.paths.artifacts
    }
    rows = [{"ClÃ©": k, "Chemin": str(v)} for k, v in paths_dict.items()]
    df = pd.DataFrame(rows).set_index("ClÃ©")

    # VÃ©rification existence
    df["Existe"] = [
        "âœ…" if Path(v).exists() else "âŒ"
        for v in paths_dict.values()
    ]

    from IPython.display import display
    display(df.style.set_table_styles([
        {"selector": "th", "props": [("text-align", "left")]},
        {"selector": "td", "props": [("text-align", "left")]},
    ]) if style else df)

display_paths()
logger.info("âœ… Initialisation complÃ¨te rÃ©ussie - Notebook 02 prÃªt !")





## 0.2 Â· Chargement des bibliothÃ¨ques â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from IPython.display import Markdown, display

# â¬‡ï¸ Imports directs des bibliothÃ¨ques nÃ©cessaires
try:
    # BibliothÃ¨ques de base
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import matplotlib
    import seaborn as sns

    # Scikit-learn et extensions
    import sklearn
    from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    from sklearn.neural_network import MLPClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import (
        classification_report, confusion_matrix, roc_auc_score,
        precision_recall_curve, f1_score, precision_score, recall_score
    )

    # Imbalanced-learn pour le traitement du dÃ©sÃ©quilibre
    import imblearn
    from imblearn.over_sampling import BorderlineSMOTE
    from imblearn.pipeline import Pipeline as ImbPipeline

    # XGBoost
    import xgboost as xgb
    from xgboost import XGBClassifier

    # Utilitaires
    import joblib
    import json
    import warnings
    from tqdm import tqdm
    import scipy

    # Configuration des warnings
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=FutureWarning)

    logger.info("ğŸ“š BibliothÃ¨ques importÃ©es avec succÃ¨s")

except ImportError as e:
    logger.error(f"âŒ Erreur d'importation : {e}")
    raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… Affichage des versions principales
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _safe_version(mod, fallback="â€”"):
    """Retourne mod.__version__ ou un fallback si le module est absent."""
    try:
        return mod.__version__
    except Exception:
        return fallback

def display_modeling_library_versions():
    mods = {
        "pandas"           : pd,
        "numpy"            : np,
        "scikit-learn"     : sklearn,
        "imbalanced-learn" : imblearn,
        "xgboost"          : xgb,
        "matplotlib"       : matplotlib,
        "seaborn"          : sns,
        "scipy"            : scipy,
        "joblib"           : joblib,
        "tqdm"             : __import__("tqdm"),
        "ipython"          : __import__("IPython")
    }
    versions_md = "\n".join(f"- `{k}` : {_safe_version(v)}" for k, v in mods.items())
    display(Markdown(f"âœ… Versions des bibliothÃ¨ques de modÃ©lisation\n{versions_md}"))

display_modeling_library_versions()
logger.info("âœ… Chargement des bibliothÃ¨ques terminÃ©")





import gc
from pathlib import Path
from IPython.display import display, Markdown

try:
    import ipywidgets as wd
    WIDGETS_AVAILABLE = True
    print("âœ… ipywidgets disponible - Interface interactive activÃ©e")
except ImportError:
    WIDGETS_AVAILABLE = False
    print("âš ï¸ ipywidgets non disponible - Mode standard activÃ©")


# âœ… CORRECTION : Utiliser cfg.paths au lieu de paths
data_dir = cfg.paths.processed  / "final_data_for_modeling"

expected_names = {
    "mice_no_outliers",
    "knn_no_outliers",
    "mice_with_outliers",
    "knn_with_outliers"
}

available_files = [f for f in data_dir.glob("df_final_for_modeling_*.csv")
                   if f.stem.replace("df_final_for_modeling_", "") in expected_names]

NAMES = [f.stem.replace("df_final_for_modeling_", "") for f in available_files]

print(f"\nğŸ“‚ {len(NAMES)} datasets dÃ©tectÃ©s : {NAMES}")

def _csv_path(name: str) -> Path:
    return data_dir / f"df_final_for_modeling_{name}.csv"

# Distribution cible renommÃ©e outcome
if NAMES:
    print("\n Distribution cible (outcome) dans chaque fichier")
    for n in NAMES:
        fp = _csv_path(n)
        try:
            df_temp = pd.read_csv(fp)
            s = df_temp["outcome"] if "outcome" in df_temp.columns else df_temp["y"]

            pct = s.map(LABEL_MAP).value_counts(normalize=True).mul(100).round(2)
            print(f"{n:<20} | ad.: {pct.get('ad.', 0):>5.1f}% | noad.: {pct.get('noad.', 0):>5.1f}%")
        except Exception as e:
            print(f"âš ï¸ Erreur lecture {fp.name} â†’ {e}")
else:
    print("âŒ Aucun dataset CSV trouvÃ© dans :", data_dir)

def load_dataset(name: str) -> pd.DataFrame | None:
    fp = _csv_path(name)
    try:
        df = pd.read_csv(fp)
        if "y" in df.columns:
            df.rename(columns={"y": "outcome"}, inplace=True)
        print(f"âœ… {name} chargÃ© : {df.shape}")
        return df
    except Exception as e:
        print(f"âŒ Erreur : {e}")
        return None

if NAMES:
    if WIDGETS_AVAILABLE:
        dropdown = wd.Dropdown(options=NAMES, value=NAMES[0], description="Dataset :")
        out = wd.Output()

        def _on_change(change):
            with out:
                out.clear_output()
                df = load_dataset(change["new"])
                if df is not None:
                    display(df.head())

        dropdown.observe(_on_change, names="value")
        display(Markdown(" SÃ©lectionnez un dataset :"))
        display(dropdown, out)
    else:
        print("\n Mode standard :")
        for i, name in enumerate(NAMES, 1):
            print(f"   {i}. {name}")
        print(" Utilisez load_dataset('nom_du_dataset') pour charger un dataset")
else:
    print(" Aucune donnÃ©e Ã  charger. VÃ©rifiez les fichiers dans :", data_dir)

print("\n Nettoyage mÃ©moire...")
gc.collect()
print("âœ… Initialisation des datasets terminÃ©e !")





print("\n Chargement des datasets principaux...")
print("â”€" * 55)

df_knn  = load_dataset("knn_no_outliers")
df_mice = load_dataset("mice_no_outliers")





# ## RÃ©partition de la variable cible (ad. vs noad.) â€” version courte

cats = ['noad.', 'ad.']

def prep(df):
    s = pd.Series(df['outcome']).map(LABEL_MAP)
    return df.assign(outcome_label=pd.Categorical(s, categories=cats, ordered=True))

def add_perc(ax, total):
    if not ax.patches: return
    ymax = max(p.get_height() for p in ax.patches)
    ax.set_ylim(0, ymax * 1.08)
    for p in ax.patches:
        ax.text(p.get_x()+p.get_width()/2, p.get_height()+ymax*0.02,
                f'{100*p.get_height()/total:.1f}%', ha='center', va='bottom', fontsize=10)

dfs = {'KNN': prep(df_knn), 'MICE': prep(df_mice)}

fig, axes = plt.subplots(1, 2, figsize=(8, 4))
for ax, (title, dfx) in zip(axes, dfs.items()):
    sns.countplot(data=dfx, x='outcome_label', order=cats, palette='Set2', ax=ax)
    ax.set_title(title); ax.set_xlabel(''); ax.set_ylabel('Nombre d\'observations')
    add_perc(ax, len(dfx))

fig.suptitle("RÃ©partition de la variable cible (KNN vs MICE)", fontsize=14, fontweight='bold')
plt.tight_layout(); plt.subplots_adjust(top=0.85)

fig_path = cfg.paths.outputs / "figures" / "notebook2" / "target_distribution_knn-mice_comparison.png"
fig_path.parent.mkdir(parents=True, exist_ok=True)

fig.savefig(fig_path, dpi=150, bbox_inches='tight'); plt.show()

print("ğŸ“Š Statistiques dÃ©taillÃ©es :\n" + "-"*50)
for name, dfx in dfs.items():
    vc = dfx['outcome_label'].value_counts(dropna=False)
    total = vc.sum(); ads = vc.get('ad.', 0); noads = vc.get('noad.', 0)
    print(f"{name:<20} | Total: {total:>4} | Ads: {ads:>3} ({ads/total:>5.1%}) | No-ads: {noads:>4} ({noads/total:>5.1%})")

logger.info(f"ğŸ“Š Graphe comparatif sauvegardÃ© â†’ {fig_path.name}")












# Split des donnÃ©es (train â€” validation â€” test) <a id="2-split-des-donnÃ©es-train--validation--test"></a>
from sklearn.model_selection import train_test_split

# âœ… Import du nouveau systÃ¨me
from modules.utils import save_artifact

log = logging.getLogger(__name__)
RANDOM_STATE = 42

TARGET = "outcome"                

def tri_split_and_save(df: pd.DataFrame, name: str):
    """
    â€¢ Split stratifiÃ© 60 / 20 / 20  (train / val / test) 
    â€¢ Sauvegarde columns + trois pickles dans le dossier spÃ©cifique (knn ou mice)
    â€¢ Retourne un dict des sous-ensembles
    """
    if TARGET not in df.columns:
        raise KeyError(f"La colonne cible Â« {TARGET} Â» est absente de {name}.")

    
    save_dir = cfg.paths.models / "notebook2" / name
    save_dir.mkdir(parents=True, exist_ok=True)

    X = df.drop(columns=[TARGET])
    y = df[TARGET].astype(int)

    # 80 / 20  âœ  temp / test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE
    )
    # 75 / 25 du temp âœ train / val  â‡’ 60 / 20 / 20 global
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_STATE
    )

    cols = X_train.columns.tolist()

    # âœ… CORRECTION 2: Remplacer joblib par save_artifact
    save_artifact(cols, f"columns_{name}.pkl", save_dir)
    save_artifact({"X": X_train, "y": y_train}, f"{name}_train.pkl", save_dir)
    save_artifact({"X": X_val,   "y": y_val},   f"{name}_val.pkl", save_dir)
    save_artifact({"X": X_test,  "y": y_test},  f"{name}_test.pkl", save_dir)

    log.info(f"{name.upper():<4} | split 60/20/20 sauvegardÃ© dans {save_dir} (cols = {len(cols)})")

    return dict(X_train=X_train, y_train=y_train,
                X_val=X_val,     y_val=y_val,
                X_test=X_test,   y_test=y_test)

# â”€â”€ CrÃ©ation pour KNN & MICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
splits = {
    "knn":  tri_split_and_save(df_knn,  "knn"),
    "mice": tri_split_and_save(df_mice, "mice"),
}

# â”€â”€ RÃ©sumÃ© visuel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_summary(spl):
    lines = []
    for name, d in spl.items():
        lines.append(
            f"- **{name.upper()}** : "
            f"train {d['X_train'].shape} (ads {d['y_train'].mean():.2%}) Â· "
            f"val {d['X_val'].shape} (ads {d['y_val'].mean():.2%}) Â· "
            f"test {d['X_test'].shape} (ads {d['y_test'].mean():.2%})"
        )
    display(Markdown(" DÃ©coupage 60 / 20 / 20 (cible = outcome)\n" + "\n".join(lines)))

split_summary(splits)


# --- Sauvegarde de l'imputeur par la mÃ©diane pour la colonne X4 ---

print(" Sauvegarde de la mÃ©diane pour l'imputation de X4...")


models_dir = cfg.paths.models / "notebook2"
models_dir.mkdir(parents=True, exist_ok=True) # CrÃ©e le dossier s'il n'existe pas

try:
    if "X4" not in splits['mice']['X_train']:
        raise KeyError("Colonne 'X4' absente de X_train.")


    median_x4_train = splits['mice']['X_train']['X4'].median()

    # 2. DÃ©finir le chemin de sauvegarde
    save_path = models_dir / "median_imputer_X4.pkl"

    # 3. Sauvegarder la valeur de la mÃ©diane avec joblib
    save_artifact(median_x4_train, "median_imputer_X4.pkl", models_dir)

    print(f" MÃ©diane de X4 ({median_x4_train:.4f}) sauvegardÃ©e dans : {save_path}")

except KeyError:
    print("âŒ ERREUR : La colonne 'X4' n'a pas Ã©tÃ© trouvÃ©e dans le jeu d'entraÃ®nement.")
except Exception as e:
    print(f"âŒ Une erreur inattendue est survenue : {e}")





from imblearn.pipeline import Pipeline 

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import BorderlineSMOTE



def to_cat_y(y):
    s = pd.Series(y, name='y').map(LABEL_MAP)
    return pd.Categorical(s, categories=cats, ordered=True)

def add_perc(ax, total):
    if not ax.patches: return
    ymax = max(p.get_height() for p in ax.patches)
    ax.set_ylim(0, ymax * 1.08)
    for p in ax.patches:
        ax.text(p.get_x()+p.get_width()/2, p.get_height()+ymax*0.02,
                f'{100*p.get_height()/total:.1f}%', ha='center', va='bottom', fontsize=9)

# DonnÃ©es brutes
X_raw = splits["knn"]["X_train"]
y_raw = splits["knn"]["y_train"]

# Avant SMOTE
y_before = to_cat_y(y_raw)

# Pipeline + BorderlineSMOTE
pipe = Pipeline([
    ("imp",   SimpleImputer(strategy="median")),
    ("scale", StandardScaler(with_mean=False)),
    ("smote", BorderlineSMOTE(sampling_strategy=0.8, random_state=RANDOM_STATE))
])
X_res, y_res = pipe.fit_resample(X_raw, y_raw)

# AprÃ¨s SMOTE
y_after = to_cat_y(y_res)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(6, 3.5), sharey=True)
sns.countplot(x=y_before, order=cats, palette="Set2", ax=axes[0])
axes[0].set_title("Avant SMOTE"); axes[0].set_xlabel(""); axes[0].set_ylabel("Observations")
add_perc(axes[0], len(y_before))

sns.countplot(x=y_after, order=cats, palette="Set2", ax=axes[1])
axes[1].set_title("AprÃ¨s BorderlineSMOTE"); axes[1].set_xlabel(""); axes[1].set_ylabel("")
add_perc(axes[1], len(y_after))

fig.suptitle("Distribution avant / aprÃ¨s SMOTE (KNN)", fontsize=12, fontweight="bold")
plt.tight_layout(); plt.subplots_adjust(top=0.88)

fig_path = cfg.paths.outputs / "figures" / "notebook2" / "smote_effect_knn.png"
fig.savefig(fig_path, dpi=150, bbox_inches="tight"); plt.show()

# Stats rapides
print(" Avant SMOTE :", dict(pd.Series(y_before).value_counts()))
print(" AprÃ¨s SMOTE :", dict(pd.Series(y_after).value_counts()))

logger.info(f" Graphe SMOTE sauvegardÃ© â†’ {fig_path.name}")






#from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import json, logging

log = logging.getLogger(__name__)
RANDOM_STATE = 42


# === GRILLES ANTI-OVERFITTING ===
# Objectif : RÃ©duire l'Ã©cart validation/test (0.9394 â†’ 0.88)

# 1. Gradient Boosting
gradboost_params = {
    "clf__n_estimators": [50, 100, 200],
    "clf__max_depth": [3, 4, 5],
    "clf__learning_rate": [0.05, 0.1],
    "clf__min_samples_split": [10, 20],
    "clf__min_samples_leaf": [5, 10],
    "clf__subsample": [0.7, 0.8]
}

# 2. Random Forest - BEAUCOUP PLUS CONSERVATEUR
rf_params = {
    "clf__n_estimators": [50, 100, 200],    # Moins d'arbres
    "clf__max_depth": [5, 10, 15],          # Profondeur limitÃ©e (vs None)
    "clf__min_samples_split": [10, 20],     # Plus strict (vs 2)
    "clf__min_samples_leaf": [5, 10],       # Feuilles plus grosses (vs 1)
    "clf__max_features": ["sqrt", "log2"],   # Retrait "None" qui overfitte
}

# 3. SVM - Plus de rÃ©gularisation
svc_params = {
    "clf__C": [0.1, 1, 10],                # Moins de valeurs extrÃªmes
    "clf__kernel": ["rbf", "linear"],       # Retrait poly (complexe)
    "clf__gamma": ["scale", "auto"],        # Retrait valeurs numÃ©riques
}

# 4. MLP - Architectures plus simples
mlp_params = {
    "clf__hidden_layer_sizes": [(50,), (100,)],  # Plus simple
    "clf__activation": ["relu", "tanh"],          # Retrait logistic
    "clf__alpha": [1e-3, 1e-2, 1e-1],           # Plus de rÃ©gularisation
    "clf__learning_rate": ["constant", "adaptive"],
    "clf__solver": ["adam"],                      # Plus stable que sgd
}

# 5. XGBoost - RÃ©gularisation renforcÃ©e
xgb_params = {
    "clf__n_estimators": [50, 100, 200],        #  Moins d'arbres
    "clf__max_depth": [3, 4, 5],                # Moins profond (vs 8)
    "clf__learning_rate": [0.05, 0.1, 0.15],    # Plus conservateur
    "clf__subsample": [0.7, 0.8],               # Sous-Ã©chantillonnage
    "clf__colsample_bytree": [0.7, 0.8],        # Idem pour features
    "clf__reg_lambda": [1, 5, 10],              # ğŸ”§ Plus de rÃ©gularisation L2
    "clf__reg_alpha": [0, 1, 5],                # Ajout rÃ©gularisation L1
    "clf__scale_pos_weight": [1, 5, 10],        # Pour dÃ©sÃ©quilibre
}

# Dictionnaire mis Ã  jour (sans Decision Tree et KNN)
param_grids = {
    "GradBoost": gradboost_params,
    "RandForest": rf_params,
    "SVM": svc_params,
    "MLP": mlp_params,
    "XGBoost": xgb_params,
}

# Sauvegarde pour rÃ©fÃ©rence
save_artifact(param_grids, "hyperparam_grids.json", cfg.paths.models)
log.info("âœ… Grilles d'hyper-paramÃ¨tres anti-overfitting sauvegardÃ©es")


# dictionnaire des modÃ¨les de base (sans class_weight : SMOTE s'en charge)
models = {
    "GradBoost": GradientBoostingClassifier(random_state=RANDOM_STATE),
    "RandForest": RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),
    "SVM": SVC(probability=True, random_state=RANDOM_STATE),
    "MLP": MLPClassifier(max_iter=500, random_state=RANDOM_STATE),
    "XGBoost": xgb.XGBClassifier(
        use_label_encoder=False, eval_metric="logloss",
        random_state=RANDOM_STATE, n_jobs=-1),
}





# === IMPORT DES FONCTIONS RFECV ===
from modules.notebook2_modeling import perform_rfecv_selection, plot_rfecv_results

# === EXÃ‰CUTION POUR KNN ET MICE ===
rfecv_results = {}

for method in ["knn", "mice"]:
    rfecv_results[method] = perform_rfecv_selection(
        splits[method],
        method,
        cfg.paths.models
    )
    plot_rfecv_results(rfecv_results[method], method)
    print("-" * 50)


# === SAUVEGARDE DES DATASETS RÃ‰DUITS ===
from modules.notebook2_modeling import save_reduced_datasets

# Sauvegarde pour KNN et MICE en une seule fois
save_reduced_datasets(splits, rfecv_results, cfg.paths.models)








# === VALIDATION CROISÃ‰E RENFORCÃ‰E + FONCTIONS D'Ã‰VALUATION ===
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import (
    f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix
)

log = logging.getLogger(__name__)

#Validation croisÃ©e renforcÃ©e (10-fold au lieu de 5)
CV = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)

# ğŸ”§ Suppression des warnings XGBoost
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')

# === OBJECTIF CIBLE ===
# RÃ©duire le gap validation/test de 0.06 Ã  < 0.02
# Score test cible : > 0.90 (au lieu de 0.88)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

## Pipeline unifiÃ© (imputation de sÃ©curitÃ©) + scale + BorderlineSMOTE + modÃ¨le)

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import BorderlineSMOTE
from sklearn.preprocessing import StandardScaler

def make_pipeline(model, sampling_ratio=0.8):
    """
    Pipeline complet :
      StandardScaler âœ BorderlineSMOTE âœ modÃ¨le
    """
    return Pipeline(steps=[
        ("scale",  StandardScaler(with_mean=False)),
        ("smote",  BorderlineSMOTE(
            sampling_strategy=sampling_ratio,
            random_state=RANDOM_STATE,
            kind="borderline-1")),
        ("clf",    model),
    ])


def _plot_cm_with_metrics(y_true, y_pred, y_proba,
                          title="Confusion Matrix", plot=True) -> dict:
    """Affiche (optionnel) une matrice compacte + mÃ©triques, retourne les scores."""
    f1_bin = f1_score(y_true, y_pred, zero_division=0)
    prec   = precision_score(y_true, y_pred, zero_division=0)
    rec    = recall_score(y_true, y_pred, zero_division=0)
    auc    = roc_auc_score(y_true, y_proba) if y_proba is not None else None

    if plot:
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(4, 3))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
        subtitle = f"F1={f1_bin:.3f} Â· P={prec:.3f} Â· R={rec:.3f}"
        if auc is not None:
            subtitle += f" Â· AUC={auc:.3f}"
        plt.title(f"{title}\n{subtitle}", pad=20)
        plt.xlabel("PrÃ©dit"); plt.ylabel("RÃ©el")
        plt.tight_layout(); plt.show()

    return dict(f1_binary=f1_bin, precision=prec, recall=rec, auc=auc)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def evaluate_model(name: str, base_model, param_grid: dict,
                   X_train, y_train, X_val, y_val, dataset: str,
                   plot_cm: bool = True):
    """
    GridSearchCV â†’ meilleur pipeline.

    ParamÃ¨tres:
    -----------
    X_train, y_train : DonnÃ©es d'entraÃ®nement (60%) pour GridSearchCV
    X_val, y_val     : DonnÃ©es de validation (20%) pour Ã©valuation finale

    Retourne:
    ---------
    (pipeline, mÃ©triques) avec surveillance de l'overfitting
    """
    pipe = make_pipeline(base_model)
    gs   = GridSearchCV(pipe, param_grid, scoring="f1",
                        cv=CV, n_jobs=-1, verbose=0).fit(X_train, y_train)

    best   = gs.best_estimator_
    y_pred = best.predict(X_val)
    y_prob = best.predict_proba(X_val)[:, 1] if hasattr(best, "predict_proba") else None

    metrics = _plot_cm_with_metrics(
        y_val, y_pred, y_prob,
        title=f"{name} ({dataset})", plot=plot_cm
    )

    # ğŸ”§ Calcul de l'Ã©cart CV/validation pour surveiller l'overfitting
    f1_cv = gs.best_score_          # Score moyen de la validation croisÃ©e
    f1_val = metrics["f1_binary"]   # Score sur le jeu de validation
    gap = f1_cv - f1_val

    metrics.update({
        "model": name,
        "dataset": dataset,
        "f1_weighted": f1_score(y_val, y_pred, average="weighted"),
        "best_params": gs.best_params_,
        "f1_cv": round(f1_cv, 4),      # Score validation croisÃ©e
        "f1_val": round(f1_val, 4),    # Score validation finale
        "gap": round(gap, 4),          # Ã‰cart CV/validation (indicateur overfitting)
    })

    # ğŸ”§ Affichage enrichi avec surveillance de l'overfitting
    print(f"ğŸ“Š {name} ({dataset}): F1_CV={f1_cv:.3f}, F1_VAL={f1_val:.3f}, Gap={gap:.3f}")
    if gap > 0.05:
        print(f"âš ï¸  Overfitting dÃ©tectÃ© (gap > 0.05)")
    elif gap > 0.03:
        print(f"ğŸ”¶ LÃ©ger overfitting (gap > 0.03)")
    else:
        print(f"âœ… Bonne gÃ©nÃ©ralisation (gap â‰¤ 0.03)")

    # â¬‡ï¸ Sauvegardes
    joblib.dump(best, MODELS_DIR / f"best_{name.lower()}_{dataset}.joblib")
    with open(MODELS_DIR / f"best_params_{name.lower()}_{dataset}.json", "w") as f:
        json.dump(gs.best_params_, f, indent=2)

    return best, metrics

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def evaluate_all(models_dict, grids_dict,
                 X_train, y_train, X_val, y_val, dataset: str,
                 plot_cm: bool = True) -> tuple[pd.DataFrame, dict]:
    """
    Boucle d'Ã©valuation sur tous les modÃ¨les.

    ParamÃ¨tres:
    -----------
    X_train, y_train : DonnÃ©es d'entraÃ®nement (60%)
    X_val, y_val     : DonnÃ©es de validation (20%)

    Retourne:
    ---------
    (DataFrame des rÃ©sultats, dict des meilleurs pipelines)
    """
    records, best_pipelines = [], {}

    print(f" Ã‰valuation sur {dataset.upper()} avec CV={CV.n_splits}-fold")
    print("=" * 60)
    print(" TRAIN â†’ VAL (sÃ©lection des modÃ¨les)")
    print("-" * 60)

    for name, base_model in models_dict.items():
        print(f" {name} ({dataset})")
        best, rec = evaluate_model(
            name, base_model, grids_dict[name],
            X_train, y_train, X_val, y_val, dataset,
            plot_cm=plot_cm
        )
        records.append(rec); best_pipelines[name] = best

    df = (pd.DataFrame(records)
            .sort_values("f1_val", ascending=False)  # Tri par F1 validation
            .reset_index(drop=True))

    # Affichage enrichi avec mÃ©triques d'overfitting
    fmt = {c: "{:.4f}" for c in ["f1_val", "precision", "recall", "auc", "f1_cv", "gap"] if c in df}
    display(df.style.format(fmt, na_rep="â€”")
                  .background_gradient(subset=["f1_val"], cmap="Greens")
                  .background_gradient(subset=["gap"], cmap="Reds_r"))  # Rouge pour gap Ã©levÃ©

    # RÃ©sumÃ© de l'overfitting
    print(f"\n RÃ‰SUMÃ‰ DE L'OVERFITTING ({dataset.upper()}):")
    print("-" * 50)
    for _, row in df.iterrows():
        gap = row.get('gap', 0)
        if gap > 0.05:
            status = "âš ï¸  OVERFITTING"
        elif gap > 0.03:
            status = "ğŸ”¶ LÃ‰GER OVERFITTING"
        else:
            status = "âœ… BONNE GÃ‰NÃ‰RALISATION"
        print(f"  {row['model']:<12}: gap={gap:+.3f} â†’ {status}")

    print(f"\nğŸ† Champion {dataset.upper()}: {df.iloc[0]['model']} (F1_VAL={df.iloc[0]['f1_val']:.4f})")

    return df, best_pipelines





# Chemins
knn_dir = MODELS_DIR / "notebook2" / "knn"
mice_dir = MODELS_DIR / "notebook2" / "mice"

# ComplÃ¨tes
splits_knn = splits["knn"]
splits_mice = splits["mice"]

# RÃ©duites
splits_knn_reduced = {
    k: joblib.load(knn_dir / "reduced" / f"knn_{k}_reduced.pkl")
    for k in ["train", "val", "test"] # Added 'test' here
}
splits_mice_reduced = {
    k: joblib.load(mice_dir / "reduced" / f"mice_{k}_reduced.pkl")
    for k in ["train", "val", "test"] # Added 'test' here
}


# KNN - RÃ‰DUIT
results_knn_reduced, best_knn_reduced = evaluate_all(
    models, param_grids,
    splits_knn_reduced["train"]["X"], splits_knn_reduced["train"]["y"],
    splits_knn_reduced["val"]["X"], splits_knn_reduced["val"]["y"],
    dataset="knn_reduced"
)



# MICE - RÃ‰DUIT
results_mice_reduced, best_mice_reduced = evaluate_all(
    models, param_grids,
    splits_mice_reduced["train"]["X"], splits_mice_reduced["train"]["y"],
    splits_mice_reduced["val"]["X"], splits_mice_reduced["val"]["y"],
    dataset="mice_reduced"
)


# KNN - COMPLET
results_knn_full, best_knn_full = evaluate_all(
    models, param_grids,
    splits_knn["X_train"], splits_knn["y_train"],
    splits_knn["X_val"], splits_knn["y_val"],
    dataset="knn_full"
)




# MICE - COMPLET
results_mice_full, best_mice_full = evaluate_all(
    models, param_grids,
    splits_mice["X_train"], splits_mice["y_train"],
    splits_mice["X_val"], splits_mice["y_val"],
    dataset="mice_full"
)



df_global_results = pd.concat([
    results_knn_full.assign(version="knn_full"),
    results_knn_reduced.assign(version="knn_reduced"),
    results_mice_full.assign(version="mice_full"),
    results_mice_reduced.assign(version="mice_reduced")
], ignore_index=True)

# TriÃ© selon le score F1 validation
df_global_results.sort_values(by="f1_val", ascending=False).reset_index(drop=True).head()

df_global_results.to_csv(MODELS_DIR / "eval_all_models_summary.csv", index=False)




print("\n Sauvegarde des rÃ©sultats globaux d'Ã©valuation")
print("=" * 60)

# Mapping des rÃ©sultats
results_dict = {
    "knn_full": results_knn_full,
    "knn_reduced": results_knn_reduced,
    "mice_full": results_mice_full,
    "mice_reduced": results_mice_reduced,
}

# Mapping des meilleurs pipelines
pipelines_dict = {
    "knn_full": best_knn_full,
    "knn_reduced": best_knn_reduced,
    "mice_full": best_mice_full,
    "mice_reduced": best_mice_reduced,
}

# RÃ©pertoire commun
eval_dir = MODELS_DIR
eval_dir.mkdir(parents=True, exist_ok=True)

# Sauvegardes CSV, JSON, pipelines
for name, df_result in results_dict.items():
    # RÃ©sultats validation
    df_result.to_csv(eval_dir / f"eval_{name}_val_scores.csv", index=False)
    df_result.to_json(eval_dir / f"eval_{name}_val_scores.json", orient="records", indent=2)

    # Pipelines
    best_pipes = pipelines_dict[name]
    for model_name, pipeline in best_pipes.items():
        pipeline_path = eval_dir / f"pipeline_{model_name.lower()}_{name}.joblib"
        joblib.dump(pipeline, pipeline_path)
        print(f" Pipeline sauvegardÃ© : {pipeline_path.name}")

    # Dictionnaire des chemins
    dict_path = eval_dir / f"best_{name}_pipelines.json"
    json.dump(
        {k: str(eval_dir / f"pipeline_{k.lower()}_{name}.joblib") for k in best_pipes},
        open(dict_path, "w"), indent=2
    )

print("\nâœ… Toutes les Ã©valuations et pipelines ont Ã©tÃ© sauvegardÃ©s.")



df_global_results





# 5.3 Â· SynthÃ¨se des performances KNN vs MICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# â”€â”€ 1. helper : DataFrame â†’ format homogÃ¨ne â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _results_to_df(res_df: pd.DataFrame, imput: str) -> pd.DataFrame:
    """
    Prend le DataFrame renvoyÃ© par evaluate_all()
    et le met au format : ModÃ¨le, F1-score, PrÃ©cision, Rappel, AUC, Gap, Imputation.
    """
    df = res_df.copy().rename(columns={"model": "ModÃ¨le"})

    rename = {"f1_val": "F1-score",
              "precision": "PrÃ©cision",
              "recall":    "Rappel",
              "auc":       "AUC",
              "gap":       "Gap CV/Val"}
    df = df.rename(columns=rename)

    keep = ["ModÃ¨le", "F1-score", "PrÃ©cision", "Rappel", "AUC", "Gap CV/Val"]
    df = df[keep]
    df["MÃ©thode d'imputation"] = imput
    return df

# â”€â”€ 2. convertir tous les rÃ©sultats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_knn_full     = _results_to_df(results_knn_full,    "KNN - Complet")
df_knn_reduced  = _results_to_df(results_knn_reduced, "KNN - RÃ©duit")
df_mice_full    = _results_to_df(results_mice_full,   "MICE - Complet")
df_mice_reduced = _results_to_df(results_mice_reduced,"MICE - RÃ©duit")

# â”€â”€ 3. concat + tri â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
results_comparison = (pd.concat(
    [df_knn_full, df_knn_reduced, df_mice_full, df_mice_reduced],
    ignore_index=True
).sort_values("F1-score", ascending=False)
 .reset_index(drop=True))

# â”€â”€ 4. affichage stylisÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
display(
    results_comparison
      .style
      .background_gradient(subset=["F1-score"], cmap="Greens", vmin=0.80, vmax=0.94)
      .background_gradient(subset=["Gap CV/Val"], cmap="Reds_r", vmin=-0.05, vmax=0.05)
      .background_gradient(subset=["AUC"], cmap="Blues", vmin=0.95, vmax=0.98)
      .format({
          "F1-score": "{:.4f}", "PrÃ©cision": "{:.4f}",
          "Rappel":   "{:.4f}", "AUC": "{:.4f}",
          "Gap CV/Val": "{:+.3f}"
      }, na_rep="â€”")
      .highlight_max(subset=["F1-score"], color="#006400",
                     props="color: white; font-weight: bold; border: 2px solid #004d00")
      .highlight_min(subset=["Gap CV/Val"], color="#000080",
                     props="color: white; font-weight: bold; border: 2px solid #000060")
)

# â”€â”€ 5. lÃ©gende â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\nğŸ¨ LÃ‰GENDE DES COULEURS :")
print("F1-score : Vert (plus foncÃ© = meilleur)")
print("Gap CV/Val : Rouge (plus foncÃ© = overfitting)")
print("AUC : Bleu (plus foncÃ© = meilleur)")
print("Meilleur F1-score : SurlignÃ© en vert foncÃ©")
print("Meilleur gap : SurlignÃ© en bleu foncÃ©")

# â”€â”€ 6. statistiques rapides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"\nğŸ“Š STATISTIQUES RAPIDES :")
print(f"Meilleur F1-score : {results_comparison['F1-score'].max():.4f}")
print(f"F1-score moyen : {results_comparison['F1-score'].mean():.4f}")
print(f"Gap moyen : {results_comparison['Gap CV/Val'].mean():.3f}")
print(f"âœ… ModÃ¨les avec gap â‰¤ 0.03 : {(results_comparison['Gap CV/Val'].abs() <= 0.03).sum()}/{len(results_comparison)}")

# â”€â”€ 7. sauvegarde CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
csv_path = MODELS_DIR / "results_comparison_knn_mice.csv"
results_comparison.to_csv(csv_path, index=False)
log.info(f"âœ… Tableau comparatif KNN & MICE sauvegardÃ© â†’ {csv_path.name}")

# â”€â”€ 8. rÃ©sumÃ© final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
champion_model = results_comparison.iloc[0]['ModÃ¨le']
champion_imputation = results_comparison.iloc[0]["MÃ©thode d'imputation"]
champion_f1 = results_comparison.iloc[0]['F1-score']
champion_gap = results_comparison.iloc[0]['Gap CV/Val']

print(f"\nğŸ¯ CONTEXTE : Performances sur jeu de VALIDATION (20%)")
print(f"ğŸ“‹ Phase suivante : Ã‰valuation finale sur jeu de TEST (20%)")
print(f"ğŸ† Champion provisoire : {champion_model} + {champion_imputation}")
print(f"     F1-score = {champion_f1:.4f}, Gap = {champion_gap:+.3f}")



# !pip install scikit-optimize --quiet









from modules.modeling import (
    optimize_threshold,
    optimize_multiple,
    save_optimized_thresholds,
    load_best_pipelines,
)






# Chargement des pipelines KNN full
best_knn_full_pipes = load_best_pipelines("knn", "full", model_dir=MODELS_DIR/"notebook2")

# Chargement des pipelines KNN reduced
best_knn_reduced_pipes = load_best_pipelines("knn", "reduced", model_dir=MODELS_DIR/"notebook2")



# Chargement des pipelines MICE full
best_mice_full_pipes = load_best_pipelines("mice", "full", model_dir=MODELS_DIR/"notebook2")

# Chargement des pipelines MICE reduced
best_mice_reduced_pipes = load_best_pipelines("mice", "reduced", model_dir=MODELS_DIR/"notebook2")



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Optimisation des seuils pour toutes les combinaisons : KNN/MICE Ã— Full/Reduced
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from modules.modeling import optimize_all_thresholds

THRESHOLDS_DIR = OUTPUTS_DIR / "modeling" / "thresholds"

if not THRESHOLDS_DIR.exists():
    THRESHOLDS_DIR.mkdir(parents=True)


# Pipelines
all_pipelines = {
    "knn_full":    best_knn_full_pipes,
    "knn_reduced": best_knn_reduced_pipes,
    "mice_full":   best_mice_full_pipes,
    "mice_reduced": best_mice_reduced_pipes,
}

# Splits

all_splits = {
    "knn_full":    splits_knn,
    "knn_reduced": splits_knn_reduced,
    "mice_full":   splits_mice,
    "mice_reduced": splits_mice_reduced,
}

# Lancer l'optimisation
optimize_all_thresholds(all_pipelines, all_splits, THRESHOLDS_DIR)






# â”€â”€ 7.4 SynthÃ¨se visuelle (KNN + MICE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” Fusion des 4 jeux : KNN / MICE Ã— Full / Reduced
from modules.modeling import load_optimized_thresholds


log = logging.getLogger(__name__) # Get a logger instance

thresholds_dir = OUTPUTS_DIR / "modeling" / "thresholds"

# Load data and add 'Imputation' and 'Version' columns
df_knn_full_thr = load_optimized_thresholds("knn", "full", thresholds_dir).assign(Imputation="KNN", Version="FULL")
df_knn_reduced_thr = load_optimized_thresholds("knn", "reduced", thresholds_dir).assign(Imputation="KNN", Version="REDUCED")
df_mice_full_thr = load_optimized_thresholds("mice", "full", thresholds_dir).assign(Imputation="MICE", Version="FULL")
df_mice_reduced_thr = load_optimized_thresholds("mice", "reduced", thresholds_dir).assign(Imputation="MICE", Version="REDUCED")


df_thr_all = pd.concat([
    df_knn_full_thr,
    df_knn_reduced_thr,
    df_mice_full_thr,
    df_mice_reduced_thr,
], ignore_index=True)

# ğŸª„ Mise en forme
df_thr_all = df_thr_all[["model", "threshold", "f1", "precision", "recall", "Imputation", "Version"]]
df_thr_all = df_thr_all.sort_values("f1", ascending=False).reset_index(drop=True)

# ğŸ¨ Affichage stylisÃ©
display(
    df_thr_all
        .style
        .background_gradient(subset=["f1"], cmap="Greens", vmin=0.85, vmax=0.95)
        .background_gradient(subset=["threshold"], cmap="Blues", vmin=0.1, vmax=0.9)
        .format({"f1": "{:.4f}", "precision": "{:.4f}", "recall": "{:.4f}", "threshold": "{:.3f}"})
        .highlight_max(subset=["f1"], color="#006400", props="color: white; font-weight: bold")
        .set_caption("Seuils optimaux par modÃ¨le et mÃ©thode d'imputation (jeu de validation)")
)

# ğŸ“Š Insights
print(f"\nğŸ“ˆ INSIGHTS RAPIDES :")
print(f"ğŸ† Meilleur F1 global : {df_thr_all['f1'].max():.4f} ({df_thr_all.iloc[0]['model']} + {df_thr_all.iloc[0]['Imputation']} {df_thr_all.iloc[0]['Version']})")
print(f"ğŸ“Š F1 moyen : {df_thr_all['f1'].mean():.4f}")
print(f"ğŸ¯ Seuil moyen : {df_thr_all['threshold'].mean():.3f}")
print(f"ğŸ“‰ Ã‰cart F1 (max-min) : {(df_thr_all['f1'].max() - df_thr_all['f1'].min()):.4f}")

# ğŸ” Comparaison KNN vs MICE (mÃªme modÃ¨le, mÃªme version)
print(f"\nğŸ” COMPARAISON KNN vs MICE (Ã  version Ã©gale) :")
print("-" * 45)
for version in ["FULL", "REDUCED"]:
    models = df_thr_all["model"].unique()
    for model in models:
        try:
            knn_f1 = df_thr_all.query("model == @model and Imputation == 'KNN' and Version == @version")["f1"].values[0]
            mice_f1 = df_thr_all.query("model == @model and Imputation == 'MICE' and Version == @version")["f1"].values[0]
            diff = knn_f1 - mice_f1
            winner = "KNN" if diff > 0 else "MICE" if diff < 0 else "Ã‰galitÃ©"
            print(f"{model:<12} ({version}): KNN={knn_f1:.4f}, MICE={mice_f1:.4f} â†’ {winner} ({diff:+.4f})")
        except IndexError:
            continue

# ğŸ’¾ Sauvegarde du tableau consolidÃ©
df_thr_all.to_csv(MODELS_DIR / "df_all_thresholds.csv", index=False)
log.info(f"âœ… Tableau des seuils optimisÃ©s consolidÃ© sauvegardÃ© â†’ df_all_thresholds.csv")


# F1 optimal + seuil (bar-plot lisible avec donnÃ©es FULL/REDUCED) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import matplotlib.ticker as mtick

# âœ… VÃ©rification des colonnes requises
required_cols = {"model", "f1", "threshold", "Imputation", "Version"}
assert required_cols.issubset(df_thr_all.columns), f"Colonnes manquantes : {required_cols - set(df_thr_all.columns)}"

print("ğŸ“Š GÃ©nÃ©ration du graphique F1 + seuils (FULL vs REDUCED)...")

fig, axes = plt.subplots(1, 2, figsize=(13, 6), sharex=True)

# Boucle sur les deux mÃ©thodes d'imputation
for ax, (imp, sub) in zip(axes, df_thr_all.groupby("Imputation")):
    sub = sub.sort_values("f1")

    # Ajout de version dans les labels pour diffÃ©rencier
    y_labels = sub.apply(lambda row: f"{row['model']} ({row['Version']})", axis=1)

    # âœ… Palette personnalisÃ©e
    colors = ["#ff7f7f" if f1 < 0.85 else "#ffcc99" if f1 < 0.90 else "#99ff99"
              for f1 in sub["f1"]]

    bars = sns.barplot(x="f1", y=y_labels, data=sub, palette=colors, ax=ax)

    # âœ… Annotations des seuils
    for i, row in enumerate(sub.itertuples()):
        x_pos = max(row.f1 + 0.005, 0.82)
        ax.text(x_pos, i, f"{row.threshold:.2f}",
                va="center", fontsize=9, color="black",
                bbox=dict(boxstyle="round,pad=0.2", facecolor="white", alpha=0.8))

    ax.set_title(f"Imputation : {imp}", fontsize=12, weight="bold")
    ax.set_xlabel("F1-score optimisÃ©", fontsize=10)
    ax.set_ylabel("ModÃ¨le (Version)" if imp == "KNN" else "", fontsize=10)
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    ax.set_axisbelow(True)

# Limites X dynamiques
f1_min, f1_max = df_thr_all["f1"].min(), df_thr_all["f1"].max()
x_min = max(0.80, f1_min - 0.02)
x_max = min(0.98, f1_max + 0.02)

for ax in axes:
    ax.set_xlim(x_min, x_max)
    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter("%.3f"))

# âœ… Titre principal
fig.suptitle("F1-score optimisÃ© + seuils de dÃ©cision\n(FULL vs REDUCED â€” Validation)", fontsize=14, weight="bold")

# âœ… LÃ©gende
legend_elements = [
    plt.Rectangle((0,0),1,1, facecolor="#ff7f7f", label="F1 < 0.85"),
    plt.Rectangle((0,0),1,1, facecolor="#ffcc99", label="0.85 â‰¤ F1 < 0.90"),
    plt.Rectangle((0,0),1,1, facecolor="#99ff99", label="F1 â‰¥ 0.90")
]
fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.02),
           ncol=3, fontsize=9, title="Performance")

plt.tight_layout(rect=[0, 0.08, 1, 0.92])

# âœ… Sauvegarde
fig_path = FIGURES_NB2_DIR / "f1_opt_with_thresholds_readable_full_vs_reduced.png"
os.makedirs(fig_path.parent, exist_ok=True)

try:
    fig.savefig(fig_path, dpi=150, bbox_inches="tight", facecolor='white')
    log.info(f"ğŸ“Š Graphique sauvegardÃ© â†’ {fig_path.name}")
    print(f"âœ… Graphique sauvegardÃ© : {fig_path}")
except Exception as e:
    log.error(f"âŒ Erreur sauvegarde : {e}")
    print(f"âŒ Erreur lors de la sauvegarde : {e}")

plt.show()

# âœ… RÃ©sumÃ© du champion
best_overall = df_thr_all.loc[df_thr_all["f1"].idxmax()]
print(f"\nğŸ† CHAMPION GLOBAL:")
print(f"   ModÃ¨le          : {best_overall['model']} + {best_overall['Imputation']} ({best_overall['Version']})")
print(f"   F1-score        : {best_overall['f1']:.4f}")
print(f"   Seuil optimal   : {best_overall['threshold']:.3f}")
print(f"   PrÃ©cision       : {best_overall['precision']:.4f}")
print(f"   Rappel          : {best_overall['recall']:.4f}")






# 7.5 Sauvegarde organisÃ©e de tous les modÃ¨les optimisÃ©s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from datetime import datetime

print("ğŸ’¾ Sauvegarde de tous les modÃ¨les optimisÃ©s...")
print("=" * 65)

# âœ… Statistiques pour le rÃ©sumÃ© final
total_models = len(df_thr_all)
saved_models = 0
errors = []

# Dictionnaires des pipelines complets
pipe_dict = {
    ("KNN", "FULL"): best_knn_full_pipes,
    ("KNN", "REDUCED"): best_knn_reduced_pipes,
    ("MICE", "FULL"): best_mice_full_pipes,
    ("MICE", "REDUCED"): best_mice_reduced_pipes,
}

for idx, row in df_thr_all.iterrows():
    model_name = row["model"]
    imp_method = row["Imputation"]
    version    = row["Version"]
    threshold  = float(row["threshold"])
    f1_score   = float(row["f1"])
    precision  = float(row["precision"])
    recall     = float(row["recall"])

    try:
        # AccÃ¨s au bon pipeline
        pipes = pipe_dict.get((imp_method, version))
        if pipes is None:
            raise ValueError(f"Dictionnaire de pipelines manquant pour {imp_method}-{version}")
        if model_name not in pipes:
            raise KeyError(f"Pipeline {model_name} non trouvÃ© dans {imp_method}-{version}")

        pipe = pipes[model_name]

        # Dossier par modÃ¨le + version + imputation
        model_dir = MODELS_DIR / model_name.lower() / version.lower()
        os.makedirs(model_dir, exist_ok=True)

        # Noms des fichiers
        suffix = f"{model_name.lower()}_{imp_method.lower()}_{version.lower()}"
        pipe_path = model_dir / f"pipeline_{suffix}.joblib"
        thr_path  = model_dir / f"threshold_{suffix}.json"
        info_path = model_dir / f"model_info_{suffix}.json"

        # ğŸ”„ Sauvegarde du pipeline
        joblib.dump(pipe, pipe_path)

        # ğŸ”„ Sauvegarde enrichie du seuil
        threshold_data = {
            "threshold": threshold,
            "model": model_name,
            "imputation": imp_method,
            "version": version,
            "performance": {
                "f1_score": f1_score,
                "precision": precision,
                "recall": recall
            },
            "ranking": idx + 1,
            "total_models": total_models,
            "save_date": datetime.now().isoformat(),
            "validation_dataset": f"splits['{imp_method.lower()}_{version.lower()}']['X_val']"
        }
        with open(thr_path, "w") as f:
            json.dump(threshold_data, f, indent=2)

        # ğŸ”„ Fichier info rapide
        model_info = {
            "model_name": model_name,
            "imputation": imp_method,
            "version": version,
            "files": {
                "pipeline": pipe_path.name,
                "threshold": thr_path.name
            },
            "performance": {
                "f1": f1_score,
                "precision": precision,
                "recall": recall,
                "threshold": threshold
            }
        }
        with open(info_path, "w") as f:
            json.dump(model_info, f, indent=2)

        # âœ… VÃ©rification
        if pipe_path.exists() and thr_path.exists():
            saved_models += 1
            size_kb = pipe_path.stat().st_size / 1024
            print(f"âœ… {model_name:<12} ({imp_method:<4}-{version:<7}) â†’ {model_dir.name} "
                  f"[F1={f1_score:.3f}, {size_kb:.1f}KB]")
        else:
            raise FileNotFoundError("Fichiers non crÃ©Ã©s correctement")

    except Exception as e:
        error_msg = f"âŒ {model_name} ({imp_method}-{version}): {e}"
        errors.append(error_msg)
        print(error_msg)

# âœ… RÃ©sumÃ© final
print("\n" + "=" * 65)
print(f"ğŸ“Š RÃ‰SUMÃ‰ DE LA SAUVEGARDE :")
print(f"   âœ… ModÃ¨les sauvegardÃ©s : {saved_models}/{total_models}")
print(f"   âŒ Erreurs             : {len(errors)}")

if errors:
    print(f"\nâš ï¸  ERREURS DÃ‰TECTÃ‰ES :")
    for error in errors:
        print(f"   {error}")

# âœ… Structure des dossiers crÃ©Ã©s
print(f"\nğŸ“ STRUCTURE CRÃ‰Ã‰E :")
print(f"   MODELS_DIR/notebook2/")
for model in df_thr_all["model"].unique():
    for version in ["full", "reduced"]:
        model_dir = MODELS_DIR / model.lower() / version
        if model_dir.exists():
            files = list(model_dir.glob("*"))
            print(f"   â”œâ”€â”€ {model.lower()}/{version}/")
            for file in sorted(files):
                print(f"   â”‚   â””â”€â”€ {file.name}")

# âœ… Instructions pour chargement futur
print(f"\nğŸ”„ CHARGEMENT FUTUR :")
print(f"   model_dir = MODELS_DIR / 'xgboost' / 'reduced'")
print(f"   pipeline = joblib.load(model_dir / 'pipeline_xgboost_knn_reduced.joblib')")
print(f"   with open(model_dir / 'threshold_xgboost_knn_reduced.json') as f:")
print(f"       threshold_data = json.load(f)")

print("\n Tous les modÃ¨les sont prÃªts pour le stacking et l'Ã©valuation finale !")






# 7.6  SÃ©lection & sauvegarde du meilleur modÃ¨le (validation F1 optimisÃ©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import joblib, json, logging, os
from datetime import datetime
from pathlib import Path

log = logging.getLogger(__name__)

print("ğŸ† SÃ©lection du modÃ¨le champion...")
print("=" * 60)

# âœ… VÃ©rification que df_thr_all contient des donnÃ©es
if df_thr_all.empty:
    raise ValueError("âŒ df_thr_all est vide ! VÃ©rifiez l'optimisation des seuils.")

# 1) Localisation du meilleur modÃ¨le (champion)
champ_row = df_thr_all.loc[df_thr_all["f1"].idxmax()]
champ_name = champ_row["model"]
champ_imp  = champ_row["Imputation"]
champ_ver  = champ_row["Version"]
champ_thr  = float(champ_row["threshold"])

# âœ… Affichage dÃ©taillÃ© du champion
print(f"ğŸ¯ CHAMPION SÃ‰LECTIONNÃ‰ :")
print(f"   ModÃ¨le      : {champ_name}")
print(f"   Imputation  : {champ_imp}")
print(f"   Version     : {champ_ver}")
print(f"   F1-score    : {champ_row['f1']:.4f}")
print(f"   PrÃ©cision   : {champ_row['precision']:.4f}")
print(f"   Rappel      : {champ_row['recall']:.4f}")
print(f"   Seuil       : {champ_thr:.3f}")

# 2) RÃ©cupÃ©ration du pipeline correspondant
key = (champ_imp, champ_ver)
pipe_dict = {
    ("KNN", "FULL"): best_knn_full_pipes,
    ("KNN", "REDUCED"): best_knn_reduced_pipes,
    ("MICE", "FULL"): best_mice_full_pipes,
    ("MICE", "REDUCED"): best_mice_reduced_pipes,
}
champ_pipes = pipe_dict.get(key)

# âœ… VÃ©rification que le pipeline existe
if champ_pipes is None or champ_name not in champ_pipes:
    raise KeyError(f"âŒ Pipeline {champ_name} non trouvÃ© dans {champ_imp}-{champ_ver}")

champ_pipe = champ_pipes[champ_name]

# 3) PrÃ©paration des chemins de sauvegarde
models_dir = MODELS_DIR / "meilleur_modele"
os.makedirs(models_dir, exist_ok=True)

suffix = f"{champ_name.lower()}_{champ_imp.lower()}_{champ_ver.lower()}"
champ_path = models_dir / f"pipeline_{suffix}_champion.joblib"
thr_path   = models_dir / f"threshold_{suffix}.json"

# 4) Sauvegarde du pipeline
joblib.dump(champ_pipe, champ_path)
print(f"âœ… Pipeline sauvegardÃ© : {champ_path.name}")

# 5) Sauvegarde du seuil avec mÃ©tadonnÃ©es
seuil_data = {
    "threshold": champ_thr,
    "model": champ_name,
    "imputation": champ_imp,
    "version": champ_ver,
    "f1_score": float(champ_row["f1"]),
    "precision": float(champ_row["precision"]),
    "recall": float(champ_row["recall"]),
    "selection_date": datetime.now().isoformat(),
    "selection_criteria": "best_f1_on_validation"
}

with open(thr_path, "w") as f:
    json.dump(seuil_data, f, indent=2)
print(f"âœ… Seuil + mÃ©tadonnÃ©es sauvegardÃ©s : {thr_path.name}")

# 6) VÃ©rification des fichiers crÃ©Ã©s
if champ_path.exists() and thr_path.exists():
    print(f"\nâœ… SAUVEGARDE RÃ‰USSIE :")
    print(f"   ğŸ“¦ Pipeline : {champ_path}")
    print(f"   ğŸ¯ Seuil   : {thr_path}")
    print(f"   ğŸ“Š Taille  : {champ_path.stat().st_size / 1024:.1f} KB")
else:
    raise FileNotFoundError("âŒ Erreur : Fichiers non crÃ©Ã©s correctement")

# 7) Info pour la suite
print(f"\nğŸš€ PRÃŠT POUR LA SUITE :")
print(f"   ğŸ“‹ Ã‰valuation finale sur TEST")
print(f"   ğŸ”— Stacking (Notebook 03)")
print(f"   ğŸ“¤ PrÃ©dictions finales")

# 8) Sauvegarde des informations du champion
champion_info = {
    "model_name": champ_name,
    "imputation": champ_imp,
    "version": champ_ver,
    "pipeline_path": str(champ_path),
    "threshold_path": str(thr_path),
    "performance": {
        "f1": float(champ_row["f1"]),
        "precision": float(champ_row["precision"]),
        "recall": float(champ_row["recall"]),
        "threshold": champ_thr
    }
}

info_path = models_dir / "champion_info.json"
with open(info_path, "w") as f:
    json.dump(champion_info, f, indent=2)
print(f"âœ… Info champion sauvegardÃ©es : {info_path.name}")












from modules.modeling import (
    evaluate_all_models_on_test,
    load_optimized_thresholds,
    plot_test_performance
)

# Ajout des imports manquants
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Chemins
knn_dir = MODELS_DIR / "notebook2" / "knn"
mice_dir = MODELS_DIR / "notebook2" / "mice"

# donnÃ©es ComplÃ¨tes
splits_knn = splits["knn"]
splits_mice = splits["mice"]

# DonnÃ©es RÃ©duites
splits_knn_reduced = {
    k: joblib.load(knn_dir / "reduced" / f"knn_{k}_reduced.pkl")
    for k in ["train", "val", "test"]
}
splits_mice_reduced = {
    k: joblib.load(mice_dir / "reduced" / f"mice_{k}_reduced.pkl")
    for k in ["train", "val", "test"]
}

# GÃ©nÃ©ration des dictionnaires
pipelines_dict = {
    "knn_full": best_knn_full_pipes,
    "knn_reduced": best_knn_reduced_pipes,
    "mice_full": best_mice_full_pipes,
    "mice_reduced": best_mice_reduced_pipes,
}

# Correction pour utiliser load_optimized_thresholds
thresholds_dict = {}
for key in ["knn_full", "knn_reduced", "mice_full", "mice_reduced"]:
    imputation, version = key.split("_")
    try:
        df_thresh = load_optimized_thresholds(imputation, version, THRESHOLDS_DIR)
        # Convertir le DataFrame en dictionnaire
        thresholds_dict[key] = {}
        for _, row in df_thresh.iterrows():
            thresholds_dict[key][row["model"]] = {
                "threshold": row["threshold"],
                "f1": row["f1"],
                "precision": row["precision"],
                "recall": row["recall"]
            }
    except Exception as e:
        print(f"Erreur lors du chargement des seuils pour {key}: {e}")

splits_dict = {
    "knn_full": splits["knn"],
    "knn_reduced": splits_knn_reduced,
    "mice_full": splits["mice"],
    "mice_reduced": splits_mice_reduced,
}

# Evaluation globale sur TEST
df_test_results = evaluate_all_models_on_test(
    pipelines_dict, thresholds_dict, splits_dict,
    output_dir=OUTPUTS_DIR / "notebook2" / "evaluation_test"
)

# Visualisation manuelle (au lieu d'utiliser plot_test_performance)
if not df_test_results.empty:
    df_sorted = df_test_results.sort_values("f1", ascending=True)
    plt.figure(figsize=(8, 4))
    ax = sns.barplot(data=df_sorted, x="f1", y="model", hue="imputation", dodge=True)
    plt.title("F1-score sur TEST (toutes configurations)", fontsize=14)
    plt.xlabel("F1-score")
    plt.ylabel("ModÃ¨le")
    plt.grid(axis="x", alpha=0.3, linestyle="--")
    plt.legend(title="Imputation")
    plt.tight_layout()
    plt.show()
else:
    print("DonnÃ©es vides. Rien Ã  afficher.")


# Comparaison des Courbes ROC (via module)
from modules.modeling import plot_best_roc_curves_comparison
from sklearn.metrics import roc_curve, auc

# Patcher le module avec les imports manquants
import modules.modeling as modeling_module
modeling_module.roc_curve = roc_curve
modeling_module.auc = auc

# DÃ©finir les chemins principaux
MODELS_DIR_ROC = MODELS_DIR
FIGURES_NB2_DIR_ROC = FIGURES_NB2_DIR / "final_comparison"

# Construction du dictionnaire des splits avec validation uniquement
all_splits = {
    "knn_full": {"val": {"X": splits["knn"]["X_val"], "y": splits["knn"]["y_val"]}},
    "knn_reduced": {"val": {"X": splits_knn_reduced["val"]["X"], "y": splits_knn_reduced["val"]["y"]}},
    "mice_full": {"val": {"X": splits["mice"]["X_val"], "y": splits["mice"]["y_val"]}},
    "mice_reduced": {"val": {"X": splits_mice_reduced["val"]["X"], "y": splits_mice_reduced["val"]["y"]}},
}

plot_best_roc_curves_comparison(
    models_dir=MODELS_DIR_ROC,
    FIGURES_NB2_DIR=FIGURES_NB2_DIR_ROC,
    splits=all_splits
)



