"""
modules/final_preprocessing.py - VERSION 3.0

Module de pr√©traitement complet pour le projet STA211 Internet Advertisements.
Inclut toutes les corrections, la r√©duction de corr√©lation optimis√©e, et les fonctions avanc√©es.

NOUVELLES FONCTIONNALIT√âS VERSION 3.0:
- Int√©gration de l'analyse de corr√©lation comprehensive
- R√©duction de dimensionnalit√© efficace
- Protection X4 renforc√©e
- Pipeline modulaire et robuste


Auteur: Abdoullatuf
Version: 3.0
Date: 2025
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from pathlib import Path
from typing import List, Optional, Union, Dict, Tuple

from sklearn.preprocessing import PowerTransformer
from outliers import detect_and_remove_outliers
from eda import load_and_clean_data


from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

import sys
import json

from config import cfg

models_dir = cfg.paths.MODELS_DIR / "notebook1"
Path(models_dir).mkdir(parents=True, exist_ok=True)

# ============================================================================
# 1. UTILITAIRES DE BASE
# ============================================================================

def convert_X4_to_int(df: pd.DataFrame, column: str = "X4", verbose: bool = True) -> pd.DataFrame:
    """
    Convertit X4 en Int64 si elle contient uniquement des valeurs binaires (0, 1).
    
    Args:
        df: DataFrame d'entr√©e
        column: Nom de la colonne √† convertir (d√©faut: "X4")
        verbose: Affichage des informations
        
    Returns:
        DataFrame avec X4 convertie en Int64
    """
    df = df.copy()
    
    if column not in df.columns:
        if verbose:
            print(f"‚ö†Ô∏è Colonne '{column}' absente du DataFrame.")
        return df

    unique_vals = df[column].dropna().unique()
    if set(unique_vals).issubset({0, 1}):
        df[column] = df[column].astype("Int64")
        if verbose:
            print(f"‚úÖ Colonne '{column}' convertie en Int64 (binaire).")
    elif verbose:
        print(f"‚ùå Colonne '{column}' contient {unique_vals}. Conversion ignor√©e.")

    return df


def apply_yeojohnson(
    df: pd.DataFrame,
    columns: List[str],
    standardize: bool = False,
    save_model: bool = False,
    model_path: Optional[Union[str, Path]] = None,
    return_transformer: bool = False
) -> Union[pd.DataFrame, Tuple[pd.DataFrame, PowerTransformer]]:
    """
    Applique la transformation Yeo-Johnson sur les colonnes sp√©cifi√©es.
    
    Args:
        df: DataFrame d'entr√©e
        columns: Colonnes √† transformer
        standardize: Standardiser apr√®s transformation
        save_model: Sauvegarder le transformateur
        model_path: Chemin de sauvegarde du mod√®le
        return_transformer: Retourner aussi le transformateur
        
    Returns:
        DataFrame transform√© (et transformateur si demand√©)
    """
    df_transformed = df.copy()

    # Charger ou cr√©er le transformateur
    if model_path and Path(model_path).exists():
        pt = joblib.load(model_path)
        print(f"üîÑ Transformateur recharg√© depuis : {model_path}")
    else:
        pt = PowerTransformer(method="yeo-johnson", standardize=standardize)
        pt.fit(df[columns])
        
        if save_model and model_path:
            Path(model_path).parent.mkdir(parents=True, exist_ok=True)
            joblib.dump(pt, model_path)
            print(f"‚úÖ Transformateur Yeo-Johnson sauvegard√© √† : {model_path}")

    # Appliquer la transformation
    transformed_values = pt.transform(df[columns])
    for i, col in enumerate(columns):
        df_transformed[f"{col}_trans"] = transformed_values[:, i]

    return (df_transformed, pt) if return_transformer else df_transformed


# ============================================================================
# 2. ANALYSE DE CORR√âLATION COMPREHENSIVE (NOUVELLE VERSION 3.0)
# ============================================================================

def analyze_correlation_comprehensive(df: pd.DataFrame, threshold=0.90, show_analysis=True):
    """
    Analyse de corr√©lation compl√®te avec protection X4 et r√©duction effective.
    
    Args:
        df: DataFrame d'entr√©e
        threshold: Seuil de corr√©lation pour suppression
        show_analysis: Afficher l'analyse d√©taill√©e
        
    Returns:
        df_reduced, correlation_report
    """
    
    if show_analysis:
        print("üîç ANALYSE DE CORR√âLATION COMPREHENSIVE")
        print("=" * 45)
        print(f"üìä Dataset initial : {df.shape}")
    
    # ========================================================================
    # 1. IDENTIFICATION DES VARIABLES BINAIRES
    # ========================================================================
    
    # Variables binaires (exclure outcome et prot√©ger X4)
    binary_vars = []
    protected_vars = ['X4']  # Variables √† prot√©ger
    excluded_vars = ['outcome']  # Variables √† exclure de l'analyse
    
    for col in df.columns:
        if col in excluded_vars:
            continue
        if df[col].dtype in ['int64', 'Int64', 'bool']:
            # V√©rifier que c'est vraiment binaire
            unique_vals = df[col].dropna().unique()
            if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):
                binary_vars.append(col)
    
    if show_analysis:
        print(f"üî¢ Variables binaires trouv√©es : {len(binary_vars)}")
        print(f"üõ°Ô∏è Variables prot√©g√©es : {protected_vars}")
        print(f"‚ö†Ô∏è Variables exclues : {excluded_vars}")
    
    if len(binary_vars) < 2:
        if show_analysis:
            print("‚ùå Pas assez de variables binaires pour l'analyse")
        return df, {"message": "Pas assez de variables binaires"}
    
    # ========================================================================
    # 2. CALCUL DE LA MATRICE DE CORR√âLATION
    # ========================================================================
    
    try:
        # Calcul sur les variables binaires seulement
        binary_df = df[binary_vars]
        corr_matrix = binary_df.corr().abs()
        
        if show_analysis:
            print(f"üìê Matrice de corr√©lation : {corr_matrix.shape}")
            
    except Exception as e:
        if show_analysis:
            print(f"‚ùå Erreur calcul corr√©lation : {e}")
        return df, {"error": str(e)}
    
    # ========================================================================
    # 3. IDENTIFICATION DES GROUPES CORR√âL√âS
    # ========================================================================
    
    # Triangle sup√©rieur pour √©viter les doublons
    upper_triangle = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    
    # Identifier les paires hautement corr√©l√©es
    highly_correlated_pairs = []
    
    for col in upper_triangle.columns:
        correlated_with = upper_triangle[col][upper_triangle[col] > threshold]
        if len(correlated_with) > 0:
            for corr_col, corr_val in correlated_with.items():
                highly_correlated_pairs.append({
                    'var1': col,
                    'var2': corr_col, 
                    'correlation': corr_val
                })
    
    if show_analysis:
        print(f"üîó Paires hautement corr√©l√©es (>{threshold}) : {len(highly_correlated_pairs)}")
        
        # Afficher quelques exemples
        if highly_correlated_pairs:
            print("üìã Exemples de corr√©lations √©lev√©es :")
            for i, pair in enumerate(highly_correlated_pairs[:5]):
                print(f"   {pair['var1']} ‚Üî {pair['var2']}: {pair['correlation']:.3f}")
            if len(highly_correlated_pairs) > 5:
                print(f"   ... et {len(highly_correlated_pairs) - 5} autres")
    
    # ========================================================================
    # 4. STRAT√âGIE DE SUPPRESSION AVEC PROTECTION
    # ========================================================================
    
    # Variables √† supprimer (en prot√©geant X4)
    vars_to_drop = set()
    
    # Pour chaque paire corr√©l√©e, supprimer une des deux variables
    for pair in highly_correlated_pairs:
        var1, var2 = pair['var1'], pair['var2']
        
        # Logique de priorit√© :
        # 1. Ne jamais supprimer X4
        # 2. Supprimer la variable avec l'index le plus √©lev√© (convention)
        
        if var1 in protected_vars:
            # Prot√©ger var1, supprimer var2 si pas prot√©g√©e
            if var2 not in protected_vars:
                vars_to_drop.add(var2)
        elif var2 in protected_vars:
            # Prot√©ger var2, supprimer var1
            vars_to_drop.add(var1)
        else:
            # Aucune des deux n'est prot√©g√©e, supprimer celle avec index plus √©lev√©
            if var1 > var2:  # Ordre alphab√©tique/num√©rique
                vars_to_drop.add(var1)
            else:
                vars_to_drop.add(var2)
    
    vars_to_drop = list(vars_to_drop)
    
    if show_analysis:
        print(f"üìâ Variables marqu√©es pour suppression : {len(vars_to_drop)}")
        
        # V√©rification protection X4
        x4_protected = all(var not in protected_vars for var in vars_to_drop)
        print(f"üõ°Ô∏è X4 correctement prot√©g√©e : {'‚úÖ' if x4_protected else '‚ùå'}")
        
        if vars_to_drop:
            print(f"üóëÔ∏è Variables √† supprimer : {vars_to_drop[:10]}{'...' if len(vars_to_drop) > 10 else ''}")
    
    # ========================================================================
    # 5. APPLICATION DE LA SUPPRESSION
    # ========================================================================
    
    # Suppression effective
    df_reduced = df.drop(columns=vars_to_drop, errors='ignore')
    
    # Rapport final
    correlation_report = {
        'original_shape': df.shape,
        'reduced_shape': df_reduced.shape,
        'binary_vars_analyzed': len(binary_vars),
        'highly_correlated_pairs': len(highly_correlated_pairs),
        'vars_dropped': len(vars_to_drop),
        'vars_dropped_list': vars_to_drop,
        'protected_vars': protected_vars,
        'x4_still_present': 'X4' in df_reduced.columns,
        'dimension_reduction': df.shape[1] - df_reduced.shape[1]
    }
    
    if show_analysis:
        print("\nüéâ R√âSULTATS DE LA R√âDUCTION :")
        print(f"üìä Dimensions avant : {correlation_report['original_shape']}")
        print(f"üìä Dimensions apr√®s : {correlation_report['reduced_shape']}")
        print(f"üìâ R√©duction : {correlation_report['dimension_reduction']} colonnes supprim√©es")
        print(f"üõ°Ô∏è X4 pr√©sente : {'‚úÖ' if correlation_report['x4_still_present'] else '‚ùå'}")
        print(f"‚öñÔ∏è Pourcentage conserv√© : {df_reduced.shape[1]/df.shape[1]*100:.1f}%")
    
    return df_reduced, correlation_report








def apply_collinearity_filter(
    df,
    cols_to_drop,
    imputation_method: str, # 'mice' ou 'knn'
    models_dir: Path,       # Le dossier de BASE (ex: '.../notebook1')
    protected_cols=['X4'],
    display_info=True,
    save_results=True
):
    """
    Supprime les colonnes corr√©l√©es et sauvegarde la liste dans un
    sous-dossier sp√©cifique √† la m√©thode d'imputation.
    """

    # --- 1. Protection des colonnes ---
    cols_to_drop_filtered = [col for col in cols_to_drop if col not in protected_cols]
    
    if display_info and protected_cols:
        protected_saved = len(cols_to_drop) - len(cols_to_drop_filtered)
        if protected_saved > 0:
            saved_cols = [col for col in cols_to_drop if col in protected_cols]
            print(f"üõ°Ô∏è {protected_saved} colonnes prot√©g√©es : {saved_cols}")

    # --- 2. Sauvegarde dynamique dans le sous-dossier ---
    if save_results:
        # Construit le chemin du sous-dossier (ex: .../notebook1/mice)
        save_directory = models_dir / imputation_method
        # Cr√©e le dossier s'il n'existe pas
        save_directory.mkdir(parents=True, exist_ok=True)
        
        # D√©finit le chemin complet du fichier
        save_path = save_directory / "cols_to_drop_corr.pkl"
        
        try:
            joblib.dump(cols_to_drop_filtered, save_path)
            if display_info:
                print(f"üíæ Liste des colonnes sauvegard√©e dans : {save_directory.name}/{save_path.name}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde : {e}")

    # --- 3. Suppression des colonnes ---
    existing_cols_to_drop = [col for col in cols_to_drop_filtered if col in df.columns]
    df_filtered = df.drop(columns=existing_cols_to_drop)

    if display_info:
        print(f"‚úÖ Colonnes supprim√©es : {len(existing_cols_to_drop)}")
        print(f"üìè Dimensions finales : {df_filtered.shape}")

    return df_filtered




# ======================================================================
# FONCTION PRINCIPALE : D√âTECTION DES VARIABLES TR√àS CORR√âL√âES
# ======================================================================

def find_highly_correlated_groups(
    df: pd.DataFrame,
    threshold: float = 0.90,
    exclude_cols: Optional[List[str]] = None,
    protected_cols: Optional[List[str]] = None,
    show_plot: bool = False,
    save_path: Optional[Union[str, Path]] = None,
    figsize: tuple = (10, 8)
) -> Dict[str, Union[List[List[str]], List[str]]]:
    """
    Identifie les groupes de variables fortement corr√©l√©es avec protection de certaines colonnes.

    VERSION 3.0 ‚Äì MISE √Ä JOUR : suppression de save_fig(), sauvegarde standard matplotlib

    Args:
        df: DataFrame d'entr√©e
        threshold: Seuil de corr√©lation (d√©faut: 0.90)
        exclude_cols: Colonnes √† exclure de l'analyse
        protected_cols: Colonnes √† prot√©ger de la suppression (d√©faut: ['X4'])
        show_plot: Afficher la heatmap de corr√©lation
        save_path: Chemin de sauvegarde de la figure (PNG ou autre format)
        figsize: Taille de la figure

    Returns:
        Dictionnaire avec groups, to_drop, et protected
    """
    if protected_cols is None:
        protected_cols = ['X4']

    if df.empty:
        print("‚ö†Ô∏è DataFrame vide - retour structure par d√©faut")
        return {"groups": [], "to_drop": [], "protected": protected_cols}

    all_exclude_cols = (exclude_cols or []) + (protected_cols or [])
    df_corr = df.drop(columns=all_exclude_cols, errors='ignore') if all_exclude_cols else df.copy()

    if df_corr.empty:
        print("‚ö†Ô∏è Aucune colonne √† analyser apr√®s exclusions")
        return {"groups": [], "to_drop": [], "protected": protected_cols}

    try:
        corr_matrix = df_corr.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur dans le calcul de corr√©lation: {e}")
        return {"groups": [], "to_drop": [], "protected": protected_cols}

    groups, visited = [], set()
    for col in upper.columns:
        if col in visited:
            continue
        correlated = upper[col][upper[col] > threshold].index.tolist()
        if correlated:
            group = sorted(set([col] + correlated))
            groups.append(group)
            visited.update(group)

    to_drop = []
    for group in groups:
        protected_in_group = [col for col in group if col in protected_cols]
        non_protected = [col for col in group if col not in protected_cols]
        if non_protected:
            to_drop.extend(non_protected[1:])  # Garder le premier non-prot√©g√©

    if show_plot and not corr_matrix.empty:
        try:
            plt.figure(figsize=figsize)
            sns.heatmap(corr_matrix, cmap="coolwarm", center=0, square=True,
                        cbar_kws={"shrink": 0.75})
            plt.title(f"Matrice de corr√©lation (>{threshold})")
            plt.tight_layout()

            if save_path:
                save_path = Path(save_path)
                save_path.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"üì∏ Figure sauvegard√©e dans : {save_path}")
            else:
                plt.show()
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur dans la visualisation: {e}")

    protected_in_drop = set(to_drop) & set(protected_cols)
    if protected_in_drop:
        print(f"üõ°Ô∏è PROTECTION ACTIV√âE: Retrait de {protected_in_drop} de la liste de suppression")
        to_drop = [col for col in to_drop if col not in protected_cols]

    result = {
        "groups": groups,
        "to_drop": to_drop,
        "protected": protected_cols
    }

    assert isinstance(result, dict)
    assert isinstance(result["groups"], list)
    assert isinstance(result["to_drop"], list)
    assert isinstance(result["protected"], list)

    return result




def drop_correlated_duplicates(
    df: pd.DataFrame,
    groups: List[List[str]],
    target_col: str = "outcome",
    extra_cols: List[str] = None,
    protected_cols: List[str] = None,
    priority_cols: List[str] = None,
    verbose: bool = False,
    summary: bool = True
) -> Tuple[pd.DataFrame, List[str], List[str]]:
    """
    Supprime les variables corr√©l√©es avec protection et ordre des colonnes optimis√©.
    
    VERSION 3.0 - ROBUSTESSE RENFORC√âE
    """
    # üõ°Ô∏è Protection par d√©faut
    if protected_cols is None:
        protected_cols = ['X4']
    
    # üìå Colonnes prioritaires par d√©faut
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']
    
    # üîß VALIDATION D'ENTR√âE RENFORC√âE
    if not isinstance(groups, list):
        print(f"‚ö†Ô∏è groups doit √™tre une liste, re√ßu: {type(groups)} - conversion forc√©e")
        groups = list(groups) if groups else []
    
    to_drop, to_keep = [], []

    # Traitement des groupes corr√©l√©s avec validation
    for group in groups:
        if not group or not isinstance(group, (list, tuple)):
            if verbose:
                print(f"‚ö†Ô∏è Groupe invalide ignor√©: {group}")
            continue
        
        group = list(group)  # Conversion s√©curis√©e
        
        # üõ°Ô∏è S√©parer les colonnes prot√©g√©es des autres
        protected_in_group = [col for col in group if col in protected_cols]
        non_protected = [col for col in group if col not in protected_cols and col in df.columns]
        
        if non_protected:
            # Garder le premier non-prot√©g√©
            keep = non_protected[0]
            drop = non_protected[1:]
            to_keep.append(keep)
            to_drop.extend(drop)
            
            if verbose:
                print(f"üßπ Groupe : {group} ‚Üí garde {keep}, retire {drop}")
                if protected_in_group:
                    print(f"üõ°Ô∏è   Prot√©g√©es dans ce groupe : {protected_in_group}")
        
        # üõ°Ô∏è Les colonnes prot√©g√©es sont toujours gard√©es
        for protected in protected_in_group:
            if protected not in to_keep:
                to_keep.append(protected)

    to_drop = sorted(set(to_drop))
    to_keep = sorted(set(to_keep))

    # Colonnes binaires restantes (non corr√©l√©es)
    all_binary = [col for col in df.select_dtypes(include=['int64', 'Int64']).columns 
                  if col != target_col]
    untouched = [col for col in all_binary if col not in to_drop + to_keep]

    # üìå CONSTRUCTION DE L'ORDRE FINAL DES COLONNES
    
    # 1. Colonnes prioritaires (en premier)
    priority_existing = [col for col in priority_cols if col in df.columns]
    
    # 2. Variable cible (apr√®s les prioritaires)
    target_existing = [target_col] if target_col and target_col in df.columns else []
    
    # 3. Extra cols (variables transform√©es, etc.)
    extra_existing = []
    if extra_cols:
        extra_existing = [col for col in extra_cols 
                         if col in df.columns and col not in priority_existing + target_existing]
    
    # 4. Variables gard√©es par corr√©lation (pas d√©j√† dans prioritaires/extra)
    kept_remaining = [col for col in to_keep 
                     if col not in priority_existing + target_existing + extra_existing]
    
    # 5. Variables intactes (pas d√©j√† list√©es)
    untouched_remaining = [col for col in untouched 
                          if col not in priority_existing + target_existing + extra_existing + kept_remaining]
    
    # üõ°Ô∏è S'assurer que les colonnes prot√©g√©es sont pr√©sentes
    protected_remaining = []
    for protected in protected_cols:
        if (protected in df.columns and 
            protected not in priority_existing + target_existing + extra_existing + 
            kept_remaining + untouched_remaining):
            protected_remaining.append(protected)
    
    # üìå ORDRE FINAL : prioritaires ‚Üí cible ‚Üí extra ‚Üí gard√©es ‚Üí intactes ‚Üí prot√©g√©es restantes
    final_cols = (priority_existing + target_existing + extra_existing + 
                  kept_remaining + untouched_remaining + protected_remaining)
    
    # Filtrage des colonnes existantes (s√©curit√©)
    existing_cols = [col for col in final_cols if col in df.columns]
    df_reduced = df[existing_cols].copy()

    # Affichage du r√©sum√©
    if summary:
        print(f"\nüìä R√©duction : {len(to_drop)} supprim√©es, {len(to_keep)} gard√©es, {len(untouched)} intactes.")
        print(f"üìå Ordre final : {priority_existing[:3]}{'...' if len(priority_existing) > 3 else ''} ‚Üí {target_existing} ‚Üí reste")
        
        if protected_cols:
            protected_in_final = [col for col in protected_cols if col in existing_cols]
            print(f"üõ°Ô∏è {len(protected_in_final)} colonnes prot√©g√©es : {protected_in_final}")
        
        if extra_cols:
            existing_extra = [col for col in extra_cols if col in existing_cols]
            print(f"üß© {len(existing_extra)} extra conserv√©es : {existing_extra}")
        
        print(f"üìê Dimensions : {df_reduced.shape}")

    return df_reduced, to_drop, to_keep


# ============================================================================
# 4. FONCTIONS DE VALIDATION ET PROTECTION X4
# ============================================================================

def validate_x4_presence(df: pd.DataFrame, step_name: str = "", verbose: bool = True) -> bool:
    """
    Valide que X4 est pr√©sente et correcte dans le DataFrame.
    """
    if 'X4' not in df.columns:
        if verbose:
            print(f"‚ùå {step_name}: X4 MANQUANTE !")
        return False
    
    # V√©rifier le type et les valeurs
    unique_vals = sorted(df['X4'].dropna().unique())
    expected_vals = [0, 1]
    
    if set(unique_vals).issubset(set(expected_vals)):
        if verbose:
            print(f"‚úÖ {step_name}: X4 pr√©sente et correcte (valeurs: {unique_vals})")
        return True
    else:
        if verbose:
            print(f"‚ö†Ô∏è {step_name}: X4 pr√©sente mais valeurs inattendues: {unique_vals}")
        return False


def quick_x4_check(df_or_dict, name: str = "Dataset") -> bool:
    """
    V√©rification rapide de X4 dans un DataFrame ou dictionnaire de DataFrames.
    """
    if isinstance(df_or_dict, dict):
        print(f"üîç V√©rification X4 dans {len(df_or_dict)} datasets:")
        all_good = True
        for dataset_name, df in df_or_dict.items():
            has_x4 = 'X4' in df.columns if df is not None else False
            print(f"  {dataset_name}: {'‚úÖ' if has_x4 else '‚ùå'}")
            if not has_x4:
                all_good = False
        return all_good
    else:
        # DataFrame unique
        has_x4 = 'X4' in df_or_dict.columns
        print(f"üîç {name}: {'‚úÖ' if has_x4 else '‚ùå'} X4")
        return has_x4


# ============================================================================
# 5. GESTION DE L'ORDRE DES COLONNES
# ============================================================================

def reorder_columns_priority(
    df: pd.DataFrame, 
    priority_cols: List[str] = None,
    target_col: str = "outcome"
) -> pd.DataFrame:
    """
    R√©organise les colonnes avec un ordre prioritaire.
    """
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']
    
    current_cols = df.columns.tolist()
    
    # 1. Variables prioritaires (en premier)
    final_priority = [col for col in priority_cols if col in current_cols]
    
    # 2. Variable cible (apr√®s les prioritaires)
    final_target = [target_col] if target_col and target_col in current_cols else []
    
    # 3. Toutes les autres colonnes (dans l'ordre actuel)
    final_others = [col for col in current_cols if col not in final_priority + final_target]
    
    # Ordre final : prioritaires ‚Üí cible ‚Üí reste
    final_order = final_priority + final_target + final_others
    
    return df[final_order]


# ============================================================================
# 6. PIPELINE PRINCIPAL DE PR√âTRAITEMENT (VERSION 3.0 COMPL√àTE)
# ============================================================================

def prepare_final_dataset(
    file_path: Union[str, Path],
    strategy: str = "mixed_mar_mcar",
    mar_method: str = "knn",
    knn_k: Optional[int] = None,
    mar_cols: List[str] = ["X1_trans", "X2_trans", "X3_trans"],
    mcar_cols: List[str] = ["X4"],
    drop_outliers: bool = False,
    correlation_threshold: float = 0.90,
    save_transformer: bool = False,
    processed_data_dir: Optional[Union[str, Path]] = None,
    models_dir: Optional[Union[str, Path]] = None,
    display_info: bool = True,
    raw_data_dir: Optional[Union[str, Path]] = None,
    require_outcome: bool = True,
    protect_x4: bool = True,
    priority_cols: List[str] = None,
    return_objects: bool = False,
    use_comprehensive_correlation: bool = True
) -> Union[pd.DataFrame, Tuple[pd.DataFrame, Dict]]:
    """
    Pipeline de pr√©traitement complet avec toutes les corrections et am√©liorations.
    
    VERSION 3.0 - COMPL√àTEMENT CORRIG√âE ET OPTIMIS√âE
    
    Args:
        file_path: Chemin vers le fichier de donn√©es
        strategy: Strat√©gie d'imputation ("mixed_mar_mcar")
        mar_method: M√©thode d'imputation MAR ("knn" ou "mice")
        knn_k: Param√®tre k pour KNN (None = auto)
        mar_cols: Colonnes √† imputer avec m√©thode MAR
        mcar_cols: Colonnes √† imputer avec m√©thode MCAR
        drop_outliers: Supprimer les outliers
        correlation_threshold: Seuil de corr√©lation pour suppression
        save_transformer: Sauvegarder les transformateurs
        processed_data_dir: Dossier de sauvegarde des donn√©es
        models_dir: Dossier de sauvegarde des mod√®les
        display_info: Affichage des informations
        raw_data_dir: Dossier des donn√©es brutes
        require_outcome: N√©cessite la variable cible
        protect_x4: Prot√©ger X4 de la suppression
        priority_cols: Colonnes prioritaires pour l'ordre
        return_objects: Retourner aussi les objets de transformation
        use_comprehensive_correlation: Utiliser la nouvelle m√©thode de corr√©lation
        
    Returns:
        DataFrame pr√©trait√© (et objets si demand√©)
        
    Raises:
        ValueError: Si X4 est perdue pendant le preprocessing
    """
    
    # üõ°Ô∏è Configuration de protection
    protected_cols = ['X4'] if protect_x4 else []
    
    # üìå Colonnes prioritaires par d√©faut
    if priority_cols is None:
        priority_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4']

    # Dictionnaire pour stocker les objets de transformation
    transform_objects = {
        'scaler': None,
        'imputer': None,
        'yeojohnson': None,
        'correlation_info': None
    }

    if display_info:
        print("üîÑ D√âMARRAGE DU PIPELINE DE PR√âTRAITEMENT (VERSION 3.0)")
        print("=" * 70)

    # ========================================================================
    # √âTAPE 1: CHARGEMENT DES DONN√âES
    # ========================================================================
    
    if display_info:
        print("üìÇ √âtape 1: Chargement des donn√©es...")
    
    try:
        df = load_and_clean_data(
            file_path=file_path,
            require_outcome=require_outcome,
            display_info=display_info,
            raw_data_dir=raw_data_dir,
            encode_target=True
        )
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement : {e}")
        raise
    
    # üõ°Ô∏è Validation X4 apr√®s chargement
    if protect_x4:
        validate_x4_presence(df, "Apr√®s chargement", display_info)

    # ========================================================================
    # √âTAPE 2: CONVERSION DE X4
    # ========================================================================
    
    if display_info:
        print("\nüîß √âtape 2: Conversion de X4...")
    
    df = convert_X4_to_int(df, verbose=display_info)
    
    # üõ°Ô∏è Validation X4 apr√®s conversion
    if protect_x4:
        validate_x4_presence(df, "Apr√®s conversion X4", display_info)

    # ========================================================================
    # √âTAPE 3: TRANSFORMATION YEO-JOHNSON
    # ========================================================================
    
    if display_info:
        print("\nüîÑ √âtape 3: Transformation Yeo-Johnson (X1, X2, X3)...")
    
    try:
        if return_objects:
            df, yeojohnson_transformer = apply_yeojohnson(
                df=df,
                columns=["X1", "X2", "X3"],
                standardize=False,
                save_model=save_transformer,
                model_path=models_dir / "yeojohnson.pkl" if save_transformer and models_dir else None,
                return_transformer=True
            )
            transform_objects['yeojohnson'] = yeojohnson_transformer
        else:
            df = apply_yeojohnson(
                df=df,
                columns=["X1", "X2", "X3"],
                standardize=False,
                save_model=save_transformer,
                model_path=models_dir / "yeojohnson.pkl" if save_transformer and models_dir else None,
                return_transformer=False
            )
        
        # Suppression des colonnes originales
        df.drop(columns=["X1", "X2", "X3"], inplace=True, errors="ignore")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la transformation Yeo-Johnson : {e}")
        if display_info:
            print("‚ö†Ô∏è Poursuite sans transformation...")
    
    # üõ°Ô∏è Validation X4 apr√®s transformation
    if protect_x4:
        validate_x4_presence(df, "Apr√®s Yeo-Johnson", display_info)

    # ========================================================================
    # √âTAPE 4: IMPUTATION DES VALEURS MANQUANTES
    # ========================================================================
    
    if display_info:
        print(f"\nüîß √âtape 4: Imputation des valeurs manquantes ({mar_method})...")
    
    try:
        df = handle_missing_values(
            df=df,
            strategy=strategy,
            mar_method=mar_method,
            knn_k=knn_k,
            mar_cols=mar_cols,
            mcar_cols=mcar_cols,
            display_info=display_info,
            save_results=False,
            processed_data_dir=processed_data_dir,
            models_dir=models_dir
        )
    except Exception as e:
        print(f"‚ùå Erreur lors de l'imputation : {e}")
        if display_info:
            print("‚ö†Ô∏è Poursuite sans imputation...")
    
    # üõ°Ô∏è Validation X4 apr√®s imputation
    if protect_x4:
        validate_x4_presence(df, "Apr√®s imputation", display_info)

    # ========================================================================
    # √âTAPE 5: R√âDUCTION DE LA COLIN√âARIT√â (VERSION 3.0 COMPL√àTE)
    # ========================================================================
    
    if display_info:
        print(f"\nüîó √âtape 5: R√©duction de la colin√©arit√© (seuil={correlation_threshold})...")
    
    try:
        if use_comprehensive_correlation:
            # üöÄ NOUVELLE M√âTHODE COMPREHENSIVE
            df_reduced, correlation_report = apply_correlation_reduction_to_dataset(
                df, threshold=correlation_threshold
            )
            df = df_reduced
            transform_objects['correlation_info'] = correlation_report
            
        else:
            # üîß ANCIENNE M√âTHODE CORRIG√âE (POUR COMPATIBILIT√â)
            binary_vars = [col for col in df.columns 
                           if pd.api.types.is_integer_dtype(df[col]) and col != "outcome"]
            
            if display_info:
                print(f"üî¢ Variables binaires candidates : {len(binary_vars)}")

            if binary_vars:
                groups_corr = find_highly_correlated_groups(
                    df[binary_vars], 
                    threshold=correlation_threshold,
                    protected_cols=protected_cols
                )
                
                # üîß VALIDATION DU TYPE DE RETOUR RENFORC√âE
                if isinstance(groups_corr, list):
                    if display_info:
                        print("‚ö†Ô∏è Format de retour d√©tect√© comme liste - conversion en dictionnaire")
                    groups_corr = {
                        "groups": groups_corr,
                        "to_drop": [],
                        "protected": protected_cols
                    }
                elif not isinstance(groups_corr, dict):
                    if display_info:
                        print(f"‚ö†Ô∏è Type de retour inattendu: {type(groups_corr)} - utilisation valeurs par d√©faut")
                    groups_corr = {
                        "groups": [],
                        "to_drop": [],
                        "protected": protected_cols
                    }
                elif "groups" not in groups_corr:
                    if display_info:
                        print("‚ö†Ô∏è Cl√© 'groups' manquante - ajout de structure par d√©faut")
                    groups_corr["groups"] = []
                    if "to_drop" not in groups_corr:
                        groups_corr["to_drop"] = []
                    if "protected" not in groups_corr:
                        groups_corr["protected"] = protected_cols
                
                # Stockage des informations de corr√©lation
                transform_objects['correlation_info'] = groups_corr
                
            else:
                # Pas de variables binaires √† analyser
                groups_corr = {
                    "groups": [],
                    "to_drop": [],
                    "protected": protected_cols
                }
                if display_info:
                    print("‚ö†Ô∏è Aucune variable binaire trouv√©e pour l'analyse de corr√©lation")

            target_col = "outcome" if "outcome" in df.columns and require_outcome else None

            # üõ°Ô∏è Protection dans drop_correlated_duplicates
            df_reduced, dropped_cols, kept_cols = drop_correlated_duplicates(
                df=df,
                groups=groups_corr["groups"],
                target_col=target_col,
                extra_cols=mar_cols + mcar_cols,
                protected_cols=protected_cols,
                priority_cols=priority_cols,
                verbose=False,
                summary=display_info
            )
            
            # R√©assignation du DataFrame
            df = df_reduced
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©duction de colin√©arit√© : {e}")
        if display_info:
            print("‚ö†Ô∏è Poursuite sans r√©duction de colin√©arit√©...")
        
        # Mode d√©grad√© - Pas de r√©duction
        if 'correlation_info' not in transform_objects:
            transform_objects['correlation_info'] = {
                "error": str(e),
                "original_shape": df.shape,
                "reduced_shape": df.shape,
                "dimension_reduction": 0
            }
    
    # üõ°Ô∏è Validation X4 apr√®s r√©duction colin√©arit√©
    if protect_x4:
        validate_x4_presence(df, "Apr√®s r√©duction colin√©arit√©", display_info)

    # ========================================================================
    # √âTAPE 6: SUPPRESSION DES OUTLIERS (OPTIONNELLE)
    # ========================================================================
    
    target_col = "outcome" if "outcome" in df.columns and require_outcome else None
    
    if drop_outliers and target_col:
        if display_info:
            print(f"\nüéØ √âtape 6: Suppression des outliers...")
        
        try:
            df = detect_and_remove_outliers(
                df=df,
                columns=mar_cols,
                method='iqr',
                remove=True,
                verbose=display_info
            )
        except Exception as e:
            print(f"‚ùå Erreur lors de la suppression des outliers : {e}")
            if display_info:
                print("‚ö†Ô∏è Poursuite sans suppression des outliers...")
        
        # üõ°Ô∏è Validation X4 apr√®s suppression outliers
        if protect_x4:
            validate_x4_presence(df, "Apr√®s suppression outliers", display_info)
    elif display_info:
        print(f"\n‚è≠Ô∏è √âtape 6: Suppression des outliers ignor√©e (drop_outliers={drop_outliers})")

    # ========================================================================
    # √âTAPE 7: SUPPRESSION DES COLONNES DUPLIQU√âES
    # ========================================================================
    
    duplicate_cols = df.columns[df.columns.duplicated()].tolist()
    if duplicate_cols:
        if display_info:
            print(f"\nüîÑ √âtape 7: Suppression des colonnes dupliqu√©es...")
        
        # üõ°Ô∏è V√©rifier qu'on ne supprime pas X4 par accident
        if 'X4' in duplicate_cols and protect_x4:
            print("üõ°Ô∏è ALERTE: X4 d√©tect√©e comme dupliqu√©e - protection activ√©e")
            # Garder la premi√®re occurrence de X4
            df = df.loc[:, ~df.columns.duplicated(keep='first')]
        else:
            df = df.loc[:, ~df.columns.duplicated()]
            
        if display_info:
            print(f"‚ö†Ô∏è Colonnes dupliqu√©es d√©tect√©es : {duplicate_cols}")
            print(f"üîπ Duplication supprim√©e : {df.shape}")
        
        # üõ°Ô∏è Validation X4 apr√®s suppression doublons
        if protect_x4:
            validate_x4_presence(df, "Apr√®s suppression doublons", display_info)
    elif display_info:
        print(f"\n‚úÖ √âtape 7: Aucune colonne dupliqu√©e d√©tect√©e")

    # ========================================================================
    # √âTAPE 8: R√âORGANISATION FINALE DES COLONNES
    # ========================================================================
    
    if display_info:
        print(f"\nüìå √âtape 8: R√©organisation finale des colonnes...")
    
    try:
        # Utiliser la fonction d√©di√©e pour la r√©organisation
        df = reorder_columns_priority(df, priority_cols, target_col)
        
        if display_info:
            print(f"üìå Ordre final : {priority_cols} ‚Üí [{target_col}] ‚Üí autres")
            print(f"üìå Premi√®res colonnes : {df.columns[:min(8, len(df.columns))].tolist()}")
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©organisation : {e}")
        if display_info:
            print("‚ö†Ô∏è Poursuite avec ordre actuel...")

    # ========================================================================
    # √âTAPE 9: VALIDATION FINALE
    # ========================================================================
    
    if display_info:
        print(f"\nüîç √âtape 9: Validation finale...")
        print(f"‚úÖ Pipeline complet termin√© ‚Äì Dimensions finales : {df.shape}")
        
        # üõ°Ô∏è Validation finale X4
        if protect_x4:
            final_status = validate_x4_presence(df, "VALIDATION FINALE", True)
            if not final_status:
                print("üö® ERREUR CRITIQUE: X4 manquante en fin de pipeline !")
                raise ValueError("X4 a √©t√© perdue pendant le preprocessing !")

    # ========================================================================
    # √âTAPE 10: SAUVEGARDE
    # ========================================================================
    
    if processed_data_dir:
        if display_info:
            print(f"\nüíæ √âtape 10: Sauvegarde...")
        
        try:
            processed_data_dir = Path(processed_data_dir)
            processed_data_dir.mkdir(parents=True, exist_ok=True)
            suffix = f"{mar_method}{'_no_outliers' if drop_outliers else ''}"
            filename = f"final_dataset_{suffix}.parquet"
            df.to_parquet(processed_data_dir / filename, index=False)
            
            if display_info:
                print(f"üíæ Sauvegarde Parquet : {processed_data_dir / filename}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde : {e}")
    elif display_info:
        print(f"\n‚è≠Ô∏è √âtape 10: Sauvegarde ignor√©e (processed_data_dir=None)")

    if display_info:
        print("\n" + "=" * 70)
        print("üéâ PIPELINE DE PR√âTRAITEMENT TERMIN√â AVEC SUCC√àS (VERSION 3.0)")
        print("=" * 70)

    # Retour selon les options
    if return_objects:
        return df, transform_objects
    else:
        return df


# ============================================================================
# 7. FONCTIONS UTILITAIRES POUR DATASETS EXISTANTS
# ============================================================================

def apply_full_preprocessing_to_existing(
    df: pd.DataFrame,
    **kwargs
) -> pd.DataFrame:
    """
    Applique le pr√©traitement complet √† un DataFrame existant.
    
    Args:
        df: DataFrame √† pr√©traiter
        **kwargs: Arguments √† passer √† prepare_final_dataset
        
    Returns:
        DataFrame pr√©trait√©
        
    Note:
        Cette fonction sauvegarde temporairement le DataFrame et utilise prepare_final_dataset
    """
    import tempfile
    
    # Sauvegarde temporaire
    with tempfile.NamedTemporaryFile(suffix='.csv', delete=False) as tmp_file:
        df.to_csv(tmp_file.name, index=False)
        
        # Application du pipeline
        result = prepare_final_dataset(
            file_path=tmp_file.name,
            raw_data_dir=Path(tmp_file.name).parent,
            **kwargs
        )
        
        # Nettoyage
        Path(tmp_file.name).unlink()
        
    return result


def batch_process_datasets(
    datasets_dict: Dict[str, Union[pd.DataFrame, str, Path]],
    **kwargs
) -> Dict[str, pd.DataFrame]:
    """
    Traite plusieurs datasets en lot avec le pipeline complet.
    
    Args:
        datasets_dict: Dictionnaire {nom: DataFrame ou chemin vers fichier}
        **kwargs: Arguments √† passer √† prepare_final_dataset
        
    Returns:
        Dictionnaire des datasets trait√©s
    """
    processed_datasets = {}
    
    for name, data in datasets_dict.items():
        print(f"\nüîÑ Traitement de {name}...")
        
        try:
            if isinstance(data, pd.DataFrame):
                # DataFrame existant
                result = apply_full_preprocessing_to_existing(data, **kwargs)
            else:
                # Chemin vers fichier
                result = prepare_final_dataset(file_path=data, **kwargs)
            
            processed_datasets[name] = result
            print(f"‚úÖ {name} trait√© avec succ√®s : {result.shape}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement de {name} : {e}")
            processed_datasets[name] = None
    
    return processed_datasets


# ============================================================================
# 8. FONCTIONS DE DIAGNOSTIC ET DEBUG AVANC√âES
# ============================================================================

def prepare_dataset_safe(file_path: Union[str, Path], **kwargs) -> pd.DataFrame:
    """
    Version s√©curis√©e du pipeline avec gestion d'erreurs automatique.
    
    Args:
        file_path: Chemin vers le fichier
        **kwargs: Arguments pour prepare_final_dataset
        
    Returns:
        DataFrame pr√©trait√©
    """
    print("üîí PIPELINE S√âCURIS√â - VERSION 3.0")
    print("=" * 45)
    
    # Param√®tres par d√©faut s√©curis√©s
    safe_defaults = {
        'strategy': 'mixed_mar_mcar',
        'mar_method': 'knn',
        'correlation_threshold': 0.90,
        'drop_outliers': False,
        'protect_x4': True,
        'display_info': True,
        'use_comprehensive_correlation': True
    }
    
    # Fusion avec les param√®tres utilisateur
    final_params = {**safe_defaults, **kwargs}
    
    try:
        # Tentative normale
        df_result = prepare_final_dataset(file_path=file_path, **final_params)
        print("‚úÖ Pipeline ex√©cut√© avec succ√®s en mode normal")
        return df_result
        
    except TypeError as e:
        if "list indices must be integers" in str(e) or "groups" in str(e):
            print("üîß Erreur de corr√©lation d√©tect√©e - application du mode de r√©cup√©ration...")
            
            # Mode de r√©cup√©ration avec m√©thode ancienne
            recovery_params = final_params.copy()
            recovery_params['use_comprehensive_correlation'] = False
            recovery_params['correlation_threshold'] = 0.95
            recovery_params['display_info'] = True
            
            try:
                df_result = prepare_final_dataset(file_path=file_path, **recovery_params)
                print("‚úÖ Pipeline ex√©cut√© avec succ√®s en mode r√©cup√©ration")
                return df_result
            except Exception as e2:
                print(f"‚ùå √âchec en mode r√©cup√©ration: {e2}")
                raise
                
    except Exception as e:
        print(f"‚ùå Erreur critique: {e}")
        print("üîç Mode d√©grad√© activ√©...")
        
        # Mode d√©grad√© - sans corr√©lation
        degraded_params = final_params.copy()
        degraded_params['correlation_threshold'] = 1.0  # Pas de suppression
        degraded_params['display_info'] = True
        
        try:
            df_result = prepare_final_dataset(file_path=file_path, **degraded_params)
            print("‚ö†Ô∏è Pipeline ex√©cut√© en mode d√©grad√© (sans r√©duction de corr√©lation)")
            return df_result
        except Exception as e3:
            print(f"‚ùå √âchec total: {e3}")
            raise


def get_preprocessing_summary(df: pd.DataFrame) -> Dict:
    """
    G√©n√®re un r√©sum√© des caract√©ristiques du dataset pr√©trait√©.
    
    Args:
        df: DataFrame pr√©trait√©
        
    Returns:
        Dictionnaire avec le r√©sum√©
    """
    summary = {
        'shape': df.shape,
        'columns': df.columns.tolist(),
        'dtypes': df.dtypes.to_dict(),
        'missing_values': df.isnull().sum().to_dict(),
        'has_x4': 'X4' in df.columns,
        'has_outcome': 'outcome' in df.columns,
        'binary_vars': [],
        'continuous_vars': [],
        'memory_usage': df.memory_usage(deep=True).sum() / 1024**2  # MB
    }
    
    # Classification des variables
    for col in df.columns:
        if col == 'outcome':
            continue
        elif pd.api.types.is_integer_dtype(df[col]):
            summary['binary_vars'].append(col)
        elif pd.api.types.is_float_dtype(df[col]):
            summary['continuous_vars'].append(col)
    
    # Statistiques additionnelles
    if 'outcome' in df.columns:
        summary['class_distribution'] = df['outcome'].value_counts().to_dict()
        summary['class_balance'] = df['outcome'].value_counts(normalize=True).to_dict()
    
    return summary


def print_preprocessing_summary(df: pd.DataFrame):
    """Affiche un r√©sum√© format√© du dataset pr√©trait√©."""
    
    summary = get_preprocessing_summary(df)
    
    print("\nüìä R√âSUM√â DU DATASET PR√âTRAIT√â")
    print("=" * 40)
    
    print(f"üìê Dimensions: {summary['shape'][0]} lignes √ó {summary['shape'][1]} colonnes")
    print(f"üíæ M√©moire utilis√©e: {summary['memory_usage']:.2f} MB")
    print(f"üõ°Ô∏è X4 pr√©sente: {'‚úÖ' if summary['has_x4'] else '‚ùå'}")
    print(f"üéØ Outcome pr√©sente: {'‚úÖ' if summary['has_outcome'] else '‚ùå'}")
    
    # Variables par type
    print(f"\nüî¢ Variables binaires ({len(summary['binary_vars'])}): {summary['binary_vars'][:5]}{'...' if len(summary['binary_vars']) > 5 else ''}")
    print(f"üìà Variables continues ({len(summary['continuous_vars'])}): {summary['continuous_vars'][:5]}{'...' if len(summary['continuous_vars']) > 5 else ''}")
    
    # Valeurs manquantes
    missing_count = sum(v for v in summary['missing_values'].values() if v > 0)
    if missing_count > 0:
        print(f"üíß Valeurs manquantes: {missing_count} au total")
        missing_cols = {k: v for k, v in summary['missing_values'].items() if v > 0}
        for col, count in list(missing_cols.items())[:3]:
            print(f"   {col}: {count}")
    else:
        print("üíß Valeurs manquantes: ‚úÖ Aucune")
    
    # Distribution des classes
    if 'class_distribution' in summary:
        print(f"\nüéØ Distribution des classes:")
        for classe, count in summary['class_distribution'].items():
            pct = summary['class_balance'][classe] * 100
            print(f"   Classe {classe}: {count} ({pct:.1f}%)")


# ============================================================================
# 9. FONCTIONS DE VISUALISATION ET RAPPORT
# ============================================================================

def visualize_correlation_impact(corr_report, save_path=None):
    """Visualise l'impact de la r√©duction de corr√©lation."""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Graphique 1: Avant/Apr√®s
    categories = ['Variables\noriginales', 'Variables\nfinales']
    values = [corr_report['original_shape'][1], corr_report['reduced_shape'][1]]
    colors = ['lightcoral', 'lightblue']
    
    bars1 = ax1.bar(categories, values, color=colors)
    ax1.set_title('Impact de la r√©duction de corr√©lation')
    ax1.set_ylabel('Nombre de variables')
    
    # Ajouter les valeurs sur les barres
    for bar, value in zip(bars1, values):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, 
                str(value), ha='center', va='bottom', fontweight='bold')
    
    # Graphique 2: Breakdown
    labels = ['Variables\nsupprim√©es', 'Variables\nconserv√©es']
    sizes = [corr_report['vars_dropped'], corr_report['reduced_shape'][1]]
    colors2 = ['lightcoral', 'lightgreen']
    
    ax2.pie(sizes, labels=labels, colors=colors2, autopct='%1.1f%%', startangle=90)
    ax2.set_title('R√©partition des variables')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()


def create_preprocessing_report(
    datasets_dict: Dict[str, pd.DataFrame],
    validation_reports: Dict[str, Dict],
    output_path: Union[str, Path],
    transform_objects: Dict = None
) -> None:
    """
    Cr√©e un rapport d√©taill√© du pr√©traitement.
    
    Args:
        datasets_dict: Dictionnaire des datasets
        validation_reports: Rapports de validation
        output_path: Chemin de sauvegarde du rapport
        transform_objects: Objets de transformation utilis√©s
    """
    from datetime import datetime
    
    report_content = f"""
# RAPPORT DE PR√âTRAITEMENT STA211 - VERSION 3.0
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## R√©sum√©

- **Nombre de datasets trait√©s**: {len(datasets_dict)}
- **Datasets valides**: {sum(1 for r in validation_reports.values() if r.get('status') == 'success')}
- **Pipeline version**: 3.0 (Compl√®tement Corrig√©e)

## Am√©liorations Version 3.0

- ‚úÖ Int√©gration analyse de corr√©lation comprehensive
- ‚úÖ R√©duction de dimensionnalit√© efficace
- ‚úÖ Fix TypeError complet dans find_highly_correlated_groups
- ‚úÖ Validation robuste des types de retour
- ‚úÖ Gestion d'erreurs am√©lior√©e √† tous les niveaux
- ‚úÖ Protection X4 renforc√©e
- ‚úÖ Fonctions de diagnostic avanc√©es

## D√©tails par dataset

"""
    
    for name, report in validation_reports.items():
        if report['status'] == 'error':
            report_content += f"### ‚ùå {name}\n- **Statut**: Erreur\n- **Message**: {report['message']}\n\n"
            continue
        
        df = datasets_dict[name]
        report_content += f"""### ‚úÖ {name}

- **Dimensions**: {report['shape']}
- **Score qualit√©**: {report['quality_score']:.2f}/1.0
- **X4 pr√©sente**: {'‚úÖ' if report['has_x4'] else '‚ùå'}
- **Outcome pr√©sente**: {'‚úÖ' if report['has_outcome'] else '‚ùå'}
- **Valeurs manquantes**: {report['missing_values']}
- **Premi√®res colonnes**: {', '.join(report['first_columns'])}

#### Statistiques descriptives (premi√®res variables)
```
{df[df.columns[:5]].describe().round(3).to_string()}
```

"""
    
    # Informations sur les transformations si disponibles
    if transform_objects:
        report_content += "\n## Objets de transformation\n\n"
        for obj_name, obj in transform_objects.items():
            if obj is not None:
                report_content += f"- **{obj_name}**: {type(obj).__name__}\n"
    
    # Sauvegarde du rapport
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"üìù Rapport sauvegard√©: {output_path}")
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde rapport: {e}")


# ============================================================================
# 10. FONCTIONS DE TEST ET VALIDATION COMPL√àTES
# ============================================================================

def run_comprehensive_test(file_path: Union[str, Path]) -> Dict:
    """Execute un test complet du pipeline avec diagnostic."""
    
    print("üß™ TEST COMPLET DU PIPELINE VERSION 3.0")
    print("=" * 50)
    
    # Test 1: Pipeline de base
    print("\n1Ô∏è‚É£ Test pipeline de base...")
    try:
        df_result = prepare_final_dataset(
            file_path=file_path,
            strategy="mixed_mar_mcar",
            mar_method="knn",
            correlation_threshold=0.90,
            drop_outliers=False,
            display_info=False
        )
        basic_success = True
        print(f"‚úÖ Pipeline de base r√©ussi - Shape: {df_result.shape}")
    except Exception as e:
        basic_success = False
        print(f"‚ùå Pipeline de base √©chou√©: {e}")
    
    # Test 2: Pipeline avec objets
    print("\n2Ô∏è‚É£ Test pipeline avec objets de transformation...")
    try:
        df_result, transform_objects = prepare_final_dataset(
            file_path=file_path,
            return_objects=True,
            display_info=False
        )
        objects_success = True
        print("‚úÖ Pipeline avec objets r√©ussi")
    except Exception as e:
        objects_success = False
        print(f"‚ùå Pipeline avec objets √©chou√©: {e}")
        transform_objects = {}
    
    # Test 3: M√©thode comprehensive de corr√©lation
    print("\n3Ô∏è‚É£ Test m√©thode comprehensive de corr√©lation...")
    try:
        df_comprehensive = prepare_final_dataset(
            file_path=file_path,
            use_comprehensive_correlation=True,
            display_info=False
        )
        comprehensive_success = True
        print(f"‚úÖ M√©thode comprehensive r√©ussie - Shape: {df_comprehensive.shape}")
    except Exception as e:
        comprehensive_success = False
        print(f"‚ùå M√©thode comprehensive √©chou√©e: {e}")
    
    # Test 4: M√©thode ancienne (compatibilit√©)
    print("\n4Ô∏è‚É£ Test m√©thode ancienne (compatibilit√©)...")
    try:
        df_legacy = prepare_final_dataset(
            file_path=file_path,
            use_comprehensive_correlation=False,
            display_info=False
        )
        legacy_success = True
        print(f"‚úÖ M√©thode ancienne r√©ussie - Shape: {df_legacy.shape}")
    except Exception as e:
        legacy_success = False
        print(f"‚ùå M√©thode ancienne √©chou√©e: {e}")
    
    # Test 5: Validation X4
    print("\n5Ô∏è‚É£ Validation protection X4...")
    x4_protected = True
    if basic_success:
        x4_protected = 'X4' in df_result.columns
        print(f"üõ°Ô∏è X4 prot√©g√©e: {'‚úÖ' if x4_protected else '‚ùå'}")
    
    # Test 6: Pipeline s√©curis√©
    print("\n6Ô∏è‚É£ Test pipeline s√©curis√©...")
    try:
        df_safe = prepare_dataset_safe(file_path)
        safe_success = True
        print(f"‚úÖ Pipeline s√©curis√© r√©ussi - Shape: {df_safe.shape}")
    except Exception as e:
        safe_success = False
        print(f"‚ùå Pipeline s√©curis√© √©chou√©: {e}")
    
    # R√©sum√© final
    print("\nüìä R√âSUM√â DU TEST COMPLET")
    print("=" * 30)
    print(f"Pipeline de base: {'‚úÖ' if basic_success else '‚ùå'}")
    print(f"Pipeline avec objets: {'‚úÖ' if objects_success else '‚ùå'}")
    print(f"M√©thode comprehensive: {'‚úÖ' if comprehensive_success else '‚ùå'}")
    print(f"M√©thode ancienne: {'‚úÖ' if legacy_success else '‚ùå'}")
    print(f"Protection X4: {'‚úÖ' if x4_protected else '‚ùå'}")
    print(f"Pipeline s√©curis√©: {'‚úÖ' if safe_success else '‚ùå'}")
    
    # Score global
    tests = [basic_success, objects_success, comprehensive_success, 
             legacy_success, x4_protected, safe_success]
    score = sum(tests) / len(tests)
    
    if score == 1.0:
        print(f"\nüèÜ SCORE PARFAIT: {score:.0%} - Tous les tests r√©ussis!")
    elif score >= 0.8:
        print(f"\n‚úÖ SCORE EXCELLENT: {score:.0%} - Pipeline op√©rationnel")
    elif score >= 0.6:
        print(f"\n‚ö†Ô∏è SCORE CORRECT: {score:.0%} - Quelques probl√®mes d√©tect√©s")
    else:
        print(f"\n‚ùå SCORE FAIBLE: {score:.0%} - R√©vision n√©cessaire")
    
    return {
        'basic_success': basic_success,
        'objects_success': objects_success,
        'comprehensive_success': comprehensive_success,
        'legacy_success': legacy_success,
        'x4_protected': x4_protected,
        'safe_success': safe_success,
        'score': score,
        'transform_objects': transform_objects if objects_success else {}
    }


def validate_all_datasets(
    datasets_dict: Dict[str, pd.DataFrame],
    expected_cols: List[str] = None,
    protect_x4: bool = True
) -> Dict[str, Dict]:
    """
    Valide la qualit√© de plusieurs datasets.
    
    Args:
        datasets_dict: Dictionnaire des datasets √† valider
        expected_cols: Colonnes attendues en premier
        protect_x4: V√©rifier la pr√©sence de X4
        
    Returns:
        Dictionnaire des rapports de validation
    """
    if expected_cols is None:
        expected_cols = ['X1_trans', 'X2_trans', 'X3_trans', 'X4', 'outcome']
    
    validation_reports = {}
    
    for name, df in datasets_dict.items():
        if df is None:
            validation_reports[name] = {'status': 'error', 'message': 'DataFrame vide'}
            continue
        
        # V√©rification ordre des colonnes
        current_order = df.columns.tolist()
        columns_order_correct = True
        for i, expected_col in enumerate(expected_cols[:5]):  # V√©rifier les 5 premi√®res
            if expected_col in current_order:
                actual_position = current_order.index(expected_col)
                if actual_position != i:
                    columns_order_correct = False
                    break
        
        report = {
            'status': 'success',
            'shape': df.shape,
            'columns_order_correct': columns_order_correct,
            'has_x4': 'X4' in df.columns if protect_x4 else True,
            'has_outcome': 'outcome' in df.columns,
            'missing_values': df.isnull().sum().sum(),
            'first_columns': df.columns[:min(8, len(df.columns))].tolist(),
            'dimension_reduction': None,
            'memory_usage': df.memory_usage(deep=True).sum() / 1024**2  # MB
        }
        
        # Score de qualit√© global
        quality_checks = [
            report['columns_order_correct'],
            report['has_x4'],
            report['has_outcome'],
            report['missing_values'] == 0,
            report['shape'][1] < 1000,  # R√©duction effective
            report['shape'][0] > 1000   # Suffisamment d'√©chantillons
        ]
        report['quality_score'] = sum(quality_checks) / len(quality_checks)
        
        validation_reports[name] = report
    
    return validation_reports


def print_validation_summary(validation_reports: Dict[str, Dict]):
    """
    Affiche un r√©sum√© des validations.
    
    Args:
        validation_reports: Rapports de validation
    """
    print("\nüìä R√âSUM√â DE LA VALIDATION DES DATASETS")
    print("=" * 60)
    
    for name, report in validation_reports.items():
        if report['status'] == 'error':
            print(f"‚ùå {name}: {report['message']}")
            continue
        
        quality = report['quality_score']
        status_icon = "‚úÖ" if quality == 1.0 else "‚ö†Ô∏è" if quality >= 0.75 else "‚ùå"
        
        print(f"{status_icon} {name}:")
        print(f"   üìê Shape: {report['shape']}")
        print(f"   üéØ Score qualit√©: {quality:.2f}/1.0")
        print(f"   üìå Ordre colonnes: {'‚úÖ' if report['columns_order_correct'] else '‚ùå'}")
        print(f"   üõ°Ô∏è X4 pr√©sente: {'‚úÖ' if report['has_x4'] else '‚ùå'}")
        print(f"   üéØ Outcome pr√©sente: {'‚úÖ' if report['has_outcome'] else '‚ùå'}")
        print(f"   üíß Valeurs manquantes: {report['missing_values']}")
        print(f"   üíæ M√©moire: {report['memory_usage']:.1f} MB")
        print(f"   üìã Premi√®res colonnes: {report['first_columns'][:4]}...")
        print()


# ============================================================================
# 11. FONCTIONS D'EXPORT ET SAUVEGARDE AVANC√âES
# ============================================================================

def export_datasets_multiple_formats(
    datasets_dict: Dict[str, pd.DataFrame],
    output_dir: Union[str, Path],
    formats: List[str] = ['parquet', 'csv'],
    compress: bool = True
) -> Dict[str, Dict[str, Path]]:
    """
    Exporte les datasets dans plusieurs formats.
    
    Args:
        datasets_dict: Dictionnaire des datasets
        output_dir: Dossier de sortie
        formats: Formats d'export ('parquet', 'csv', 'xlsx')
        compress: Compresser les fichiers
        
    Returns:
        Dictionnaire des chemins de sauvegarde
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    export_paths = {}
    
    for dataset_name, df in datasets_dict.items():
        if df is None:
            continue
        
        export_paths[dataset_name] = {}
        
        for fmt in formats:
            try:
                if fmt == 'parquet':
                    file_path = output_dir / f"{dataset_name}.parquet"
                    df.to_parquet(file_path, index=False, compression='snappy' if compress else None)
                    
                elif fmt == 'csv':
                    file_path = output_dir / f"{dataset_name}.csv"
                    compression = 'gzip' if compress else None
                    if compression:
                        file_path = file_path.with_suffix('.csv.gz')
                    df.to_csv(file_path, index=False, compression=compression)
                    
                elif fmt == 'xlsx':
                    file_path = output_dir / f"{dataset_name}.xlsx"
                    df.to_excel(file_path, index=False)
                
                export_paths[dataset_name][fmt] = file_path
                print(f"üíæ {dataset_name}.{fmt} sauvegard√©: {file_path}")
                
            except Exception as e:
                print(f"‚ùå Erreur sauvegarde {dataset_name}.{fmt}: {e}")
    
    return export_paths


# ============================================================================
# 12. FONCTIONS DE COMPARAISON ET BENCHMARKING
# ============================================================================

def compare_preprocessing_methods(
    file_path: Union[str, Path],
    methods: List[str] = None,
    thresholds: List[float] = None
) -> pd.DataFrame:
    """
    Compare diff√©rentes m√©thodes de pr√©traitement.
    
    Args:
        file_path: Chemin vers le fichier de donn√©es
        methods: M√©thodes √† comparer
        thresholds: Seuils de corr√©lation √† tester
        
    Returns:
        DataFrame de comparaison
    """
    if methods is None:
        methods = ['comprehensive', 'legacy']
    
    if thresholds is None:
        thresholds = [0.85, 0.90, 0.95]
    
    results = []
    
    for method in methods:
        for threshold in thresholds:
            config_name = f"{method}_t{threshold}"
            print(f"\nüß™ Test {config_name}...")
            
            try:
                start_time = pd.Timestamp.now()
                
                df_result = prepare_final_dataset(
                    file_path=file_path,
                    correlation_threshold=threshold,
                    use_comprehensive_correlation=(method == 'comprehensive'),
                    display_info=False
                )
                
                end_time = pd.Timestamp.now()
                duration = (end_time - start_time).total_seconds()
                
                result = {
                    'method': method,
                    'threshold': threshold,
                    'config_name': config_name,
                    'status': 'success',
                    'final_shape': df_result.shape,
                    'features_count': df_result.shape[1] - 1,  # -1 pour outcome
                    'reduction_rate': 1 - (df_result.shape[1] / 1559),  # Baseline 1559
                    'has_x4': 'X4' in df_result.columns,
                    'has_outcome': 'outcome' in df_result.columns,
                    'missing_values': df_result.isnull().sum().sum(),
                    'duration_seconds': duration,
                    'memory_usage_mb': df_result.memory_usage(deep=True).sum() / 1024**2
                }
                
            except Exception as e:
                result = {
                    'method': method,
                    'threshold': threshold,
                    'config_name': config_name,
                    'status': 'error',
                    'error': str(e),
                    'final_shape': None,
                    'features_count': None,
                    'reduction_rate': None,
                    'has_x4': False,
                    'has_outcome': False,
                    'missing_values': None,
                    'duration_seconds': None,
                    'memory_usage_mb': None
                }
            
            results.append(result)
    
    return pd.DataFrame(results)


# ============================================================================
# FIN DU MODULE - INFORMATIONS DE VERSION ET UTILISATION
# ============================================================================

__version__ = "3.0"
__status__ = "Compl√®tement Corrig√© et Optimis√©"
__new_features__ = [
    "Int√©gration analyse de corr√©lation comprehensive",
    "R√©duction de dimensionnalit√© efficace",
    "Fonctions de diagnostic avanc√©es",
    "Pipeline modulaire et robuste",
    "Fonctions de benchmarking et comparaison",
    "Export multi-formats",
    "Rapport automatique complet"
]
__corrections__ = [
    "Fix TypeError complet dans find_highly_correlated_groups",
    "Validation robuste des types de retour",
    "Gestion d'erreurs am√©lior√©e √† tous les niveaux",
    "Protection X4 renforc√©e",
    "Mode de r√©cup√©ration automatique",
    "Pipeline s√©curis√© avec fallbacks"
]


def print_version_info():
    """Affiche les informations de version du module."""
    print(f"\nüìã MODULE final_preprocessing.py")
    print(f"Version: {__version__}")
    print(f"Statut: {__status__}")
    
    print(f"\nüöÄ NOUVELLES FONCTIONNALIT√âS VERSION 3.0:")
    for feature in __new_features__:
        print(f"  ‚ú® {feature}")
    
    print(f"\nüîß CORRECTIONS APPORT√âES:")
    for correction in __corrections__:
        print(f"  ‚úÖ {correction}")


def print_usage_examples():
    """Affiche des exemples d'utilisation recommand√©s."""
    print(f"\nüöÄ EXEMPLES D'UTILISATION RECOMMAND√âS:")
    
    examples = """
# 1. Utilisation basique (recommand√©e)
df = prepare_dataset_safe('data_train.csv')

# 2. Pipeline complet avec objets
df, objects = prepare_final_dataset('data_train.csv', return_objects=True)

# 3. Configuration personnalis√©e
df = prepare_final_dataset(
    'data_train.csv',
    correlation_threshold=0.85,
    use_comprehensive_correlation=True,
    drop_outliers=False
)

# 4. Test complet du pipeline
test_results = run_comprehensive_test('data_train.csv')

# 5. Comparaison de m√©thodes
comparison = compare_preprocessing_methods('data_train.csv')

# 6. Traitement en lot
datasets = {'train': 'data_train.csv', 'test': 'data_test.csv'}
results = batch_process_datasets(datasets)

# 7. Export multi-formats
export_datasets_multiple_formats(
    {'final': df}, 
    'output/', 
    formats=['parquet', 'csv', 'xlsx']
)

# 8. R√©sum√© du preprocessing
print_preprocessing_summary(df)

# 9. Analyse de corr√©lation seule
df_reduced, report = apply_correlation_reduction_to_dataset(df)

# 10. Visualisation impact
visualize_correlation_impact(report)
"""
    
    print(examples)


if __name__ == "__main__":
    print_version_info()
    print_usage_examples()
    
    print(f"\nüí° CONSEILS D'UTILISATION:")
    print("‚Ä¢ Utilisez prepare_dataset_safe() pour une utilisation simple et s√©curis√©e")
    print("‚Ä¢ Activez use_comprehensive_correlation=True pour une r√©duction optimale")
    print("‚Ä¢ Testez avec run_comprehensive_test() avant utilisation en production")
    print("‚Ä¢ Surveillez la protection X4 avec les fonctions de validation")
    print("‚Ä¢ Exportez en Parquet pour des performances optimales")
    
    print(f"\nüÜò EN CAS DE PROBL√àME:")
    print("‚Ä¢ Le pipeline s√©curis√© inclut des modes de r√©cup√©ration automatiques")
    print("‚Ä¢ Utilisez display_info=True pour diagnostiquer les probl√®mes")
    print("‚Ä¢ Les fonctions de validation permettent de v√©rifier l'int√©grit√©")
    print("‚Ä¢ Consultez les rapports automatiques pour l'analyse d√©taill√©e")