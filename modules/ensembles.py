"""
Module de mod√©lisation d'ensemble pour le Notebook 3.

Fonctionnalit√©s:
- Entra√Ænement d'ensembles (Voting, Bagging, Stacking)
- Optimisation des m√©ta-mod√®les
- √âvaluation compar√©e des ensembles vs mod√®les individuels
- Sauvegarde et rechargement des pipelines d'ensemble

Auteur: Maoulida Abdoullatuf
Version: 1.0
"""

import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
import logging

# Imports pour le chargement de donn√©es
from modules.utils import load_artifact
from modules.config import cfg

# Scikit-learn imports
from sklearn.ensemble import (
    VotingClassifier, BaggingClassifier, RandomForestClassifier,
    ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier
)
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    f1_score, precision_score, recall_score, roc_auc_score,
    classification_report, confusion_matrix
)

# Imports du projet
from modules.config import cfg
from modules.utils.storage import save_artifact, load_artifact

# Configuration du logging
log = cfg.get_logger(__name__)


# =============================================================================
# 0. CHARGEMENT DE DONN√âES POUR STACKING
# =============================================================================

def load_stacking_data() -> Tuple[Dict, Dict, pd.DataFrame, Dict]:
    """
    Charge sp√©cifiquement les donn√©es n√©cessaires pour le stacking Notebook 03.
    Utilise le syst√®me load_artifact unifi√©.
    
    Returns:
        Tuple[splits, pipelines, metrics_df, features]
        - splits: Dict des donn√©es split√©es {method: {X_train, y_train, ...}}
        - pipelines: Dict des pipelines optimis√©s {method_version: {model: pipeline}}
        - metrics_df: DataFrame des m√©triques et seuils optimaux
        - features: Dict des colonnes de features {method: [list]}
    """
    log.info("üì¶ Chargement optimis√© des donn√©es pour le stacking...")
    
    models_dir = cfg.paths.models / "notebook2"
    
    # ‚úÖ 1. Chargement des splits (train/val/test)
    log.info("üìä Chargement des splits KNN et MICE...")
    splits = {}
    
    for method in ["knn", "mice"]:
        method_dir = models_dir / method
        splits[method] = {}
        
        for subset in ["train", "val", "test"]:
            try:
                data = load_artifact(f"{method}_{subset}.pkl", method_dir)
                splits[method][f"X_{subset}"] = data["X"]
                splits[method][f"y_{subset}"] = data["y"]
            except Exception as e:
                log.warning(f"‚ö†Ô∏è Erreur chargement {method} {subset}: {e}")
        
        if f"X_train" in splits[method]:
            log.info(f"‚úÖ {method.upper()} splits charg√©s : {splits[method]['X_train'].shape}")
    
    # ‚úÖ 2. Chargement des pipelines via les fichiers JSON
    log.info("üîß Chargement des pipelines optimis√©s...")
    pipelines = {}
    total_models = 0
    
    for method in ["knn", "mice"]:
        for version in ["full", "reduced"]:
            key = f"{method}_{version}"
            try:
                # Charger les chemins des pipelines
                paths_dict = load_artifact(f"best_{key}_pipelines.json", models_dir)
                pipelines[key] = {}
                
                # Charger chaque pipeline
                for model_name, path_str in paths_dict.items():
                    try:
                        filename = Path(path_str).name
                        pipeline = load_artifact(filename, models_dir)
                        pipelines[key][model_name] = pipeline
                        total_models += 1
                    except Exception as e:
                        log.warning(f"‚ö†Ô∏è Pipeline {model_name} ({key}) non charg√©: {e}")
                        
            except Exception as e:
                log.warning(f"‚ö†Ô∏è Erreur configuration {key}: {e}")
    
    log.info(f"‚úÖ {total_models} pipelines charg√©s")
    
    # ‚úÖ 3. Chargement des m√©triques (bon r√©pertoire)
    log.info("üìà Chargement des m√©triques...")
    try:
        metrics_df = pd.read_csv(cfg.paths.artifacts / "models" / "df_all_thresholds.csv")
        log.info(f"‚úÖ M√©triques charg√©es : {len(metrics_df)} mod√®les")
    except Exception as e:
        log.error(f"‚ùå Erreur m√©triques: {e}")
        metrics_df = pd.DataFrame()
    
    # ‚úÖ 4. Extraction des features
    log.info("üîç Extraction des features...")
    features = {}
    for method in ["knn", "mice"]:
        if method in splits and "X_train" in splits[method]:
            features[method] = splits[method]["X_train"].columns.tolist()
        else:
            features[method] = []
    
    log.info(f"‚úÖ Features extraites : KNN({len(features.get('knn', []))}), MICE({len(features.get('mice', []))})")
    
    # ‚úÖ 5. R√©sum√©
    log.info("üöÄ Chargement termin√© pour le stacking")
    log.info(f"   ‚Ä¢ Splits : {len(splits)} m√©thodes")
    log.info(f"   ‚Ä¢ Pipelines : {total_models} mod√®les")
    log.info(f"   ‚Ä¢ M√©triques : {len(metrics_df)} entr√©es")
    
    return splits, pipelines, metrics_df, features


def select_champion_models(metrics_df: pd.DataFrame, 
                          pipelines: Dict,
                          version: str = "FULL",
                          top_n: int = 4) -> Tuple[Dict, pd.DataFrame]:
    """
    S√©lectionne les mod√®les champions pour le stacking.
    
    Args:
        metrics_df: DataFrame des m√©triques
        pipelines: Dict des pipelines
        version: Version √† s√©lectionner ("FULL" ou "REDUCED")
        top_n: Nombre de champions √† s√©lectionner
        
    Returns:
        Tuple[champion_pipelines, champion_metrics]
    """
    log.info(f"üèÜ S√©lection des {top_n} champions {version}")
    
    # Filtrage des champions
    champions_df = metrics_df[
        (metrics_df['Version'] == version) & 
        (metrics_df['f1'] >= 0.917)
    ].head(top_n)
    
    log.info(f"‚úÖ {len(champions_df)} champions s√©lectionn√©s")
    
    # Extraction des pipelines correspondants
    champion_pipelines = {}
    
    for _, row in champions_df.iterrows():
        model = row['model']
        imputation = row['Imputation'].lower()
        
        # Cl√© du pipeline
        pipeline_key = f"{imputation}_{version.lower()}"
        
        if pipeline_key in pipelines and model in pipelines[pipeline_key]:
            champion_key = f"{model}_{imputation}_{version}"
            champion_pipelines[champion_key] = pipelines[pipeline_key][model]
            log.info(f"   ‚Ä¢ {champion_key}: F1={row['f1']:.4f}")
        else:
            log.warning(f"‚ö†Ô∏è Pipeline non trouv√© pour {model} {imputation} {version}")
    
    log.info(f"üéØ {len(champion_pipelines)} pipelines champions r√©cup√©r√©s")
    
    return champion_pipelines, champions_df


# =============================================================================
# 1. CR√âATION D'ENSEMBLES
# =============================================================================

def create_voting_ensemble(base_models: Dict[str, Any],
                          voting: str = 'soft',
                          weights: Optional[List[float]] = None) -> VotingClassifier:
    """
    Cr√©e un ensemble de type Voting.
    
    Args:
        base_models: Dict {nom: mod√®le} des mod√®les de base
        voting: Type de vote ('hard' ou 'soft')
        weights: Poids optionnels pour chaque mod√®le
        
    Returns:
        VotingClassifier configur√©
    """
    
    log.info(f"üó≥Ô∏è Cr√©ation d'un ensemble Voting ({voting})")
    log.info(f"üìä Mod√®les de base: {list(base_models.keys())}")
    
    # Conversion en liste de tuples pour VotingClassifier
    estimators = [(name, model) for name, model in base_models.items()]
    
    voting_ensemble = VotingClassifier(
        estimators=estimators,
        voting=voting,
        weights=weights
    )
    
    return voting_ensemble


def create_bagging_ensemble(base_estimator,
                           n_estimators: int = 10,
                           max_samples: float = 1.0,
                           max_features: float = 1.0,
                           random_state: int = None) -> BaggingClassifier:
    """
    Cr√©e un ensemble de type Bagging.
    
    Args:
        base_estimator: Estimateur de base
        n_estimators: Nombre d'estimateurs dans l'ensemble
        max_samples: Fraction d'√©chantillons pour chaque estimateur
        max_features: Fraction de features pour chaque estimateur
        random_state: Graine al√©atoire
        
    Returns:
        BaggingClassifier configur√©
    """
    
    if random_state is None:
        random_state = cfg.project.random_state
    
    log.info(f"üéí Cr√©ation d'un ensemble Bagging")
    log.info(f"üìä Estimateur de base: {type(base_estimator).__name__}")
    log.info(f"üî¢ Nombre d'estimateurs: {n_estimators}")
    
    bagging_ensemble = BaggingClassifier(
        base_estimator=base_estimator,
        n_estimators=n_estimators,
        max_samples=max_samples,
        max_features=max_features,
        random_state=random_state,
        n_jobs=-1
    )
    
    return bagging_ensemble


def create_stacking_ensemble(base_models: Dict[str, Any],
                            meta_classifier=None,
                            cv: int = 5,
                            use_probas: bool = True,
                            use_features_in_secondary: bool = False) -> Any:
    """
    Cr√©e un ensemble de type Stacking.
    
    Args:
        base_models: Dict {nom: mod√®le} des mod√®les de base
        meta_classifier: M√©ta-classificateur (LogisticRegression par d√©faut)
        cv: Nombre de folds pour la validation crois√©e
        use_probas: Utiliser les probabilit√©s comme features
        use_features_in_secondary: Inclure les features originales
        
    Returns:
        StackingClassifier configur√©
    """
    
    from sklearn.ensemble import StackingClassifier
    
    if meta_classifier is None:
        meta_classifier = LogisticRegression(
            random_state=cfg.project.random_state,
            max_iter=1000
        )
    
    log.info(f"ü•û Cr√©ation d'un ensemble Stacking")
    log.info(f"üìä Mod√®les de base: {list(base_models.keys())}")
    log.info(f"üß† M√©ta-classificateur: {type(meta_classifier).__name__}")
    
    # Conversion en liste de tuples
    estimators = [(name, model) for name, model in base_models.items()]
    
    # Configuration de la CV
    cv_strategy = StratifiedKFold(
        n_splits=cv, 
        shuffle=True, 
        random_state=cfg.project.random_state
    )
    
    stacking_ensemble = StackingClassifier(
        estimators=estimators,
        final_estimator=meta_classifier,
        cv=cv_strategy,
        stack_method='predict_proba' if use_probas else 'predict',
        passthrough=use_features_in_secondary,
        n_jobs=-1
    )
    
    return stacking_ensemble


# =============================================================================
# 2. OPTIMISATION DES ENSEMBLES
# =============================================================================

def optimize_ensemble(ensemble,
                     X_train: pd.DataFrame,
                     y_train: pd.Series,
                     X_val: pd.DataFrame,
                     y_val: pd.Series,
                     param_grid: Optional[Dict] = None,
                     cv: int = 5,
                     scoring: str = 'f1',
                     verbose: bool = True) -> Dict:
    """
    Optimise les hyperparam√®tres d'un ensemble.
    
    Args:
        ensemble: Ensemble √† optimiser
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        param_grid: Grille de param√®tres √† tester
        cv: Nombre de folds pour la validation crois√©e
        scoring: M√©trique d'optimisation
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire avec les r√©sultats d'optimisation
    """
    
    ensemble_name = type(ensemble).__name__
    
    if verbose:
        log.info(f"üîß OPTIMISATION ENSEMBLE: {ensemble_name}")
        log.info(f"üìä Donn√©es: {X_train.shape[0]} train, {X_val.shape[0]} validation")
    
    # Grille par d√©faut si non fournie
    if param_grid is None:
        param_grid = _get_default_ensemble_params(ensemble)
    
    if verbose and param_grid:
        log.info(f"üéØ Param√®tres √† optimiser: {list(param_grid.keys())}")
    
    # Configuration de la validation crois√©e
    cv_strategy = StratifiedKFold(
        n_splits=cv,
        shuffle=True, 
        random_state=cfg.project.random_state
    )
    
    # Grid Search
    start_time = time.time()
    if param_grid:
        grid_search = GridSearchCV(
            estimator=ensemble,
            param_grid=param_grid,
            cv=cv_strategy,
            scoring=scoring,
            n_jobs=-1,
            verbose=1 if verbose else 0
        )
        grid_search.fit(X_train, y_train)
        best_ensemble = grid_search.best_estimator_
        best_params = grid_search.best_params_
        best_cv_score = grid_search.best_score_
    else:
        # Pas d'optimisation, juste entra√Ænement
        ensemble.fit(X_train, y_train)
        best_ensemble = ensemble
        best_params = {}
        best_cv_score = None
    
    training_time = time.time() - start_time
    
    # √âvaluation sur validation
    y_val_pred = best_ensemble.predict(X_val)
    y_val_pred_proba = best_ensemble.predict_proba(X_val)[:, 1]
    
    # M√©triques
    val_metrics = {
        'f1': f1_score(y_val, y_val_pred),
        'precision': precision_score(y_val, y_val_pred),
        'recall': recall_score(y_val, y_val_pred),
        'auc_roc': roc_auc_score(y_val, y_val_pred_proba)
    }
    
    if verbose:
        log.info(f"‚úÖ Optimisation termin√©e en {training_time:.2f}s")
        if best_cv_score:
            log.info(f"üèÜ Meilleur score CV: {best_cv_score:.4f}")
        log.info("üìà M√©triques de validation:")
        for metric, value in val_metrics.items():
            log.info(f"   ‚Ä¢ {metric}: {value:.4f}")
    
    results = {
        'ensemble_name': ensemble_name,
        'best_ensemble': best_ensemble,
        'best_params': best_params,
        'best_cv_score': best_cv_score,
        'training_time': training_time,
        'validation_metrics': val_metrics,
        'predictions': {
            'y_val_pred': y_val_pred,
            'y_val_pred_proba': y_val_pred_proba
        }
    }
    
    return results


def _get_default_ensemble_params(ensemble) -> Dict:
    """Retourne les param√®tres par d√©faut selon le type d'ensemble."""
    
    ensemble_name = type(ensemble).__name__
    
    if ensemble_name == 'VotingClassifier':
        return {
            'weights': [None, [1, 1, 1], [2, 1, 1], [1, 2, 1], [1, 1, 2]]
        }
    
    elif ensemble_name == 'BaggingClassifier':
        return {
            'n_estimators': [10, 20, 50],
            'max_samples': [0.8, 1.0],
            'max_features': [0.8, 1.0]
        }
    
    elif ensemble_name == 'StackingClassifier':
        return {
            'final_estimator__C': [0.1, 1, 10],
            'final_estimator__max_iter': [1000]
        }
    
    return {}


# =============================================================================
# 3. ENTRA√éNEMENT COMPLET D'ENSEMBLES
# =============================================================================

def train_all_ensembles(base_models: Dict[str, Any],
                       X_train: pd.DataFrame,
                       y_train: pd.Series,
                       X_val: pd.DataFrame,
                       y_val: pd.Series,
                       ensemble_types: Optional[List[str]] = None,
                       save_ensembles: bool = True,
                       verbose: bool = True) -> Dict:
    """
    Entra√Æne tous les types d'ensembles sp√©cifi√©s.
    
    Args:
        base_models: Dict des mod√®les de base √† utiliser
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        ensemble_types: Types d'ensembles √† cr√©er
        save_ensembles: Sauvegarder les ensembles
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire avec tous les r√©sultats d'ensembles
    """
    
    if ensemble_types is None:
        ensemble_types = ['voting_soft', 'voting_hard', 'bagging', 'stacking']
    
    if verbose:
        log.info("üöÄ ENTRA√éNEMENT DE TOUS LES ENSEMBLES")
        log.info("=" * 50)
        log.info(f"üìä Types d'ensembles: {ensemble_types}")
        log.info(f"üß† Mod√®les de base: {list(base_models.keys())}")
    
    results = {
        'ensemble_results': {},
        'comparison_data': [],
        'best_ensembles': {}
    }
    
    for ensemble_type in ensemble_types:
        if verbose:
            log.info(f"\n{'='*20} {ensemble_type.upper()} {'='*20}")
        
        try:
            # Cr√©ation de l'ensemble
            if ensemble_type == 'voting_soft':
                ensemble = create_voting_ensemble(base_models, voting='soft')
            elif ensemble_type == 'voting_hard':
                ensemble = create_voting_ensemble(base_models, voting='hard')
            elif ensemble_type == 'bagging':
                # Utilise le premier mod√®le comme base pour le bagging
                base_model = list(base_models.values())[0]
                ensemble = create_bagging_ensemble(base_model)
            elif ensemble_type == 'stacking':
                ensemble = create_stacking_ensemble(base_models)
            else:
                log.warning(f"Type d'ensemble '{ensemble_type}' non support√©")
                continue
            
            # Optimisation
            ensemble_results = optimize_ensemble(
                ensemble, X_train, y_train, X_val, y_val,
                verbose=verbose
            )
            
            # Stockage des r√©sultats
            results['ensemble_results'][ensemble_type] = ensemble_results
            results['best_ensembles'][ensemble_type] = ensemble_results['best_ensemble']
            
            # Donn√©es pour comparaison
            results['comparison_data'].append({
                'Ensemble': ensemble_type,
                'F1_Score': ensemble_results['validation_metrics']['f1'],
                'Precision': ensemble_results['validation_metrics']['precision'],
                'Recall': ensemble_results['validation_metrics']['recall'],
                'AUC_ROC': ensemble_results['validation_metrics']['auc_roc'],
                'CV_Score': ensemble_results.get('best_cv_score', 'N/A'),
                'Training_Time': ensemble_results['training_time']
            })
            
            # Sauvegarde
            if save_ensembles:
                filename = f"ensemble_{ensemble_type}.pkl"
                save_artifact(
                    ensemble_results['best_ensemble'], 
                    filename, 
                    cfg.paths.models
                )
                log.info(f"üíæ Ensemble sauvegard√©: {filename}")
            
        except Exception as e:
            log.error(f"‚ùå Erreur avec {ensemble_type}: {str(e)}")
            continue
    
    # Tableau de comparaison
    if results['comparison_data']:
        comparison_df = pd.DataFrame(results['comparison_data'])
        comparison_df = comparison_df.sort_values('F1_Score', ascending=False)
        results['comparison_dataframe'] = comparison_df
        
        if verbose:
            log.info("\nüèÜ TABLEAU DE COMPARAISON DES ENSEMBLES")
            log.info("=" * 50)
            print(comparison_df.round(4).to_string(index=False))
        
        # Meilleur ensemble
        best_ensemble_type = comparison_df.iloc[0]['Ensemble']
        results['champion_ensemble'] = {
            'type': best_ensemble_type,
            'model': results['best_ensembles'][best_ensemble_type],
            'metrics': results['ensemble_results'][best_ensemble_type]['validation_metrics']
        }
        
        if verbose:
            log.info(f"\nü•á MEILLEUR ENSEMBLE: {best_ensemble_type.upper()}")
            log.info(f"   F1-Score: {comparison_df.iloc[0]['F1_Score']:.4f}")
    
    if verbose:
        log.info(f"\nüéâ ENTRA√éNEMENT TERMIN√â - {len(results['best_ensembles'])} ensembles")
    
    return results


# =============================================================================
# 4. √âVALUATION ET COMPARAISON
# =============================================================================

def compare_models_vs_ensembles(individual_results: Dict,
                               ensemble_results: Dict,
                               save_comparison: bool = True) -> pd.DataFrame:
    """
    Compare les performances des mod√®les individuels vs ensembles.
    
    Args:
        individual_results: R√©sultats des mod√®les individuels
        ensemble_results: R√©sultats des ensembles
        save_comparison: Sauvegarder le tableau de comparaison
        
    Returns:
        DataFrame de comparaison
    """
    
    log.info("üìä COMPARAISON MOD√àLES INDIVIDUELS vs ENSEMBLES")
    log.info("=" * 60)
    
    comparison_data = []
    
    # Mod√®les individuels
    if 'comparison_data' in individual_results:
        for row in individual_results['comparison_data']:
            row_copy = row.copy()
            row_copy['Type'] = 'Individual'
            comparison_data.append(row_copy)
    
    # Ensembles
    if 'comparison_data' in ensemble_results:
        for row in ensemble_results['comparison_data']:
            row_copy = row.copy()
            row_copy['Type'] = 'Ensemble'
            # Renommer la colonne si n√©cessaire
            if 'Ensemble' in row_copy:
                row_copy['Model'] = row_copy['Ensemble']
                del row_copy['Ensemble']
            comparison_data.append(row_copy)
    
    # Cr√©ation du DataFrame
    comparison_df = pd.DataFrame(comparison_data)
    
    if not comparison_df.empty:
        # Tri par F1-Score
        comparison_df = comparison_df.sort_values('F1_Score', ascending=False)
        comparison_df = comparison_df.reset_index(drop=True)
        
        # Affichage
        log.info("üèÜ CLASSEMENT GLOBAL:")
        print(comparison_df[['Type', 'Model', 'F1_Score', 'Precision', 'Recall', 'AUC_ROC']].round(4).to_string(index=False))
        
        # Sauvegarde
        if save_comparison:
            comparison_path = cfg.paths.outputs / "models_vs_ensembles_comparison.csv"
            comparison_df.to_csv(comparison_path, index=False)
            log.info(f"üíæ Comparaison sauvegard√©e: {comparison_path}")
    
    return comparison_df


# =============================================================================
# 5. UTILITAIRES
# =============================================================================

def load_ensemble(ensemble_type: str) -> Any:
    """
    Charge un ensemble sauvegard√©.
    
    Args:
        ensemble_type: Type d'ensemble √† charger
        
    Returns:
        L'ensemble charg√©
    """
    
    filename = f"ensemble_{ensemble_type}.pkl"
    try:
        ensemble = load_artifact(filename, cfg.paths.models)
        log.info(f"‚úÖ Ensemble charg√©: {filename}")
        return ensemble
    except FileNotFoundError:
        log.error(f"‚ùå Ensemble non trouv√©: {filename}")
        raise


def get_ensemble_feature_importance(ensemble, 
                                  feature_names: List[str],
                                  top_n: int = 20) -> pd.DataFrame:
    """
    Extrait l'importance des features d'un ensemble (si disponible).
    
    Args:
        ensemble: Mod√®le d'ensemble
        feature_names: Noms des features
        top_n: Nombre de top features √† retourner
        
    Returns:
        DataFrame avec l'importance des features
    """
    
    ensemble_name = type(ensemble).__name__
    
    if hasattr(ensemble, 'feature_importances_'):
        # Ensembles bas√©s sur des arbres
        importances = ensemble.feature_importances_
    elif ensemble_name == 'VotingClassifier':
        # Moyenne des importances des estimateurs (si disponible)
        importances = []
        for name, estimator in ensemble.named_estimators_.items():
            if hasattr(estimator, 'feature_importances_'):
                importances.append(estimator.feature_importances_)
        
        if importances:
            importances = np.mean(importances, axis=0)
        else:
            log.warning("Aucune importance de features disponible")
            return pd.DataFrame()
    else:
        log.warning(f"Importance des features non disponible pour {ensemble_name}")
        return pd.DataFrame()
    
    # Cr√©ation du DataFrame
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values('Importance', ascending=False)
    
    return importance_df.head(top_n)