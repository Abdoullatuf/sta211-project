"""
Module de pr√©traitement pour le Notebook 1 - EDA et pr√©paration des donn√©es
Extrait et adapt√© de data_processing.py

Fonctionnalit√©s:
- Chargement et nettoyage des donn√©es
- Analyse exploratoire (EDA) 
- Imputations KNN et MICE
- Transformations optimales (Yeo-Johnson, Box-Cox)
- D√©tection et traitement des outliers
- G√©n√©ration des features polynomiales

Auteur: Maoulida Abdoullatuf  
Version: 1.0 (restructur√©)
"""

import os
import re
import time
import chardet
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
from IPython.display import display, Markdown

# Imports du projet
from modules.config import cfg
from modules.utils.storage import save_artifact, load_artifact

# Scikit-learn imports
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, KNNImputer
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import BayesianRidge
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder, PowerTransformer
from scipy import stats

warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)

# Configuration du logging
log = cfg.get_logger(__name__)

# =============================================================================
# 1. CHARGEMENT ET NETTOYAGE DES DONN√âES
# =============================================================================

def load_and_clean_data(
    file_path: Union[str, Path], 
    target_col: str = "outcome",
    display_info: bool = True,
    encoding: Optional[str] = None,
    max_file_size_mb: float = 500
) -> Tuple[pd.DataFrame, Dict]:
    """
    Charge un fichier CSV proprement et v√©rifie la pr√©sence de la colonne cible.
    
    Args:
        file_path: Chemin vers le fichier CSV
        target_col: Nom de la colonne cible  
        display_info: Afficher les informations de chargement
        encoding: Encodage du fichier (auto-d√©tect√© si None)
        max_file_size_mb: Taille maximale autoris√©e en MB
        
    Returns:
        Tuple contenant le DataFrame et un dictionnaire d'informations
    """
    
    start_time = time.time()
    file_path = Path(file_path)
    
    # V√©rifications pr√©liminaires
    if not file_path.exists():
        raise FileNotFoundError(f"Fichier non trouv√© : {file_path}")
        
    file_size_mb = file_path.stat().st_size / (1024 * 1024)
    if file_size_mb > max_file_size_mb:
        raise ValueError(f"Fichier trop volumineux : {file_size_mb:.1f}MB > {max_file_size_mb}MB")
    
    # Auto-d√©tection de l'encodage si n√©cessaire
    if encoding is None:
        with open(file_path, 'rb') as f:
            raw_data = f.read(10000)  # Lire les premiers 10KB
            encoding_info = chardet.detect(raw_data)
            encoding = encoding_info['encoding']
            if display_info:
                print(f"üîç Encodage d√©tect√© : {encoding} (confiance: {encoding_info['confidence']:.2%})")
    
    # Chargement des donn√©es
    try:
        df = pd.read_csv(file_path, encoding=encoding)
        load_time = time.time() - start_time
        
        if display_info:
            print(f"‚úÖ Donn√©es charg√©es avec succ√®s !")
            print(f"üìä Shape: {df.shape}")
            print(f"‚è±Ô∏è  Temps de chargement: {load_time:.2f}s")
            print(f"üíæ M√©moire utilis√©e: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
        
        # V√©rification de la colonne cible
        target_present = target_col in df.columns
        if not target_present and display_info:
            print(f"‚ö†Ô∏è  Colonne cible '{target_col}' non trouv√©e (mode test/pr√©diction)")
            
        # Informations de base
        info_dict = {
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': df.dtypes.to_dict(),
            'target_present': target_present,
            'missing_values': df.isnull().sum().sum(),
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
            'load_time_seconds': load_time,
            'file_size_mb': file_size_mb
        }
        
        return df, info_dict
        
    except Exception as e:
        raise Exception(f"Erreur lors du chargement : {str(e)}")

# =============================================================================
# 2. ANALYSE EXPLORATOIRE (EDA)
# =============================================================================

def analyze_missing_patterns(df: pd.DataFrame, target_col: Optional[str] = None, 
                           save_path: Optional[Path] = None, figsize: Tuple[int, int] = (15, 10)) -> Dict:
    """
    Analyse compl√®te des patterns de valeurs manquantes.
    
    Args:
        df: DataFrame √† analyser
        target_col: Colonne cible (optionnelle)
        save_path: Chemin pour sauvegarder les graphiques
        figsize: Taille des graphiques
        
    Returns:
        Dictionnaire avec les statistiques des valeurs manquantes
    """
    
    print("üîç ANALYSE DES VALEURS MANQUANTES")
    print("=" * 50)
    
    # Statistiques g√©n√©rales
    missing_stats = {}
    total_cells = df.shape[0] * df.shape[1]
    total_missing = df.isnull().sum().sum()
    
    missing_stats['total_cells'] = total_cells
    missing_stats['total_missing'] = total_missing
    missing_stats['missing_percentage'] = (total_missing / total_cells) * 100
    
    print(f"üìä Cellules totales : {total_cells:,}")
    print(f"üï≥Ô∏è  Valeurs manquantes : {total_missing:,} ({missing_stats['missing_percentage']:.2f}%)")
    
    # Par colonne
    missing_by_col = df.isnull().sum()
    missing_by_col = missing_by_col[missing_by_col > 0].sort_values(ascending=False)
    
    if len(missing_by_col) > 0:
        print(f"\nüìà Colonnes avec valeurs manquantes ({len(missing_by_col)}/{len(df.columns)}):")
        
        missing_df = pd.DataFrame({
            'Missing_Count': missing_by_col,
            'Missing_Percentage': (missing_by_col / len(df)) * 100
        })
        
        # Top 10 des colonnes avec le plus de valeurs manquantes
        top_missing = missing_df.head(10)
        for col, row in top_missing.iterrows():
            print(f"  ‚Ä¢ {col}: {row['Missing_Count']:,} ({row['Missing_Percentage']:.1f}%)")
            
        missing_stats['by_column'] = missing_df.to_dict()
        
        # Visualisations
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # 1. Heatmap des valeurs manquantes
        if len(df.columns) <= 50:  # √âviter les heatmaps trop denses
            sns.heatmap(df.isnull(), cbar=True, ax=axes[0,0], cmap='viridis')
            axes[0,0].set_title('Pattern des valeurs manquantes')
        else:
            # Prendre un √©chantillon de colonnes
            sample_cols = missing_by_col.head(20).index.tolist()
            sns.heatmap(df[sample_cols].isnull(), cbar=True, ax=axes[0,0], cmap='viridis')
            axes[0,0].set_title('Pattern des valeurs manquantes (top 20 colonnes)')
        
        # 2. Distribution du pourcentage de valeurs manquantes par colonne
        axes[0,1].hist(missing_df['Missing_Percentage'], bins=20, edgecolor='black', alpha=0.7)
        axes[0,1].set_xlabel('Pourcentage de valeurs manquantes')
        axes[0,1].set_ylabel('Nombre de colonnes')
        axes[0,1].set_title('Distribution des taux de valeurs manquantes')
        
        # 3. Top 15 colonnes avec valeurs manquantes
        top_15 = missing_df.head(15)
        axes[1,0].barh(range(len(top_15)), top_15['Missing_Percentage'])
        axes[1,0].set_yticks(range(len(top_15)))
        axes[1,0].set_yticklabels([col[:20] + '...' if len(col) > 20 else col for col in top_15.index])
        axes[1,0].set_xlabel('Pourcentage manquant')
        axes[1,0].set_title('Top 15 colonnes avec valeurs manquantes')
        
        # 4. Valeurs manquantes par ligne
        missing_by_row = df.isnull().sum(axis=1)
        axes[1,1].hist(missing_by_row, bins=30, edgecolor='black', alpha=0.7)
        axes[1,1].set_xlabel('Nombre de valeurs manquantes par ligne')
        axes[1,1].set_ylabel('Nombre de lignes')
        axes[1,1].set_title('Distribution des valeurs manquantes par ligne')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path / "missing_values_analysis.png", dpi=300, bbox_inches='tight')
            print(f"üíæ Graphique sauvegard√© : {save_path / 'missing_values_analysis.png'}")
        
        plt.show()
        
    else:
        print("‚úÖ Aucune valeur manquante d√©tect√©e !")
        missing_stats['by_column'] = {}
    
    return missing_stats


# =============================================================================
# 3. IMPUTATIONS
# =============================================================================

def perform_knn_imputation(df: pd.DataFrame, 
                          cols_to_impute: List[str],
                          n_neighbors: int = 7,
                          save_imputer: bool = True,
                          random_state: int = 42) -> Tuple[pd.DataFrame, object]:
    """
    Effectue une imputation KNN sur les colonnes sp√©cifi√©es.
    
    Args:
        df: DataFrame avec valeurs manquantes
        cols_to_impute: Liste des colonnes √† imputer  
        n_neighbors: Nombre de voisins pour KNN
        save_path: Chemin pour sauvegarder l'imputer
        random_state: Graine al√©atoire
        
    Returns:
        Tuple (DataFrame imput√©, imputer entra√Æn√©)
    """
    
    print(f"üîß IMPUTATION KNN (k={n_neighbors})")
    print("=" * 40)
    
    df_imputed = df.copy()
    
    # V√©rification des colonnes
    missing_cols = []
    for col in cols_to_impute:
        if col not in df.columns:
            print(f"‚ö†Ô∏è  Colonne '{col}' non trouv√©e, ignor√©e")
            continue
        missing_count = df[col].isnull().sum()
        if missing_count > 0:
            missing_cols.append(col)
            print(f"  ‚Ä¢ {col}: {missing_count:,} valeurs manquantes ({missing_count/len(df)*100:.1f}%)")
    
    if not missing_cols:
        print("‚úÖ Aucune valeur manquante dans les colonnes sp√©cifi√©es")
        return df_imputed, None
    
    # Configuration de l'imputer KNN
    knn_imputer = KNNImputer(
        n_neighbors=n_neighbors,
        weights='uniform'  # Poids uniformes pour plus de stabilit√©
    )
    
    # Imputation
    start_time = time.time()
    imputed_values = knn_imputer.fit_transform(df[missing_cols])
    impute_time = time.time() - start_time
    
    # Mise √† jour du DataFrame
    df_imputed[missing_cols] = imputed_values
    
    print(f"‚úÖ Imputation termin√©e en {impute_time:.2f}s")
    print(f"üéØ Colonnes imput√©es: {len(missing_cols)}")
    
    # V√©rification post-imputation
    remaining_missing = df_imputed[missing_cols].isnull().sum().sum()
    if remaining_missing > 0:
        print(f"‚ö†Ô∏è  {remaining_missing} valeurs toujours manquantes apr√®s imputation")
    else:
        print("‚úÖ Toutes les valeurs manquantes ont √©t√© imput√©es")
    
    # Sauvegarde de l'imputer
    if save_imputer and knn_imputer is not None:
        filename = f"imputer_knn_k{n_neighbors}.pkl"
        save_artifact(knn_imputer, filename, cfg.paths.imputers)
        log.info(f"üíæ Imputer KNN sauvegard√© : {filename}")
    
    return df_imputed, knn_imputer


def perform_mice_imputation(df: pd.DataFrame,
                           cols_to_impute: List[str],
                           max_iter: int = 10,
                           estimator=None,
                           save_imputer: bool = True,
                           random_state: int = 42) -> Tuple[pd.DataFrame, object]:
    """
    Effectue une imputation MICE (Multiple Imputation by Chained Equations).
    
    Args:
        df: DataFrame avec valeurs manquantes
        cols_to_impute: Liste des colonnes √† imputer
        max_iter: Nombre maximum d'it√©rations
        estimator: Estimateur √† utiliser (BayesianRidge par d√©faut)
        save_path: Chemin pour sauvegarder l'imputer
        random_state: Graine al√©atoire
        
    Returns:
        Tuple (DataFrame imput√©, imputer entra√Æn√©)
    """
    
    print(f"üîß IMPUTATION MICE (max_iter={max_iter})")
    print("=" * 40)
    
    df_imputed = df.copy()
    
    # Estimateur par d√©faut
    if estimator is None:
        estimator = BayesianRidge()
        print("üß† Estimateur: BayesianRidge (d√©faut)")
    else:
        print(f"üß† Estimateur: {type(estimator).__name__}")
    
    # V√©rification des colonnes
    missing_cols = []
    for col in cols_to_impute:
        if col not in df.columns:
            print(f"‚ö†Ô∏è  Colonne '{col}' non trouv√©e, ignor√©e")
            continue
        missing_count = df[col].isnull().sum()
        if missing_count > 0:
            missing_cols.append(col)
            print(f"  ‚Ä¢ {col}: {missing_count:,} valeurs manquantes ({missing_count/len(df)*100:.1f}%)")
    
    if not missing_cols:
        print("‚úÖ Aucune valeur manquante dans les colonnes sp√©cifi√©es")
        return df_imputed, None
    
    # Configuration de l'imputer MICE
    mice_imputer = IterativeImputer(
        estimator=estimator,
        max_iter=max_iter,
        random_state=random_state,
        verbose=1  # Pour suivre la progression
    )
    
    # Imputation
    start_time = time.time()
    print("üîÑ D√©marrage de l'imputation MICE...")
    imputed_values = mice_imputer.fit_transform(df[missing_cols])
    impute_time = time.time() - start_time
    
    # Mise √† jour du DataFrame
    df_imputed[missing_cols] = imputed_values
    
    print(f"‚úÖ Imputation termin√©e en {impute_time:.2f}s")
    print(f"üéØ Colonnes imput√©es: {len(missing_cols)}")
    print(f"üîÑ It√©rations effectu√©es: {mice_imputer.n_iter_}")
    
    # V√©rification post-imputation
    remaining_missing = df_imputed[missing_cols].isnull().sum().sum()
    if remaining_missing > 0:
        print(f"‚ö†Ô∏è  {remaining_missing} valeurs toujours manquantes apr√®s imputation")
    else:
        print("‚úÖ Toutes les valeurs manquantes ont √©t√© imput√©es")
    
    # Sauvegarde de l'imputer
    if save_imputer and mice_imputer is not None:
        filename = f"imputer_mice_maxiter{max_iter}.pkl"
        save_artifact(mice_imputer, filename, cfg.paths.imputers)
        log.info(f"üíæ Imputer MICE sauvegard√© : {filename}")
    
    return df_imputed, mice_imputer

# =============================================================================
# 4. TRANSFORMATIONS OPTIMALES
# =============================================================================

def apply_optimal_transformations(df: pd.DataFrame,
                                 continuous_cols: List[str],
                                 method_mapping: Dict[str, str],
                                 save_transformers: bool = True) -> Tuple[pd.DataFrame, Dict]:
    """
    Applique les transformations optimales (Yeo-Johnson, Box-Cox) aux colonnes.
    
    Args:
        df: DataFrame √† transformer
        continuous_cols: Liste des colonnes continues
        method_mapping: Dict {colonne: m√©thode} o√π m√©thode in ['yeo-johnson', 'box-cox']
        save_path: Chemin pour sauvegarder les transformateurs
        
    Returns:
        Tuple (DataFrame transform√©, dict des transformateurs)
    """
    
    print("üîÑ APPLICATION DES TRANSFORMATIONS OPTIMALES")
    print("=" * 50)
    
    df_transformed = df.copy()
    transformers = {}
    
    for col in continuous_cols:
        if col not in df.columns:
            print(f"‚ö†Ô∏è  Colonne '{col}' non trouv√©e, ignor√©e")
            continue
            
        method = method_mapping.get(col, 'yeo-johnson')  # d√©faut
        print(f"üîß {col}: {method}")
        
        if method == 'yeo-johnson':
            transformer = PowerTransformer(method='yeo-johnson', standardize=False)
        elif method == 'box-cox':
            transformer = PowerTransformer(method='box-cox', standardize=False) 
            # S'assurer que les valeurs sont positives pour Box-Cox
            min_val = df[col].min()
            if min_val <= 0:
                shift = abs(min_val) + 1e-6
                df_transformed[col] = df_transformed[col] + shift
                print(f"  üìà D√©calage appliqu√©: +{shift:.6f}")
        else:
            print(f"  ‚ö†Ô∏è M√©thode '{method}' non support√©e, Yeo-Johnson utilis√©e")
            transformer = PowerTransformer(method='yeo-johnson', standardize=False)
        
        # Application de la transformation
        try:
            df_transformed[[col]] = transformer.fit_transform(df_transformed[[col]])
            transformers[col] = transformer
            print(f"  ‚úÖ Transformation appliqu√©e")
            
            # Renommer la colonne pour indiquer la transformation
            new_col_name = f"{col}_transformed"
            df_transformed = df_transformed.rename(columns={col: new_col_name})
            transformers[new_col_name] = transformers.pop(col)  # Update key
            
        except Exception as e:
            print(f"  ‚ùå Erreur: {str(e)}")
            continue
    
    # Sauvegarde des transformateurs
    if save_transformers and transformers:
        # Sauvegarder par type de transformation  
        yj_transformers = {k: v for k, v in transformers.items() 
                          if hasattr(v, 'method') and v.method == 'yeo-johnson'}
        bc_transformers = {k: v for k, v in transformers.items() 
                          if hasattr(v, 'method') and v.method == 'box-cox'}
        
        if yj_transformers:
            save_artifact(yj_transformers, "yeo_johnson_transformers.pkl", cfg.paths.transformers)
            log.info("üíæ Transformateurs Yeo-Johnson sauvegard√©s")
            
        if bc_transformers:
            save_artifact(bc_transformers, "box_cox_transformers.pkl", cfg.paths.transformers)
            log.info("üíæ Transformateurs Box-Cox sauvegard√©s")
    
    print(f"‚úÖ Transformations termin√©es: {len(transformers)} colonnes")
    return df_transformed, transformers

# =============================================================================
# 5. D√âTECTION ET TRAITEMENT DES OUTLIERS
# =============================================================================

def detect_and_cap_outliers(df: pd.DataFrame,
                           cols_to_process: List[str], 
                           method: str = 'iqr',
                           factor: float = 1.5,
                           save_params: bool = True) -> Tuple[pd.DataFrame, Dict]:
    """
    D√©tecte et traite les outliers par capping (√©cr√™tage).
    
    Args:
        df: DataFrame √† traiter
        cols_to_process: Colonnes √† traiter
        method: 'iqr' ou 'zscore'
        factor: Facteur multiplicateur (1.5 pour IQR, 3 pour z-score)
        save_params_path: Chemin pour sauvegarder les param√®tres de capping
        
    Returns:
        Tuple (DataFrame avec outliers trait√©s, param√®tres de capping)
    """
    
    print(f"üéØ D√âTECTION ET TRAITEMENT DES OUTLIERS ({method.upper()})")
    print("=" * 50)
    
    df_capped = df.copy()
    capping_params = {}
    
    for col in cols_to_process:
        if col not in df.columns:
            print(f"‚ö†Ô∏è  Colonne '{col}' non trouv√©e, ignor√©e")
            continue
            
        print(f"üîç Analyse de {col}:")
        
        if method == 'iqr':
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - factor * IQR
            upper_bound = Q3 + factor * IQR
            
        elif method == 'zscore':
            mean_val = df[col].mean()
            std_val = df[col].std()
            lower_bound = mean_val - factor * std_val
            upper_bound = mean_val + factor * std_val
            
        else:
            print(f"  ‚ùå M√©thode '{method}' non support√©e")
            continue
        
        # Compter les outliers avant traitement
        outliers_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
        n_outliers = outliers_mask.sum()
        outlier_percentage = (n_outliers / len(df)) * 100
        
        print(f"  üìä Outliers d√©tect√©s: {n_outliers} ({outlier_percentage:.1f}%)")
        print(f"  üìè Bornes: [{lower_bound:.3f}, {upper_bound:.3f}]")
        
        if n_outliers > 0:
            # Appliquer le capping
            df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)
            
            # Sauvegarder les param√®tres
            capping_params[col] = (lower_bound, upper_bound)
            
            print(f"  ‚úÖ Capping appliqu√©")
        else:
            print(f"  ‚ÑπÔ∏è  Aucun outlier d√©tect√©")
    
    # Sauvegarde des param√®tres
    if save_params and capping_params:
        filename = f"capping_params_{method}_factor{factor}.pkl"
        save_artifact(capping_params, filename, cfg.paths.transformers)
        log.info(f"üíæ Param√®tres de capping sauvegard√©s : {filename}")
    
    print(f"‚úÖ Traitement termin√©: {len(capping_params)} colonnes trait√©es")
    return df_capped, capping_params

# =============================================================================
# 6. G√âN√âRATION DE FEATURES POLYNOMIALES
# =============================================================================

def generate_polynomial_features(df: pd.DataFrame,
                                feature_cols: List[str],
                                degree: int = 2,
                                interaction_only: bool = False,
                                save_transformer: bool = True) -> Tuple[pd.DataFrame, object]:
    """
    G√©n√®re des features polynomiales √† partir des colonnes sp√©cifi√©es.
    
    Args:
        df: DataFrame source
        feature_cols: Colonnes pour g√©n√©rer les features
        degree: Degr√© polynomial
        interaction_only: Inclure seulement les interactions (pas les puissances)
        save_path: Chemin pour sauvegarder le transformateur
        
    Returns:
        Tuple (DataFrame avec nouvelles features, transformateur polynomial)
    """
    
    from sklearn.preprocessing import PolynomialFeatures
    
    print(f"üî¢ G√âN√âRATION DE FEATURES POLYNOMIALES (degr√©={degree})")
    print("=" * 50)
    
    # V√©rification des colonnes
    valid_cols = [col for col in feature_cols if col in df.columns]
    if not valid_cols:
        print("‚ùå Aucune colonne valide trouv√©e")
        return df, None
        
    missing_cols = [col for col in feature_cols if col not in df.columns]
    if missing_cols:
        print(f"‚ö†Ô∏è  Colonnes manquantes ignor√©es: {missing_cols}")
    
    print(f"üìä Colonnes utilis√©es: {valid_cols}")
    
    # Configuration du g√©n√©rateur polynomial
    poly_transformer = PolynomialFeatures(
        degree=degree,
        interaction_only=interaction_only,
        include_bias=False  # Pas de terme constant
    )
    
    # G√©n√©ration des features
    start_time = time.time()
    poly_features = poly_transformer.fit_transform(df[valid_cols])
    generation_time = time.time() - start_time
    
    # Noms des nouvelles features
    feature_names = poly_transformer.get_feature_names_out(valid_cols)
    
    # Cr√©ation du nouveau DataFrame
    df_with_poly = df.copy()
    
    # Ajouter les nouvelles features (exclure les features originales)
    n_original = len(valid_cols)
    new_features = poly_features[:, n_original:]  # Exclure les colonnes originales
    new_feature_names = feature_names[n_original:]
    
    for i, name in enumerate(new_feature_names):
        df_with_poly[name] = new_features[:, i]
    
    print(f"‚úÖ G√©n√©ration termin√©e en {generation_time:.2f}s")
    print(f"üéØ Features cr√©√©es: {len(new_feature_names)}")
    print(f"üìà Shape finale: {df_with_poly.shape}")
    
    # Quelques exemples de nouvelles features
    if new_feature_names:
        print(f"üìù Exemples de nouvelles features:")
        for name in new_feature_names[:5]:  # Top 5
            print(f"  ‚Ä¢ {name}")
        if len(new_feature_names) > 5:
            print(f"  ... et {len(new_feature_names) - 5} autres")
    
    # Sauvegarde du transformateur
    if save_transformer and poly_transformer is not None:
        filename = f"poly_transformer_degree{degree}_interact{interaction_only}.pkl"
        save_artifact(poly_transformer, filename, cfg.paths.transformers)
        log.info(f"üíæ Transformateur polynomial sauvegard√© : {filename}")
    
    return df_with_poly, poly_transformer

# =============================================================================
# 7. PIPELINE COMPLET DE PR√âTRAITEMENT
# =============================================================================

def preprocess_complete_pipeline(df: pd.DataFrame,
                                target_col: str,
                                continuous_cols: List[str],
                                imputation_method: str = 'knn',
                                transformation_mapping: Optional[Dict] = None,
                                save_artifacts: bool = True,
                                **kwargs) -> Dict:
    """
    Pipeline complet de pr√©traitement combinant toutes les √©tapes.
    
    Args:
        df: DataFrame source
        target_col: Colonne cible
        continuous_cols: Colonnes continues √† traiter
        imputation_method: 'knn' ou 'mice'
        transformation_mapping: Mapping des transformations par colonne
        save_dir: R√©pertoire pour sauvegarder tous les artefacts
        **kwargs: Param√®tres suppl√©mentaires pour les √©tapes
        
    Returns:
        Dictionnaire contenant tous les r√©sultats et artefacts
    """
    
    print("üöÄ PIPELINE COMPLET DE PR√âTRAITEMENT")
    print("=" * 60)
    
    results = {
        'original_shape': df.shape,
        'steps_completed': [],
        'artifacts': {},
        'processing_time': {}
    }
    
    # Configuration du logging et sauvegarde
    log.info("üöÄ D√©marrage du pipeline de pr√©traitement")
    log.info(f"üìä Donn√©es d'entr√©e: {df.shape}")
    
    # 1. Imputation
    step_start = time.time()
    if imputation_method == 'knn':
        df_imputed, imputer = perform_knn_imputation(
            df, continuous_cols, 
            save_imputer=save_artifacts,
            **kwargs.get('knn_params', {})
        )
    else:
        df_imputed, imputer = perform_mice_imputation(
            df, continuous_cols,
            save_imputer=save_artifacts, 
            **kwargs.get('mice_params', {})
        )
    
    results['steps_completed'].append('imputation')
    results['artifacts']['imputer'] = imputer
    results['processing_time']['imputation'] = time.time() - step_start
    
    # 2. Transformations optimales
    if transformation_mapping:
        step_start = time.time()
        df_transformed, transformers = apply_optimal_transformations(
            df_imputed, continuous_cols, transformation_mapping,
            save_transformers=save_artifacts
        )
        results['steps_completed'].append('transformations')
        results['artifacts']['transformers'] = transformers
        results['processing_time']['transformations'] = time.time() - step_start
    else:
        df_transformed = df_imputed
    
    # 3. Traitement des outliers
    step_start = time.time()
    outlier_cols = [f"{col}_transformed" for col in continuous_cols if f"{col}_transformed" in df_transformed.columns]
    if not outlier_cols:
        outlier_cols = continuous_cols
        
    df_capped, capping_params = detect_and_cap_outliers(
        df_transformed, outlier_cols,
        save_params=save_artifacts,
        **kwargs.get('outlier_params', {})
    )
    results['steps_completed'].append('outlier_treatment')
    results['artifacts']['capping_params'] = capping_params  
    results['processing_time']['outlier_treatment'] = time.time() - step_start
    
    # 4. Features polynomiales
    step_start = time.time()
    poly_cols = [col for col in outlier_cols if col in df_capped.columns]
    df_final, poly_transformer = generate_polynomial_features(
        df_capped, poly_cols,
        save_transformer=save_artifacts,
        **kwargs.get('poly_params', {})
    )
    results['steps_completed'].append('polynomial_features')
    results['artifacts']['poly_transformer'] = poly_transformer
    results['processing_time']['polynomial_features'] = time.time() - step_start
    
    # R√©sultats finaux
    results['final_shape'] = df_final.shape
    results['final_dataframe'] = df_final
    results['total_time'] = sum(results['processing_time'].values())
    
    print(f"üéâ PIPELINE TERMIN√â !")
    print(f"‚è±Ô∏è  Temps total: {results['total_time']:.2f}s")
    print(f"üìä Shape: {results['original_shape']} ‚Üí {results['final_shape']}")
    print(f"‚úÖ √âtapes: {' ‚Üí '.join(results['steps_completed'])}")
    
    return results