"""
Module de mod√©lisation pour le Notebook 2 - Entra√Ænement des mod√®les individuels
Extrait et adapt√© de modeling.py

Fonctionnalit√©s:
- Entra√Ænement des mod√®les individuels (RF, XGB, SVM, MLP, GradBoost)
- Optimisation des hyperparam√®tres avec validation crois√©e
- S√©lection de features avec RFE et importance
- √âvaluation et comparaison des mod√®les
- Optimisation des seuils de classification
- Sauvegarde des pipelines optimis√©s

Auteur: Maoulida Abdoullatuf
Version: 1.0 (restructur√©)
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

# Imports du projet
from modules.config import cfg
from modules.utils.storage import save_artifact, load_artifact

# Scikit-learn imports
from sklearn.metrics import (
    precision_recall_curve, f1_score, precision_score, recall_score, 
    roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc,
    classification_report
)
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.feature_selection import RFECV
from sklearn.inspection import permutation_importance
from xgboost import XGBClassifier

# Configuration du logging
log = cfg.get_logger(__name__)

# =============================================================================
# 1. CONFIGURATION DES MOD√àLES ET HYPERPARAM√àTRES
# =============================================================================

def get_default_param_grids() -> Dict[str, Dict]:
    """
    Retourne les grilles d'hyperparam√®tres par d√©faut pour chaque mod√®le.
    
    Returns:
        Dictionnaire avec les param√®tres par mod√®le
    """
    
    param_grids = {
        'randforest': {
            'clf__n_estimators': [100, 200],
            'clf__max_depth': [10, 20, None],
            'clf__min_samples_split': [2, 5],
            'clf__min_samples_leaf': [1, 2],
            'clf__max_features': ['sqrt', 'log2']
        },
        
        'xgboost': {
            'clf__n_estimators': [100, 200],
            'clf__max_depth': [6, 10],
            'clf__learning_rate': [0.01, 0.1],
            'clf__subsample': [0.8, 1.0],
            'clf__colsample_bytree': [0.8, 1.0]
        },
        
        'gradboost': {
            'clf__n_estimators': [100, 200],
            'clf__max_depth': [6, 10], 
            'clf__learning_rate': [0.01, 0.1],
            'clf__subsample': [0.8, 1.0],
            'clf__colsample_bytree': [0.8, 1.0]
        },
        
        'svm': {
            'clf__C': [0.1, 1, 10],
            'clf__kernel': ['rbf', 'linear'],
            'clf__gamma': ['scale', 'auto']
        },
        
        'mlp': {
            'clf__hidden_layer_sizes': [(100,), (100, 50), (200, 100)],
            'clf__alpha': [0.0001, 0.001],
            'clf__learning_rate': ['constant', 'adaptive'],
            'clf__max_iter': [500]
        }
    }
    
    return param_grids


def create_model_estimators() -> Dict[str, object]:
    """
    Cr√©e les instances des estimateurs par d√©faut.
    
    Returns:
        Dictionnaire contenant les estimateurs
    """
    
    estimators = {
        'randforest': RandomForestClassifier(
            random_state=42,
            n_jobs=-1
        ),
        
        'xgboost': XGBClassifier(
            random_state=42,
            eval_metric='logloss',
            verbosity=0
        ),
        
        'gradboost': XGBClassifier(
            random_state=42,
            eval_metric='logloss', 
            verbosity=0
        ),
        
        'svm': SVC(
            random_state=42,
            probability=True  # N√©cessaire pour predict_proba
        ),
        
        'mlp': MLPClassifier(
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )
    }
    
    return estimators


# =============================================================================
# 2. ENTRA√éNEMENT ET OPTIMISATION DES MOD√àLES
# =============================================================================

def train_and_optimize_model(X_train: pd.DataFrame,
                            y_train: pd.Series,
                            X_val: pd.DataFrame, 
                            y_val: pd.Series,
                            model_name: str,
                            param_grid: Optional[Dict] = None,
                            cv_folds: int = 5,
                            scoring: str = 'f1',
                            n_jobs: int = -1,
                            verbose: bool = True) -> Dict:
    """
    Entra√Æne et optimise un mod√®le avec validation crois√©e.
    
    Args:
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        model_name: Nom du mod√®le
        param_grid: Grille d'hyperparam√®tres
        cv_folds: Nombre de folds pour la validation crois√©e
        scoring: M√©trique d'optimisation
        n_jobs: Nombre de processus parall√®les
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire contenant les r√©sultats d'optimisation
    """
    
    if verbose:
        print(f"üöÄ OPTIMISATION DU MOD√àLE: {model_name.upper()}")
        print("=" * 50)
    
    # R√©cup√©rer l'estimateur et les param√®tres
    estimators = create_model_estimators()
    default_grids = get_default_param_grids()
    
    if model_name not in estimators:
        raise ValueError(f"Mod√®le '{model_name}' non support√©")
    
    estimator = estimators[model_name]
    if param_grid is None:
        param_grid = default_grids.get(model_name, {})
    
    if verbose:
        print(f"üìä Donn√©es: {X_train.shape[0]} train, {X_val.shape[0]} validation")
        print(f"üîß Hyperparam√®tres √† tester: {len(param_grid)} groupes")
        print(f"üéØ M√©trique d'optimisation: {scoring}")
    
    # Configuration de la validation crois√©e
    cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    # Cr√©ation du pipeline (si n√©cessaire)
    from sklearn.pipeline import Pipeline
    pipeline = Pipeline([('clf', estimator)])
    
    # Grid Search
    grid_search = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        cv=cv_strategy,
        scoring=scoring,
        n_jobs=n_jobs,
        verbose=1 if verbose else 0,
        return_train_score=True
    )
    
    # Entra√Ænement
    start_time = time.time()
    grid_search.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    # Meilleur mod√®le
    best_model = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_cv_score = grid_search.best_score_
    
    # Pr√©dictions sur validation
    y_val_pred = best_model.predict(X_val)
    y_val_pred_proba = best_model.predict_proba(X_val)[:, 1]
    
    # M√©triques de validation
    val_f1 = f1_score(y_val, y_val_pred)
    val_precision = precision_score(y_val, y_val_pred)
    val_recall = recall_score(y_val, y_val_pred)
    val_auc = roc_auc_score(y_val, y_val_pred_proba)
    
    if verbose:
        print(f"‚úÖ Optimisation termin√©e en {training_time:.2f}s")
        print(f"üèÜ Meilleur score CV ({scoring}): {best_cv_score:.4f}")
        print(f"üìà M√©triques de validation:")
        print(f"   ‚Ä¢ F1-score: {val_f1:.4f}")
        print(f"   ‚Ä¢ Pr√©cision: {val_precision:.4f}")  
        print(f"   ‚Ä¢ Rappel: {val_recall:.4f}")
        print(f"   ‚Ä¢ AUC-ROC: {val_auc:.4f}")
    
    # R√©sultats complets
    results = {
        'model_name': model_name,
        'best_model': best_model,
        'best_params': best_params,
        'best_cv_score': best_cv_score,
        'cv_results': grid_search.cv_results_,
        'training_time': training_time,
        'validation_metrics': {
            'f1': val_f1,
            'precision': val_precision,
            'recall': val_recall,
            'auc_roc': val_auc
        },
        'predictions': {
            'y_val_pred': y_val_pred,
            'y_val_pred_proba': y_val_pred_proba
        }
    }
    
    return results


# =============================================================================
# 3. S√âLECTION DE FEATURES
# =============================================================================

def perform_feature_selection(X_train: pd.DataFrame,
                             y_train: pd.Series,
                             X_val: pd.DataFrame,
                             y_val: pd.Series, 
                             estimator,
                             method: str = 'rfecv',
                             cv_folds: int = 5,
                             scoring: str = 'f1',
                             save_selector: bool = True,
                             verbose: bool = True) -> Dict:
    """
    Effectue la s√©lection de features avec diff√©rentes m√©thodes.
    
    Args:
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation
        estimator: Estimateur pour la s√©lection
        method: 'rfecv', 'importance', ou 'permutation'
        cv_folds: Nombre de folds
        scoring: M√©trique pour l'√©valuation
        save_path: Chemin pour sauvegarder le s√©lecteur
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire avec les r√©sultats de s√©lection
    """
    
    if verbose:
        print(f"üîç S√âLECTION DE FEATURES - {method.upper()}")
        print("=" * 40)
    
    results = {
        'method': method,
        'original_features': X_train.shape[1],
        'selected_features': None,
        'feature_names': None,
        'scores': None,
        'selector': None
    }
    
    if method == 'rfecv':
        # Recursive Feature Elimination with CV
        cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        
        selector = RFECV(
            estimator=estimator,
            cv=cv_strategy,
            scoring=scoring,
            n_jobs=-1,
            verbose=1 if verbose else 0
        )
        
        # Fit du s√©lecteur
        start_time = time.time()
        selector.fit(X_train, y_train)
        selection_time = time.time() - start_time
        
        # Features s√©lectionn√©es
        selected_mask = selector.support_
        selected_features = X_train.columns[selected_mask].tolist()
        n_selected = selector.n_features_
        
        results.update({
            'selected_features': n_selected,
            'feature_names': selected_features,
            'scores': selector.cv_results_['mean_test_score'],
            'selector': selector,
            'selection_time': selection_time
        })
        
        if verbose:
            print(f"‚úÖ RFECV termin√© en {selection_time:.2f}s")
            print(f"üéØ Features s√©lectionn√©es: {n_selected}/{X_train.shape[1]}")
            print(f"üèÜ Meilleur score: {selector.cv_results_['mean_test_score'][n_selected-1]:.4f}")
    
    elif method == 'importance':
        # Feature importance native du mod√®le
        start_time = time.time()
        estimator.fit(X_train, y_train)
        selection_time = time.time() - start_time
        
        if hasattr(estimator, 'feature_importances_'):
            importances = estimator.feature_importances_
            
            # Trier par importance
            indices = np.argsort(importances)[::-1]
            feature_names = X_train.columns[indices].tolist()
            
            results.update({
                'scores': importances,
                'feature_names': feature_names,
                'selection_time': selection_time,
                'estimator': estimator
            })
            
            if verbose:
                print(f"‚úÖ Importance calcul√©e en {selection_time:.2f}s")
                print("üèÜ Top 10 features les plus importantes:")
                for i in range(min(10, len(feature_names))):
                    idx = indices[i]
                    print(f"   {i+1}. {X_train.columns[idx]}: {importances[idx]:.4f}")
        else:
            print("‚ùå L'estimateur ne supporte pas feature_importances_")
            
    elif method == 'permutation':
        # Permutation importance
        start_time = time.time()
        estimator.fit(X_train, y_train)
        
        perm_importance = permutation_importance(
            estimator, X_val, y_val,
            scoring=scoring,
            n_repeats=5,
            random_state=42,
            n_jobs=-1
        )
        selection_time = time.time() - start_time
        
        # Trier par importance
        indices = np.argsort(perm_importance.importances_mean)[::-1]
        feature_names = X_train.columns[indices].tolist()
        
        results.update({
            'scores': perm_importance.importances_mean,
            'scores_std': perm_importance.importances_std,
            'feature_names': feature_names,
            'selection_time': selection_time,
            'estimator': estimator
        })
        
        if verbose:
            print(f"‚úÖ Permutation importance calcul√©e en {selection_time:.2f}s")
            print("üèÜ Top 10 features les plus importantes:")
            for i in range(min(10, len(feature_names))):
                idx = indices[i]
                mean_imp = perm_importance.importances_mean[idx]
                std_imp = perm_importance.importances_std[idx]
                print(f"   {i+1}. {X_train.columns[idx]}: {mean_imp:.4f} ¬± {std_imp:.4f}")
    
    else:
        raise ValueError(f"M√©thode '{method}' non support√©e")
    
    # Sauvegarde du s√©lecteur
    if save_selector and results['selector'] is not None:
        filename = f"selector_{method}_{scoring}.pkl"
        save_artifact(results['selector'], filename, cfg.paths.selectors)
        if verbose:
            log.info(f"üíæ S√©lecteur sauvegard√© : {filename}")
    
    return results


# =============================================================================
# 4. OPTIMISATION DES SEUILS DE CLASSIFICATION
# =============================================================================

def optimize_classification_threshold(y_true: np.ndarray,
                                    y_proba: np.ndarray,
                                    metric: str = 'f1',
                                    verbose: bool = True) -> Dict:
    """
    Optimise le seuil de classification pour maximiser une m√©trique donn√©e.
    
    Args:
        y_true: Labels vrais
        y_proba: Probabilit√©s pr√©dites
        metric: M√©trique √† optimiser ('f1', 'precision', 'recall')
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire avec le seuil optimal et les m√©triques
    """
    
    if verbose:
        print(f"üéØ OPTIMISATION DU SEUIL - {metric.upper()}")
        print("=" * 40)
    
    # G√©n√©rer une gamme de seuils
    thresholds = np.arange(0.1, 1.0, 0.01)
    
    scores = []
    best_threshold = 0.5
    best_score = 0.0
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        
        if metric == 'f1':
            score = f1_score(y_true, y_pred)
        elif metric == 'precision':
            score = precision_score(y_true, y_pred)
        elif metric == 'recall':
            score = recall_score(y_true, y_pred)
        else:
            raise ValueError(f"M√©trique '{metric}' non support√©e")
        
        scores.append(score)
        
        if score > best_score:
            best_score = score
            best_threshold = threshold
    
    # M√©triques avec le meilleur seuil
    y_pred_optimal = (y_proba >= best_threshold).astype(int)
    
    optimal_metrics = {
        'f1': f1_score(y_true, y_pred_optimal),
        'precision': precision_score(y_true, y_pred_optimal),
        'recall': recall_score(y_true, y_pred_optimal),
        'auc_roc': roc_auc_score(y_true, y_proba)
    }
    
    if verbose:
        print(f"üèÜ Seuil optimal: {best_threshold:.3f}")
        print(f"üìà Score optimis√© ({metric}): {best_score:.4f}")
        print("üìä M√©triques avec seuil optimal:")
        for metric_name, value in optimal_metrics.items():
            print(f"   ‚Ä¢ {metric_name}: {value:.4f}")
    
    results = {
        'best_threshold': best_threshold,
        'best_score': best_score,
        'optimized_metric': metric,
        'threshold_scores': list(zip(thresholds, scores)),
        'optimal_metrics': optimal_metrics,
        'y_pred_optimal': y_pred_optimal
    }
    
    return results


# =============================================================================
# 5. √âVALUATION ET COMPARAISON DES MOD√àLES
# =============================================================================

def evaluate_model_performance(y_true: np.ndarray,
                              y_pred: np.ndarray,
                              y_proba: np.ndarray,
                              model_name: str,
                              save_path: Optional[Path] = None,
                              verbose: bool = True) -> Dict:
    """
    √âvalue les performances d'un mod√®le avec m√©triques compl√®tes.
    
    Args:
        y_true: Labels vrais
        y_pred: Pr√©dictions
        y_proba: Probabilit√©s pr√©dites
        model_name: Nom du mod√®le
        save_path: Chemin pour sauvegarder les graphiques
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire avec toutes les m√©triques
    """
    
    if verbose:
        print(f"üìä √âVALUATION: {model_name.upper()}")
        print("=" * 40)
    
    # M√©triques de base
    metrics = {
        'model_name': model_name,
        'f1': f1_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'auc_roc': roc_auc_score(y_true, y_proba)
    }
    
    # Matrice de confusion
    cm = confusion_matrix(y_true, y_pred)
    metrics['confusion_matrix'] = cm
    
    # Rapport de classification
    class_report = classification_report(y_true, y_pred, output_dict=True)
    metrics['classification_report'] = class_report
    
    if verbose:
        print("üìà M√©triques principales:")
        print(f"   ‚Ä¢ F1-score: {metrics['f1']:.4f}")
        print(f"   ‚Ä¢ Pr√©cision: {metrics['precision']:.4f}")
        print(f"   ‚Ä¢ Rappel: {metrics['recall']:.4f}")
        print(f"   ‚Ä¢ AUC-ROC: {metrics['auc_roc']:.4f}")
        
        print("\nüéØ Matrice de confusion:")
        print(cm)
    
    # Visualisations
    if save_path:
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # 1. Matrice de confusion
        ConfusionMatrixDisplay(cm, display_labels=['Non-Ad', 'Ad']).plot(ax=axes[0])
        axes[0].set_title(f'Matrice de Confusion - {model_name}')
        
        # 2. Courbe ROC
        fpr, tpr, _ = roc_curve(y_true, y_proba)
        auc_score = auc(fpr, tpr)
        axes[1].plot(fpr, tpr, label=f'ROC (AUC = {auc_score:.3f})')
        axes[1].plot([0, 1], [0, 1], 'k--', label='Al√©atoire')
        axes[1].set_xlabel('Taux de Faux Positifs')
        axes[1].set_ylabel('Taux de Vrais Positifs')
        axes[1].set_title(f'Courbe ROC - {model_name}')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        # 3. Courbe Pr√©cision-Rappel
        precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_proba)
        axes[2].plot(recall_vals, precision_vals, label=f'PR (F1 = {metrics["f1"]:.3f})')
        axes[2].set_xlabel('Rappel')
        axes[2].set_ylabel('Pr√©cision')
        axes[2].set_title(f'Courbe Pr√©cision-Rappel - {model_name}')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Sauvegarder
        plot_path = save_path / f'{model_name}_evaluation.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        if verbose:
            print(f"üíæ Graphiques sauvegard√©s : {plot_path}")
    
    return metrics


# =============================================================================
# 6. PIPELINE COMPLET D'ENTRA√éNEMENT
# =============================================================================

def train_all_models(X_train: pd.DataFrame,
                     y_train: pd.Series,
                     X_val: pd.DataFrame,
                     y_val: pd.Series,
                     models_to_train: Optional[List[str]] = None,
                     param_grids: Optional[Dict] = None,
                     save_dir: Optional[Path] = None,
                     cv_folds: int = 5,
                     scoring: str = 'f1',
                     n_jobs: int = -1,
                     verbose: bool = True) -> Dict:
    """
    Entra√Æne tous les mod√®les sp√©cifi√©s avec optimisation compl√®te.
    
    Args:
        X_train, y_train: Donn√©es d'entra√Ænement
        X_val, y_val: Donn√©es de validation  
        models_to_train: Liste des mod√®les √† entra√Æner
        param_grids: Grilles d'hyperparam√®tres personnalis√©es
        save_dir: R√©pertoire pour sauvegarder tous les artefacts
        cv_folds: Nombre de folds pour la CV
        scoring: M√©trique d'optimisation
        n_jobs: Processus parall√®les
        verbose: Affichage d√©taill√©
        
    Returns:
        Dictionnaire avec tous les r√©sultats d'entra√Ænement
    """
    
    if models_to_train is None:
        models_to_train = ['randforest', 'xgboost', 'gradboost', 'svm', 'mlp']
    
    if param_grids is None:
        param_grids = get_default_param_grids()
    
    if verbose:
        print("üöÄ ENTRA√éNEMENT DE TOUS LES MOD√àLES")
        print("=" * 60)
        print(f"üìä Mod√®les √† entra√Æner: {', '.join(models_to_train)}")
        print(f"üíæ Sauvegarde: {'Oui' if save_dir else 'Non'}")
    
    results = {
        'training_summary': {},
        'best_models': {},
        'all_metrics': {},
        'comparison_data': []
    }
    
    if save_dir:
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
    
    for model_name in models_to_train:
        if verbose:
            print(f"\n{'='*20} {model_name.upper()} {'='*20}")
        
        try:
            # 1. Entra√Ænement et optimisation
            training_results = train_and_optimize_model(
                X_train, y_train, X_val, y_val,
                model_name=model_name,
                param_grid=param_grids.get(model_name),
                cv_folds=cv_folds,
                scoring=scoring,
                n_jobs=n_jobs,
                verbose=verbose
            )
            
            # 2. Optimisation du seuil
            threshold_results = optimize_classification_threshold(
                y_val, training_results['predictions']['y_val_pred_proba'],
                metric='f1', verbose=verbose
            )
            
            # 3. √âvaluation compl√®te
            evaluation_results = evaluate_model_performance(
                y_val, 
                threshold_results['y_pred_optimal'],
                training_results['predictions']['y_val_pred_proba'],
                model_name=model_name,
                save_path=save_dir / 'evaluation_plots' if save_dir else None,
                verbose=verbose
            )
            
            # Stocker les r√©sultats
            results['training_summary'][model_name] = training_results
            results['best_models'][model_name] = training_results['best_model']
            results['all_metrics'][model_name] = {
                **training_results['validation_metrics'],
                **threshold_results['optimal_metrics'],
                'best_threshold': threshold_results['best_threshold']
            }
            
            # Donn√©es pour comparaison
            results['comparison_data'].append({
                'Model': model_name,
                'F1_Score': evaluation_results['f1'],
                'Precision': evaluation_results['precision'],
                'Recall': evaluation_results['recall'],
                'AUC_ROC': evaluation_results['auc_roc'],
                'Best_Threshold': threshold_results['best_threshold'],
                'CV_Score': training_results['best_cv_score'],
                'Training_Time': training_results['training_time']
            })
            
            # Sauvegarde des artefacts
            if save_dir:
                # Pipeline complet
                pipeline_filename = f'pipeline_{model_name}.pkl'
                save_artifact(training_results['best_model'], pipeline_filename, save_dir)
                
                # Param√®tres
                params_filename = f'best_params_{model_name}.json'
                save_artifact(training_results['best_params'], params_filename, save_dir)
                
                # Seuil optimal
                threshold_data = {
                    'best_threshold': threshold_results['best_threshold'],
                    'optimized_metric': 'f1',
                    'optimal_metrics': threshold_results['optimal_metrics']
                }
                threshold_filename = f'threshold_{model_name}.json'
                save_artifact(threshold_data, threshold_filename, save_dir)
                
                if verbose:
                    print(f"üíæ Artefacts sauvegard√©s pour {model_name}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'entra√Ænement de {model_name}: {str(e)}")
            continue
    
    # Tableau de comparaison
    if results['comparison_data']:
        comparison_df = pd.DataFrame(results['comparison_data'])
        comparison_df = comparison_df.sort_values('F1_Score', ascending=False)
        results['comparison_dataframe'] = comparison_df
        
        if verbose:
            print("\nüèÜ TABLEAU DE COMPARAISON FINAL")
            print("=" * 60)
            print(comparison_df.round(4).to_string(index=False))
        
        # Meilleur mod√®le
        best_model_name = comparison_df.iloc[0]['Model']
        results['champion_model'] = {
            'name': best_model_name,
            'model': results['best_models'][best_model_name],
            'metrics': results['all_metrics'][best_model_name]
        }
        
        if verbose:
            print(f"\nü•á MOD√àLE CHAMPION: {best_model_name.upper()}")
            print(f"   F1-Score: {comparison_df.iloc[0]['F1_Score']:.4f}")
        
        # Sauvegarder le tableau de comparaison
        if save_dir:
            comparison_path = save_dir / 'models_comparison.csv'
            comparison_df.to_csv(comparison_path, index=False)
            
            # Sauvegarder le mod√®le champion s√©par√©ment
            champion_filename = f'champion_{best_model_name}.pkl'
            save_artifact(results['champion_model']['model'], champion_filename, save_dir)
            
            if verbose:
                print(f"üíæ Comparaison sauvegard√©e : {comparison_path}")
                print(f"üíæ Mod√®le champion sauvegard√© : {champion_filename}")
    
    if verbose:
        print(f"\nüéâ ENTRA√éNEMENT TERMIN√â - {len(results['best_models'])} mod√®les entra√Æn√©s")
    
    return results


# Utilitaires suppl√©mentaires
import time

def clean_xgb_params(params: Dict) -> Dict:
    """Nettoie les param√®tres XGBoost des cl√©s incompatibles."""
    params_clean = params.copy()
    params_to_remove = ['use_label_encoder', 'eval_metric', 'feature_weights']
    
    for param in params_to_remove:
        if param in params_clean:
            del params_clean[param]
    
    return params_clean


def clean_svm_params(params: Dict) -> Dict:
    """Nettoie les param√®tres SVM pour s'assurer de la compatibilit√©."""
    params_clean = params.copy()
    
    # S'assurer que probability=True pour predict_proba
    params_clean['probability'] = True
    return params_clean