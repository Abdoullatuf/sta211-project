{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de Migration - Nouvelle Structure Modulaire\n",
    "\n",
    "Ce notebook teste la nouvelle structure modulaire du projet STA211."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test de la configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modules.config import cfg\n",
    "\n",
    "print(\"✅ Configuration chargée\")\n",
    "print(f\"📁 Racine du projet: {cfg.root}\")\n",
    "print(f\"🏷️ Nom du projet: {cfg.project.project_name}\")\n",
    "print(f\"👤 Auteur: {cfg.project.author}\")\n",
    "print(f\"📊 Random state: {cfg.project.random_state}\")\n",
    "\n",
    "print(\"\\n📂 Chemins configurés:\")\n",
    "for attr_name in ['data', 'raw', 'processed', 'outputs', 'figures', 'models', 'imputers', 'transformers']:\n",
    "    path = getattr(cfg.paths, attr_name)\n",
    "    exists = \"✅\" if path.exists() else \"❌\"\n",
    "    print(f\"  {exists} {attr_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test des utilitaires de stockage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import save_artifact, load_artifact, artifact_exists\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ Utilitaires de stockage importés\")\n",
    "\n",
    "# Test de sauvegarde/rechargement\n",
    "test_data = {'test': 'data', 'array': np.array([1, 2, 3])}\n",
    "test_filename = \"test_artifact.pkl\"\n",
    "\n",
    "# Sauvegarder\n",
    "saved_path = save_artifact(test_data, test_filename, cfg.paths.models)\n",
    "print(f\"💾 Données sauvegardées: {saved_path}\")\n",
    "\n",
    "# Vérifier existence\n",
    "exists = artifact_exists(test_filename, cfg.paths.models)\n",
    "print(f\"🔍 Artefact existe: {exists}\")\n",
    "\n",
    "# Recharger\n",
    "loaded_data = load_artifact(test_filename, cfg.paths.models)\n",
    "print(f\"📂 Données rechargées: {loaded_data}\")\n",
    "\n",
    "# Vérifier intégrité\n",
    "integrity_check = (\n",
    "    loaded_data['test'] == test_data['test'] and \n",
    "    np.array_equal(loaded_data['array'], test_data['array'])\n",
    ")\n",
    "print(f\"✅ Intégrité des données: {integrity_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test du module de prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.notebook1_preprocessing import (\n",
    "    load_and_clean_data,\n",
    "    perform_knn_imputation,\n",
    "    apply_optimal_transformations,\n",
    "    detect_and_cap_outliers\n",
    ")\n",
    "\n",
    "print(\"✅ Module de prétraitement importé\")\n",
    "print(\"📦 Fonctions disponibles:\")\n",
    "print(\"  - load_and_clean_data\")\n",
    "print(\"  - perform_knn_imputation\")\n",
    "print(\"  - apply_optimal_transformations\")\n",
    "print(\"  - detect_and_cap_outliers\")\n",
    "\n",
    "# Test avec des données factices\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "# Créer des données de test\n",
    "n_samples = 100\n",
    "test_df = pd.DataFrame({\n",
    "    'feature1': np.random.normal(0, 1, n_samples),\n",
    "    'feature2': np.random.normal(5, 2, n_samples),\n",
    "    'target': np.random.binomial(1, 0.3, n_samples)\n",
    "})\n",
    "\n",
    "# Introduire quelques valeurs manquantes\n",
    "missing_idx = np.random.choice(n_samples, 10, replace=False)\n",
    "test_df.loc[missing_idx, 'feature1'] = np.nan\n",
    "\n",
    "print(f\"\\n📊 Données de test créées: {test_df.shape}\")\n",
    "print(f\"🕳️ Valeurs manquantes: {test_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Test KNN imputation\n",
    "df_imputed, imputer = perform_knn_imputation(\n",
    "    test_df, ['feature1'], n_neighbors=5, save_imputer=False\n",
    ")\n",
    "\n",
    "remaining_missing = df_imputed.isnull().sum().sum()\n",
    "print(f\"✅ Imputation KNN: {remaining_missing} valeurs manquantes restantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test du module de modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.notebook2_modeling import (\n",
    "    get_default_param_grids,\n",
    "    create_model_estimators,\n",
    "    optimize_classification_threshold\n",
    ")\n",
    "\n",
    "print(\"✅ Module de modélisation importé\")\n",
    "\n",
    "# Test des grilles de paramètres\n",
    "param_grids = get_default_param_grids()\n",
    "print(f\"\\n📋 Grilles de paramètres disponibles: {list(param_grids.keys())}\")\n",
    "\n",
    "# Test des estimateurs\n",
    "estimators = create_model_estimators()\n",
    "print(f\"🤖 Estimateurs disponibles: {list(estimators.keys())}\")\n",
    "\n",
    "# Test d'optimisation de seuil avec des données factices\n",
    "y_true = np.random.binomial(1, 0.3, 1000)\n",
    "y_proba = np.random.beta(2, 5, 1000)  # Probabilités factices\n",
    "\n",
    "threshold_results = optimize_classification_threshold(\n",
    "    y_true, y_proba, metric='f1', verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Seuil optimal: {threshold_results['best_threshold']:.3f}\")\n",
    "print(f\"📈 Score F1 optimal: {threshold_results['best_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test du module d'ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.modeling import (\n",
    "    create_voting_ensemble,\n",
    "    create_stacking_ensemble,\n",
    "    get_ensemble_feature_importance\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"✅ Module d'ensembles importé\")\n",
    "\n",
    "# Créer des modèles de base pour test\n",
    "base_models = {\n",
    "    'rf': RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    'svm': SVC(probability=True, random_state=42),\n",
    "    'lr': LogisticRegression(random_state=42)\n",
    "}\n",
    "\n",
    "# Test Voting Ensemble\n",
    "voting_ensemble = create_voting_ensemble(base_models, voting='soft')\n",
    "print(f\"🗳️ Voting Ensemble créé: {type(voting_ensemble).__name__}\")\n",
    "\n",
    "# Test Stacking Ensemble\n",
    "stacking_ensemble = create_stacking_ensemble(base_models)\n",
    "print(f\"🥞 Stacking Ensemble créé: {type(stacking_ensemble).__name__}\")\n",
    "\n",
    "print(\"\\n📦 Fonctions d'ensemble disponibles:\")\n",
    "print(\"  - create_voting_ensemble\")\n",
    "print(\"  - create_bagging_ensemble\")\n",
    "print(\"  - create_stacking_ensemble\")\n",
    "print(\"  - optimize_ensemble\")\n",
    "print(\"  - train_all_ensembles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test du module d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.evaluation import (\n",
    "    calculate_basic_metrics,\n",
    "    calculate_detailed_metrics,\n",
    "    optimize_threshold,\n",
    "    generate_evaluation_report\n",
    ")\n",
    "\n",
    "print(\"✅ Module d'évaluation importé\")\n",
    "\n",
    "# Test avec des prédictions factices\n",
    "y_true = np.random.binomial(1, 0.4, 200)\n",
    "y_pred = np.random.binomial(1, 0.4, 200)\n",
    "y_proba = np.random.beta(2, 3, 200)\n",
    "\n",
    "# Test des métriques de base\n",
    "basic_metrics = calculate_basic_metrics(y_true, y_pred, y_proba)\n",
    "print(f\"\\n📊 Métriques de base calculées: {list(basic_metrics.keys())}\")\n",
    "print(f\"   F1-Score: {basic_metrics['f1']:.3f}\")\n",
    "print(f\"   AUC-ROC: {basic_metrics.get('auc_roc', 'N/A')}\")\n",
    "\n",
    "# Test des métriques détaillées\n",
    "detailed_metrics = calculate_detailed_metrics(y_true, y_pred, y_proba)\n",
    "print(f\"\\n📈 Métriques détaillées disponibles: {list(detailed_metrics.keys())}\")\n",
    "\n",
    "# Test d'optimisation de seuil\n",
    "threshold_opt = optimize_threshold(y_true, y_proba, metric='f1')\n",
    "print(f\"\\n🎯 Optimisation de seuil:\")\n",
    "print(f\"   Seuil optimal: {threshold_opt['best_threshold']:.3f}\")\n",
    "print(f\"   Score optimal: {threshold_opt['best_score']:.3f}\")\n",
    "\n",
    "print(\"\\n📋 Fonctions d'évaluation disponibles:\")\n",
    "print(\"  - calculate_basic_metrics\")\n",
    "print(\"  - calculate_detailed_metrics\") \n",
    "print(\"  - optimize_threshold\")\n",
    "print(\"  - analyze_threshold_sensitivity\")\n",
    "print(\"  - plot_evaluation_dashboard\")\n",
    "print(\"  - generate_evaluation_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test de génération de rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation des résultats d'un modèle\n",
    "model_results = {\n",
    "    'basic_metrics': basic_metrics,\n",
    "    'classification_report': detailed_metrics['classification_report'],\n",
    "    'confusion_matrix': detailed_metrics['confusion_matrix'],\n",
    "    'optimal_threshold': threshold_opt,\n",
    "    'training_time': 45.2,\n",
    "    'best_cv_score': 0.742\n",
    "}\n",
    "\n",
    "# Générer un rapport\n",
    "report = generate_evaluation_report(\n",
    "    model_results, \n",
    "    model_name=\"TestModel\", \n",
    "    save_report=False\n",
    ")\n",
    "\n",
    "print(\"📝 Rapport d'évaluation généré:\")\n",
    "print(\"=\" * 50)\n",
    "print(report[:500] + \"...\" if len(report) > 500 else report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Résumé des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 RÉSUMÉ DES TESTS DE MIGRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Configuration centralisée\")\n",
    "print(\"✅ Utilitaires de stockage\")\n",
    "print(\"✅ Module de prétraitement (Notebook 1)\")\n",
    "print(\"✅ Module de modélisation (Notebook 2)\")\n",
    "print(\"✅ Module d'ensembles (Notebook 3)\")\n",
    "print(\"✅ Module d'évaluation\")\n",
    "print(\"✅ Génération de rapports\")\n",
    "print(\"\\n🚀 La migration vers la nouvelle structure modulaire est RÉUSSIE !\")\n",
    "print(\"\\n📖 Consultez MIGRATION_GUIDE.md pour les détails d'utilisation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}