{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de Migration - Nouvelle Structure Modulaire\n",
    "\n",
    "Ce notebook teste la nouvelle structure modulaire du projet STA211."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test de la configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modules.config import cfg\n",
    "\n",
    "print(\"âœ… Configuration chargÃ©e\")\n",
    "print(f\"ðŸ“ Racine du projet: {cfg.root}\")\n",
    "print(f\"ðŸ·ï¸ Nom du projet: {cfg.project.project_name}\")\n",
    "print(f\"ðŸ‘¤ Auteur: {cfg.project.author}\")\n",
    "print(f\"ðŸ“Š Random state: {cfg.project.random_state}\")\n",
    "\n",
    "print(\"\\nðŸ“‚ Chemins configurÃ©s:\")\n",
    "for attr_name in ['data', 'raw', 'processed', 'outputs', 'figures', 'models', 'imputers', 'transformers']:\n",
    "    path = getattr(cfg.paths, attr_name)\n",
    "    exists = \"âœ…\" if path.exists() else \"âŒ\"\n",
    "    print(f\"  {exists} {attr_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test des utilitaires de stockage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import save_artifact, load_artifact, artifact_exists\n",
    "import numpy as np\n",
    "\n",
    "print(\"âœ… Utilitaires de stockage importÃ©s\")\n",
    "\n",
    "# Test de sauvegarde/rechargement\n",
    "test_data = {'test': 'data', 'array': np.array([1, 2, 3])}\n",
    "test_filename = \"test_artifact.pkl\"\n",
    "\n",
    "# Sauvegarder\n",
    "saved_path = save_artifact(test_data, test_filename, cfg.paths.models)\n",
    "print(f\"ðŸ’¾ DonnÃ©es sauvegardÃ©es: {saved_path}\")\n",
    "\n",
    "# VÃ©rifier existence\n",
    "exists = artifact_exists(test_filename, cfg.paths.models)\n",
    "print(f\"ðŸ” Artefact existe: {exists}\")\n",
    "\n",
    "# Recharger\n",
    "loaded_data = load_artifact(test_filename, cfg.paths.models)\n",
    "print(f\"ðŸ“‚ DonnÃ©es rechargÃ©es: {loaded_data}\")\n",
    "\n",
    "# VÃ©rifier intÃ©gritÃ©\n",
    "integrity_check = (\n",
    "    loaded_data['test'] == test_data['test'] and \n",
    "    np.array_equal(loaded_data['array'], test_data['array'])\n",
    ")\n",
    "print(f\"âœ… IntÃ©gritÃ© des donnÃ©es: {integrity_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test du module de prÃ©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.notebook1_preprocessing import (\n",
    "    load_and_clean_data,\n",
    "    perform_knn_imputation,\n",
    "    apply_optimal_transformations,\n",
    "    detect_and_cap_outliers\n",
    ")\n",
    "\n",
    "print(\"âœ… Module de prÃ©traitement importÃ©\")\n",
    "print(\"ðŸ“¦ Fonctions disponibles:\")\n",
    "print(\"  - load_and_clean_data\")\n",
    "print(\"  - perform_knn_imputation\")\n",
    "print(\"  - apply_optimal_transformations\")\n",
    "print(\"  - detect_and_cap_outliers\")\n",
    "\n",
    "# Test avec des donnÃ©es factices\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "# CrÃ©er des donnÃ©es de test\n",
    "n_samples = 100\n",
    "test_df = pd.DataFrame({\n",
    "    'feature1': np.random.normal(0, 1, n_samples),\n",
    "    'feature2': np.random.normal(5, 2, n_samples),\n",
    "    'target': np.random.binomial(1, 0.3, n_samples)\n",
    "})\n",
    "\n",
    "# Introduire quelques valeurs manquantes\n",
    "missing_idx = np.random.choice(n_samples, 10, replace=False)\n",
    "test_df.loc[missing_idx, 'feature1'] = np.nan\n",
    "\n",
    "print(f\"\\nðŸ“Š DonnÃ©es de test crÃ©Ã©es: {test_df.shape}\")\n",
    "print(f\"ðŸ•³ï¸ Valeurs manquantes: {test_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Test KNN imputation\n",
    "df_imputed, imputer = perform_knn_imputation(\n",
    "    test_df, ['feature1'], n_neighbors=5, save_imputer=False\n",
    ")\n",
    "\n",
    "remaining_missing = df_imputed.isnull().sum().sum()\n",
    "print(f\"âœ… Imputation KNN: {remaining_missing} valeurs manquantes restantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test du module de modÃ©lisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.notebook2_modeling import (\n",
    "    get_default_param_grids,\n",
    "    create_model_estimators,\n",
    "    optimize_classification_threshold\n",
    ")\n",
    "\n",
    "print(\"âœ… Module de modÃ©lisation importÃ©\")\n",
    "\n",
    "# Test des grilles de paramÃ¨tres\n",
    "param_grids = get_default_param_grids()\n",
    "print(f\"\\nðŸ“‹ Grilles de paramÃ¨tres disponibles: {list(param_grids.keys())}\")\n",
    "\n",
    "# Test des estimateurs\n",
    "estimators = create_model_estimators()\n",
    "print(f\"ðŸ¤– Estimateurs disponibles: {list(estimators.keys())}\")\n",
    "\n",
    "# Test d'optimisation de seuil avec des donnÃ©es factices\n",
    "y_true = np.random.binomial(1, 0.3, 1000)\n",
    "y_proba = np.random.beta(2, 5, 1000)  # ProbabilitÃ©s factices\n",
    "\n",
    "threshold_results = optimize_classification_threshold(\n",
    "    y_true, y_proba, metric='f1', verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Seuil optimal: {threshold_results['best_threshold']:.3f}\")\n",
    "print(f\"ðŸ“ˆ Score F1 optimal: {threshold_results['best_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test du module d'ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.modeling import (\n",
    "    create_voting_ensemble,\n",
    "    create_stacking_ensemble,\n",
    "    get_ensemble_feature_importance\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"âœ… Module d'ensembles importÃ©\")\n",
    "\n",
    "# CrÃ©er des modÃ¨les de base pour test\n",
    "base_models = {\n",
    "    'rf': RandomForestClassifier(n_estimators=10, random_state=42),\n",
    "    'svm': SVC(probability=True, random_state=42),\n",
    "    'lr': LogisticRegression(random_state=42)\n",
    "}\n",
    "\n",
    "# Test Voting Ensemble\n",
    "voting_ensemble = create_voting_ensemble(base_models, voting='soft')\n",
    "print(f\"ðŸ—³ï¸ Voting Ensemble crÃ©Ã©: {type(voting_ensemble).__name__}\")\n",
    "\n",
    "# Test Stacking Ensemble\n",
    "stacking_ensemble = create_stacking_ensemble(base_models)\n",
    "print(f\"ðŸ¥ž Stacking Ensemble crÃ©Ã©: {type(stacking_ensemble).__name__}\")\n",
    "\n",
    "print(\"\\nðŸ“¦ Fonctions d'ensemble disponibles:\")\n",
    "print(\"  - create_voting_ensemble\")\n",
    "print(\"  - create_bagging_ensemble\")\n",
    "print(\"  - create_stacking_ensemble\")\n",
    "print(\"  - optimize_ensemble\")\n",
    "print(\"  - train_all_ensembles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test du module d'Ã©valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.evaluation import (\n",
    "    calculate_basic_metrics,\n",
    "    calculate_detailed_metrics,\n",
    "    optimize_threshold,\n",
    "    generate_evaluation_report\n",
    ")\n",
    "\n",
    "print(\"âœ… Module d'Ã©valuation importÃ©\")\n",
    "\n",
    "# Test avec des prÃ©dictions factices\n",
    "y_true = np.random.binomial(1, 0.4, 200)\n",
    "y_pred = np.random.binomial(1, 0.4, 200)\n",
    "y_proba = np.random.beta(2, 3, 200)\n",
    "\n",
    "# Test des mÃ©triques de base\n",
    "basic_metrics = calculate_basic_metrics(y_true, y_pred, y_proba)\n",
    "print(f\"\\nðŸ“Š MÃ©triques de base calculÃ©es: {list(basic_metrics.keys())}\")\n",
    "print(f\"   F1-Score: {basic_metrics['f1']:.3f}\")\n",
    "print(f\"   AUC-ROC: {basic_metrics.get('auc_roc', 'N/A')}\")\n",
    "\n",
    "# Test des mÃ©triques dÃ©taillÃ©es\n",
    "detailed_metrics = calculate_detailed_metrics(y_true, y_pred, y_proba)\n",
    "print(f\"\\nðŸ“ˆ MÃ©triques dÃ©taillÃ©es disponibles: {list(detailed_metrics.keys())}\")\n",
    "\n",
    "# Test d'optimisation de seuil\n",
    "threshold_opt = optimize_threshold(y_true, y_proba, metric='f1')\n",
    "print(f\"\\nðŸŽ¯ Optimisation de seuil:\")\n",
    "print(f\"   Seuil optimal: {threshold_opt['best_threshold']:.3f}\")\n",
    "print(f\"   Score optimal: {threshold_opt['best_score']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Fonctions d'Ã©valuation disponibles:\")\n",
    "print(\"  - calculate_basic_metrics\")\n",
    "print(\"  - calculate_detailed_metrics\") \n",
    "print(\"  - optimize_threshold\")\n",
    "print(\"  - analyze_threshold_sensitivity\")\n",
    "print(\"  - plot_evaluation_dashboard\")\n",
    "print(\"  - generate_evaluation_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test de gÃ©nÃ©ration de rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation des rÃ©sultats d'un modÃ¨le\n",
    "model_results = {\n",
    "    'basic_metrics': basic_metrics,\n",
    "    'classification_report': detailed_metrics['classification_report'],\n",
    "    'confusion_matrix': detailed_metrics['confusion_matrix'],\n",
    "    'optimal_threshold': threshold_opt,\n",
    "    'training_time': 45.2,\n",
    "    'best_cv_score': 0.742\n",
    "}\n",
    "\n",
    "# GÃ©nÃ©rer un rapport\n",
    "report = generate_evaluation_report(\n",
    "    model_results, \n",
    "    model_name=\"TestModel\", \n",
    "    save_report=False\n",
    ")\n",
    "\n",
    "print(\"ðŸ“ Rapport d'Ã©valuation gÃ©nÃ©rÃ©:\")\n",
    "print(\"=\" * 50)\n",
    "print(report[:500] + \"...\" if len(report) > 500 else report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RÃ©sumÃ© des tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ RÃ‰SUMÃ‰ DES TESTS DE MIGRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… Configuration centralisÃ©e\")\n",
    "print(\"âœ… Utilitaires de stockage\")\n",
    "print(\"âœ… Module de prÃ©traitement (Notebook 1)\")\n",
    "print(\"âœ… Module de modÃ©lisation (Notebook 2)\")\n",
    "print(\"âœ… Module d'ensembles (Notebook 3)\")\n",
    "print(\"âœ… Module d'Ã©valuation\")\n",
    "print(\"âœ… GÃ©nÃ©ration de rapports\")\n",
    "print(\"\\nðŸš€ La migration vers la nouvelle structure modulaire est RÃ‰USSIE !\")\n",
    "print(\"\\nðŸ“– Consultez MIGRATION_GUIDE.md pour les dÃ©tails d'utilisation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}